
## Source Systems, Data Ingestion and Pipelines
## Course 2
## Week 1

## Introduction to Source Systems


# Types of Source Systems


Structured Data     :  organized as table of rows and columns 
Semi-Structured Data:  Data that is not in tabular form but still has some structure
            i.e. JSON, XML

Unstructured Data:  Data that does not have any predefined structure 
        text, video, audio, images


General types of source systems

    Databases                   Files               Streaming Systems

stored in organized way      sequence of bytes       continuous flow of data

structured                     text                   semi-structured
semi-structured                images
                               audio                 [producer] -> [Message Q, streaming platform]
                               csv                                      |
   CRUD                                                                 +-->> [consumer] 
   DBMS                     structured                  IoT
    Relational              semi-structured             
    NoSQL                   unstructured



# Relational Databases
 
  - Online Transaction Processing: OLTP
  - Primary Key + Foreign Key


Data Normalization: 
    - minimize redundancy
    - ensure data integrity

                                    SQL Commands
                                         |
      +----------------------+-----------------------+---------------------+                                   
      |                      |                       |                     |
      |                      |                       |                     |
     Data                   Data                    Data                  Data  
    Cleaning               Joining               Aggregating            Filtering

    DROP                   INNER JOIN               SUM                  WHERE    
  TRUNCATE                 LEFT JOIN                AVG                   AND    
    TRIM                  RIGHT JOIN               COUNT                  OR    
   REPLACE                 FULL JOIN                MAX                   IS NULL
   SELECT DISTINCT           UNION                  MIN                   IS NOT NULL
                                                   GROUP BY               IN 
                                                                          LIKE 


# SQL Queries 
 

# NoSQL Databases 

NoSQL == Not Only SQL

- non-tabular values 
    - key value
    - document 
    - graph
    - wide column

- no predefined schemas
- more flexibility w/ data


Horizontal Scaling 
                                +---------> Secondary   (Eventual consistency: data read may not be up to date)
                                |                           
            client  ---->> [NoSQL DB]  ---> Primary


        NoSQL DB                        Relational DB
     -------------                    ----------------   
    Eventual Consistency              Strong Consistency

     speed is prioritized            read data only when all nodes
     system availability and           have been updated
      scalability are important

Not All NoSQL databases guarantee ACID:
    Atomicity
    Consistency         NOTE: mongoDB is ACID compliant
    Isolation
    Durability


Key-Value Database:
    - fast lookup: i.e. user session data

Document Store:
    - data stored in JSON-like documents
    - each document as unique key
    - documents organized into collections
        document    ==  row
        collection  ==  table

    - all information stored in one document
    - document stores don't support joins
    - flexible scheam

    use cases: content management, catalogs, sensor readings

NOTE: document databases can become a nightmare to manage and query


# Database ACID Compliance

    Relational Databases                NoSQL Databases
     Atomicity                           Not ACID compliant by default
     Consistency                         
     Isolation                           NOTE: mongoDB is ACID compliant
     Durability

ensures transactions are
processed reliably and 
accurately in an OLTP system

Atomicity:   transactions are atomic, treated as single indivisible unit
                all or no part of a transaction is done

Consistency: changes to data within a transaction follow the set of rules 
             or constraints defined by database schema

Isolation:   each transaction is executed independently in sequential order

Durability:  Once transaction completed, its effects are permanent and will survive 
             any subsequent system failures. (i.e. power loss)

ACID Principle guarantees that a database will maintain a consistent picture of the world
        - Strong Consistency:  Data consisten across the entire network 


# Interacting with DynamoDB

DynamoDB: Key-value Database
        - row has attributes of one item 
        - identified by a key
        - simple primary key == partition key
        - composite primary key == partition key + sort key
        - schemaless

Python Boto3:  python package that allows you to interact with AWS services

    CREATE:       create_table
    READ  :       scan / get_item / query
    UPDATE:       put_item /  write_batch_items /  update_item
    DELETE:       delete_item
                   
import boto3 
client = boto3.client('dynamodb')

KeySchema=[
   {
        'AttributeName': 'ForumName',
        'KeyType': 'HASH'
   },
   {
        'AttributeName': 'Subject',
        'KeyType': 'RANGE'
   },
]
HASH  == partition key
RANGE == sort key

def put_item_db( table_name: str, item: Dict[str, Any], **kwargs):
    ### START CODE HERE ### (~ 2 lines of code)
    client = boto3.client("dynamodb")
    response = client.put_item(TableName=table_name, Item=item, **kwargs)
    ### END CODE HERE ###

    return response

for dynamodb_tab in [product_catalog_table, thread_table]:
    file_name = dynamodb_tab['table_name'].split('-')[-1]    
    items = read_data(file_path=f'./data/aws_sample_data/{file_name}.json')
    
    for item in items[dynamodb_tab["table_name"]]:
        put_item_db(table_name=dynamodb_tab["table_name"], item=item['PutRequest']['Item'])

def batch_write_item_db(items: Dict[str, Any], **kwargs):
    ### START CODE HERE ### (~ 2 lines of code)
    client = boto3.client("dynamodb")
    response = client.batch_write_item(RequestItems=items, **kwargs)
    ### END CODE HERE ###
    
    return response

for dynamodb_tab in [reply_table, forum_table]:
    file_name = dynamodb_tab['table_name'].split('-')[-1]    
    items = read_data(file_path=f'./data/aws_sample_data/{file_name}.json')
    response = batch_write_item_db(items=items)
    print(response)


Queried data for table de-c2w1-dynamodb-ProductCatalog:

Read Capacity Unit  / consistent vs. strongly consistent reads: 

The previous request consumed 1.0 RCU because this item is less than 4KB. 
(RCU stands for Read Capacity Unit: "One read capacity unit represents one strongly consistent read per second, 
 or two eventually consistent reads per second, for an item up to 4 KB in size", reference).



The DynamoDB DeleteItem() method is used to delete an item. Deletes in DynamoDB are singleton operations. 
There is no single command you can run that would delete all the rows in the table. 



# Object Store

 - file treated as an individual object
 - object storage has no hierarchy
 - any type of data
    - semi-structured and unstructured
    - serving data for training ML models

 - each object has:
    -  Universal Unique Identifier (UUID key)
    -  Metadata: creation_date, file type, owner

NOTE: after initial write object is immutable
        - no random write or append

    - A write requires a new write with UUID pointing to new object
    - Can enable versioning: 
        - write metadata which specified version

Why Use Object Storage
    - store files of various formats without a file system structure
    - easily scale out to provide virtually limitless storage space
    - replicate data across several availability zones

        S3: 99.999999999% data durability (11 9's)

    - cheaper than other storage options
    - ideal for data lakes and data lakehouses


Object Store Lab

s3_client.create_bucket(Bucket=bucket_name)
s3_client.upload_file(local_file_path, bucket_name, object_key)
s3_client.select_object_content(bucket_name, object_key)
s3_client.download_file(bucket_name, object_key, local_file_path)

NOTE: can apply a SQL expression to apply on the return from select_object_content()

# aws cli 
aws s3 ls
aws s3 ls de-c2w1lab3-265729830485/csv/

# set query parameters
# 
file_s3_key = 'csv/ratings_ml_training_dataset.csv'
kwargs = {'ExpressionType': 'SQL',
          'Expression': """SELECT * FROM s3object AS s WHERE s.\"productline\" = 'Trains' LIMIT 20""",
          'InputSerialization': {'CSV': {"FileHeaderInfo": "Use"}, 'CompressionType': 'NONE'},
          'OutputSerialization': {'CSV': {}},
}

response = s3_select_object_content(bucket_name=BUCKET_NAME, object_key=file_s3_key, **kwargs)


NOTE: by default S3 buckets and objects in it are private

# below updates bucket's public access settings
# 
    s3_client.put_public_access_block(
        Bucket=bucket_name,
        PublicAccessBlockConfiguration=public_access_block_configuration
    )

# Define the public access settings  
public_access_configuration = {
    'BlockPublicAcls': False,
    'IgnorePublicAcls': False,
    'BlockPublicPolicy': False,
    'RestrictPublicBuckets': False
}

s3_public_access_setup(bucket_name=BUCKET_NAME,  
                       public_access_block_configuration=public_access_configuration)
                       
# apply a policy to a bucket
response = s3_client.put_bucket_policy(Bucket=bucket_name, Policy=json.dumps(policy))

# policy to the bucket to allow anyone on the internet to have reading access to the 
#  objects whose key starts with images/

policy = { 
    "Version": "2012-10-17", 
    "Statement": [
        {
            "Effect": "Allow",
            "Principal": "*",
            "Action": "s3:GetObject",
            "Resource": f"arn:aws:s3:::{BUCKET_NAME}/images/*"
        }
    ]
}

# update bucket versioning
# 
response = s3_client.put_bucket_versioning(
        Bucket=bucket_name,
        VersioningConfiguration=versioning_config
    )

# versioning config
# 
versioning_config = {'Status': 'Enabled'}

response = configure_bucket_versioning(bucket_name=BUCKET_NAME, 
                                       versioning_config=versioning_config)


## S3 select_object_content
https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3/client/select_object_content.html

## S3 query data
https://aws.amazon.com/blogs/storage/querying-data-without-servers-or-databases-using-amazon-s3-select/


## S3 pub_public_access_block
https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3/client/put_public_access_block.html




# Logs

Log: apend only sequence of records ordered by time, capturing info about events

       Rich data source                  downstream use case
    ------------------------          ------------------------
    Web Server Logs        --------->>     Analysis of user behavior patterns
    Database Sysetem Logs  --------->>     Track changes in source database
    Security Sysetem Logs  --------->>     Machine Learning anomaly detection


Log Level:  tag to categorize the event
    - debug         - info
    - warn          - error
    - fatal          

# Streaming Systems



Event:
Something happened in the world or change to a system

Message:
A record of info about an event

Stream: 
A sequence of messages



Straming System Components


[  Event   ]  ----->>   [   Event Router /  ]  --------->>  [  Event   ]
[ Producer ]            [  Streaking Broker ]     |         [ Consumer ]
                                                  |
 - IoT                    - acts as buffer        |         [  Event   ]
 - Mobile App               to filter and         +=---->>  [ Consumer ]
 - API                      distribute messages
 - Website                - decouples producer 
                            and consumer

                            - Message Queues  (FIFO / SQS)
                            - Event Streaming (Append only log: Kinesis, Kafka)




## Source Systems, Data Ingestion and Pipelines
## Course 2
## Week 1

## Connecting to Source Systems


- boto3
- cloud9
- jupyter

- API Connector 
    - JDBC
    - ODBC

# Connecting to an Amazon RDS MySQL Database

https://aws.amazon.com/getting-started/hands-on/create-mysql-db/

# Access RDS via CloudShell

## MySql Connect
https://dev.mysql.com/doc/refman/8.0/en/connecting.html

## Pgsql Connect
https://www.postgresql.org/docs/9.1/app-psql.html



mysql --host=[hostname]            --port=[port number] 
      --user=[database user name]  --password=[database user password]

mysql> show databases;
mysql> use  <db_name>;
mysql> show tables;
mysql> select * from <tbl_name>;
mysql> exit


get the endpoint and port via cli

aws rds describe-db-instances --filters "Name=engine,Values=mysql" --query "*[].[DBInstanceIdentifier,Endpoint.Address,Endpoint.Port,MasterUsername]"


# connect via python
import boto3

access_key_id="AXXXXXXXXXXXXH"
secret_access_key="bXXXXXXXXXXXXZ"
region_name = "us-east-1"

session = boto3.Session(aws_access_key_id=access_key_id, aws_secret_access_key=secret_access_key, region_name=region_name) 
rds=session.client("rds)
dbInstance = rds.describe_db_instances()['DBInstances'[0]]

import pymysql

try: 
    conn = pymysql.connect(host=ENDPOINT, user=USER, passwd=toekn, port=PORT, database=DBNAME)
    cur = conn.cursor()
    cur.execute("""SELECT * from pet""")
    query_results = curr.fetchall()
    print(query_results)
 except Exception as e:
    print("Database connection failed due to {}".format(e))   


# Basics of IAM and Permissions


IAM: framework for managing permissions
    - permissions define which actions an identity (person, app)
      can perform on a specific set of resources



AWS Identity and Access Management


  - Policies used to grant identities access to resources

                                              Resources
[------------]                              [------------]                          
[            ]                              [    [S3]    ]       
[ Identities ]  --->>  [Policies ] --->>    [    [RDS]   ]
[            ]                              [    [EC2]   ]       
[------------]                              [------------]                          

Types of Identities:
Root User:   Has unrestricted access to all resources

IAM  User:   Has specific permissions to certain resources
                - Username and password
                - Access Key

IAM  Group:  A collection of users that inherit the same permission  
             from the group policy
                    i.e.  DB Users or DB Admins

IAM  Role :  A user, application or services that's been granted 
             temporary permissions


        [EC2]     -----XX---->   [S3]
        EC2 be default cannot access S3


    [ROLE] [EC2]     ----------->   [S3]
        Role allows EC2 instance to access S3



# Basics of AWS IAM 

IAM: web servcies that helps you manage and securely control access to AWS resources and services.


Root User:  full access to all AWS resources and services in an account

IAM User:   person or service that interacts with AWS resources.
                - define what resources IAM user can access
                - what actions they can perform
                - credentials generated:
                    - username / password
                    - access keys for apps
                - grant access to AWS resources via policies

Policy:     specifies what actions are allowed or denied for a resource
                - read only, write only, full access
            policy can be attached to several users
            a user can have several policies

When a request is made, AWS evaluates policies to determine if request is allowed

IAM Group:  collection of users, can attach policy to group i.e. Data Scientists or DBAs
                - each user in group inherit's group's permissions 
                - groups can have multiple users
                - user can have no group, one, group or multiple groups (up to 10)
                NOTE: groups CANNOT be nested

IAM Role:   specific permissions with short-term credentials
            Roles can be assumed by entities: people, applications, AWS resources 
            No long-term credentials:  temporary security credentials provided 
               for duration of the role sessions.
            1. Create IAM role   
            2. Attach a policy to it 
            3. Specify which resource can assume this role

            Example: EC2 instance needs to read from S3. 
                     Default EC2 instance does not have read permisssion from S3
                     DON'T transfer credentials to EC2 to read S3
                     Create a role, attach policy to read from S3
                     Assign role to EC2 instance

            Example: Glue job needs to write to S3. 
                     Create role with S3 write perms and assign to Glue job


IAM Policy:  an object in AWS that defines the permissions of the attached user or role.
             can manage access in AWS by creating policies and attaching them to 
                users, groups, roles

             Stored in AWS as JSON doc
             Create custom policy with AWS-managed policies

IAM Documentation
https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html

ex. 
{
    "Version": "2012-10-17", 
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "s3:*",
                "s3-object-lambda:*"
            ],
            "Resource": "*"
        }
    ] 
}

Version  : specialty the version of policy language
Statement: container of the details of given perms or denials.
           can include more than one statement in a policy
           multiple statements: AWS applies locial OR across statements

    Sid:      optional statement id to differntiate between statements
    Effect:   Use Allow or Deny wheter policy allows or denies access
    Action:   list of actions policy allows / denies. 
                eg. s3:*  menas all read and write actions on S3 allowed
    Resource: object or list of objects which the actions apply. 
              i.e. can specify a specific S3 bucket or all resources "*"    

ex. 2

{
    "Version": "2012-10-17", 
    "Statement": [
        {
            Sid: "allow_statement"
            "Effect": "Allow",
            "Action": [
                "s3:*"
            ],
            "Resource": "*"
        },
        {
            Sid: "deny_statement"
            "Effect": "Deny",
            "Action": [
                "s3:DeleteBucket",
            ],
            "Resource": "arn:aws:s3:::confidential"
        }
    ] 
}

IAM / Policy Resources

https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-bucket-policies.html
https://docs.aws.amazon.com/IAM/latest/UserGuide/access.html
https://docs.aws.amazon.com/IAM/latest/UserGuide/id.html


# Basics of Networking in the Cloud

- Region considerations:
    - legacl compliance
    - latency
    - availability
    - cost

VPC:  smaller network that spans multiple availability zones within a region


# AWS Networking Overview: VPC & Subnets
                                                                              
                                                                              
   VPC                                                                
 ┌────────────────────────────────────────────────────────────┐       
 │   ┌──────────────────┐                                     │       
 │   │Availability Zone │           [172.16.0.0]              │       
 │   │                  │           [172.16.0.0]              │       
 │   │                  │           [172.16.0.0]              │       
 │   │  ┌────────────┐  │              ^                      │       
 │   │  │            │  │              |                      │       
 │   │  │   Public   │ <---->   [ACL]--+                      │       
 │   │  │            │  │              |                      │       
 │   │  │   Subnet   │  │              |                      │       
 │   │  │            │  │              |                      │ [ Internet ]      
 │   │  └────────────┘  │              +------------------>   │ [ Gateway  ] <--> Internet      
 │   │                  │              |                      │       
 │   │  ┌────────────┐  │              |                      │       
 │   │  │            │  │              |                      │       
 │   │  │   Private  │ <---->   [ACL]--+                      │       
 │   │  │            │  │              |                      │       
 │   │  │   Subnet   │  │              v                      │       
 │   │  │            │  │           [172.16.0.0]              │       
 │   │  └────────────┘  │           [172.16.1.0]              │       
 │   │                  │           [172.16.2.0]              │       
 │   └──────────────────┘                                     │       
 └────────────────────────────────────────────────────────────┘       
                                                                              
Each Subnet has it's own Network Access Control List (NACL)                                                                              
                                                                              
Routing configs in Internet Gateway                                                                              
                                                                              
                                                                              
                                                                              
# AWS Networking: VPC and Subnets

Scenario: 
    - EC2 instace running web app that queries RDS



                                    User
                                      ^                                       
                                      |                                       
       Region                         |                                       
     ┌────────────────────────────────|───────────────────────────────────┐   
     │     VPC                        |                                   │   
     │   ┌────────────────────────────|───────────────────────────────┐   │   
     │   │                            |                               │   │   
     │   │   ┌──────────────────┐     |      ┌─────────────────┐      │   │   
     │   │   │      AZ 1        │     |      │     AZ 2        │      │   │   
     │   │   │  ┌────────────┐  │     |      │  ┌───────────┐  │      │   │   
     │   │   │  │   Public   │  │     |      │  │   Public  │  │      │   │   
     │   │   │  │   Subnet   │  │     |      │  │   Subnet  │  │      │   │   
     │   │   │  │            │  │     |      │  │           │  │      │   │   
     │   │   │  │ [  NAT   ] │  │     |      │  │ [  NAT  ] │  │      │   │   
     │   │   │  │ [Gateway ] │  │     |      │  │ [Gateway] │  │      │   │   
     │   │   │  └────────────┘  │     |      │  └───────────┘  │      │   │   
     │   │   │                  │     |      │                 │      │   │   
     │   │   │  ┌────────────┐  │     |      │  ┌───────────┐  │      │   │   
     │   │   │  │  Private   │  │     |      │  │  Private  │  │      │   │   
     │   │   │  │  Subnet    │  │     |      │  │  Subnet   │  │      │   │   
     │   │   │  │            │  │     |      │  │           │  │      │   │   
     │   │   │  │   [RDS]    │  │     v      │  │    [RDS]  │  │      │   │   
     │   │   │  │   [EC2] <--│--│---[ALB]----│--│--> [EC2]  │  │      │   │   
     │   │   │  │            │  │            │  │           │  │      │   │   
     │   │   │  └────────────┘  │            │  └───────────┘  │      │   │   
     │   │   └──────────────────┘            └─────────────────┘      │   │   
     │   └────────────────────────────────────────────────────────────┘   │   
     └────────────────────────────────────────────────────────────────────┘   
NOTE: VPC can span all AZs in a Region
        - can have multiple VPCs in a Region 
        - resources in same VPC can communicate
        - cannot by default communicate accross VPCs (needs to be configured)

NOTE: Default VPC in each Region in an AWS account 
        - includes public subnet in each AZ in region  + Internet Gateway
            -  can launch public facing EC2 instances quickly



Network creation steps:
1.   AWS Console -> VPC -> [Create VPC]
        - Name                                                                              
        - Private IP address range: 10.0.0.0/16 (CIDR: Classless Inter-Domain Rouding)
                / notation is the size of the network 
                /16 means network is 16 bits in length (first two digits in dot notation)

        - Region: i.e. US-East-1
    [Create Subnet]
        - Select VPC subnet will reside in
        - Subnet Name
        - Choose Availability Zone
        - Enter subnet CIDR block (must be a subnet of VPC)
                eg. 10.0.1.0/24

        - Add new subnet

   NOTE:  subnet created in specific AZ
            - usually 1 private and 1 public subnet in each AZ

subnet                 CIDR                 AZ
public_subnet_1       10.0.1.0/24      us-east-1a
private_subnet_1      10.0.2.0/24      us-east-1a

public_subnet_2       10.0.3.0/24      us-east-1b
private_subnet_3      10.0.4.0/24      us-east-1b


# Internet Gateway & NAT Gateway

Per scenario:
    - EC2 instances would need internet access to download updates and patches
    - need away to submit requests web app on EC2 instance (via ALB)

Attach Internet Gateway to VPC
    - allow public subnets to connect to internet


NAT Gateway: Network Address Translation Gateway
    - allow resources in private subnet to connect to 
      Internet or other AWS services

    - prevents Internet from initiating connections to those resources


Application Load Balancer (ALB): 
    - Distributes incoming application traffic across multiple backend targets
    - Entry point for external users
    - Handles load and ensures app is responsive and available
    - Keeps EC2 instances private


# Create Internet Gateway
AWS Console -> VPC -> Internet Gateways -> [Create Internet Gateway]
    - Name:
    - [Create Internet Gateway]


NOTE: A VPC can have one Internet Gateway  
        - Internet Gateway can only be attached to one VPC at aa time

# Attach Intenet Gateway to VCP
AWS Console -> VPC -> Internet Gateways -> new Internet gateway -> Actions -> [Attach to VPC]
    - select VPC from dropdown
    - [Attach Internet gateway]

# Create NAT Gateway
AWS Console -> VPC -> NAT Gateways -> [Create NAT Gateway]
    - Name:
    - select subnet from dropdown
    - connectivity type: Public
    - [ Allocate Elastic IP ]  <static IP address> 
    - [Create NAT Gateway]

NOTE: repeat for each public subnet 
      best practice, create NAT Gateway each AZ  


# Route Tables

Rotue Tables: direct network traffice within VPC
    - Each Subnet can have a route table 
    - route table is a set of rules / routes of where network traffic is directed

NOTE: when VPC created, default route table created

Default route table allows internal communication within VPC
        - resources in different subnets can communicate

Public  subnets will direct all internet bound traffic to Internet Gateway
Private subnets will direct all internet bound traffic to NAT Gateway in Public Subnet
    - private subnet resources can make outbound Internet calls
        and prvent inbound connections from Internet



# Create Route Tables
AWS Console -> VPC -> Route tables -> [Create Route Table]
    - Name:
    - VPC: [select VPC from dropdown]
    - [Create Route Table] 


# Associate Subnet to Route Table
AWS Console -> VPC -> Route tables 
    - Choose Route Table
    - Actions: [Edit Subnet Associations]  <<-- dropdown
    - select subnet
    - [Save associations]

# Create Routes
    - public  subnets to Intenet Gateway
    - private subnets to NAT     Gateway


AWS Console -> VPC -> Route tables 
    - Choose Route Table
        NOTE: choose PUBLIC route table
    - [Routes]  << Tab
    - [Edit Routes] 
    - [Add Route] 
        - Destination: 0.0.0.0/0    << ANY IP ADDRESS
        - Target: [Internet Gateway]   from dropdown
            [select Internet Gateway for this VPC]
            [save changes]

Now private subnet route table setup
    - Choose Route Table
        NOTE: choose PRIVATE route table
    - [Routes]  << Tab
    - [Edit Routes] 
    - [Add Route] 
        - Destination: 0.0.0.0/0    << ANY IP ADDRESS
        - Target: [NAT Gateway]   from dropdown
            [select NAT Gateway associated for the subnet the route table is in]
            [save changes]


# Network ACLSs and Security Groups

Security Groups: Instance level virtual firewalls 
                    control inbound and outbound traffic

            Default:  deny all inbound traffic / allow all outbound traffic


Security Groups Are Stateful:
    - allow inbound traffic to an instance automatically allows the return traffic
        DON'T NEED EXPLICIT OUTBOUND RULE
    - security groups can reference other security groups

Resources placed in VPCs use Security Groups
    - i.e. EC2, RDS, LBs



Security Group Chaining

    [ALB]                       [EC2]                           [RDS]

sec grp id: sg-123  <<------+   sec grp id: sg-456  <<----+  sec grp id: sg-789
Source    |Protocol| Port   |   Source |Protocol| Port    |  Source  |Protocol| Port
----------|--------| ----   |   -------|--------| ----    |  --------|--------| ----       
0.0.0.0/0 | HTTP   | 80     +-- sg-123 | HTTP   | 80      +-- sg-456 | TCP    | 3306           
0.0.0.0/0 | HTTPS  | 443        sg-123 | HTTPS  | 443           


# Create Security Group (for ALB)

AWS Console -> VPC -> Security -> Security Groups -> [Create Security Group]
    - Name: alb-sg
    - VPC: select from drop down 
    Inbound Rule:  [Add Rule]
        Type: HTTP
        Source: 0.0.0.0/0
    Inbound Rule:  [Add Rule]
        Type: HTTPS
        Source: 0.0.0.0/0
    [Create Security Group]


Network Access Control Lists (NACL)
    - Provide additional layer of security at subnet level
    - NACLs are STATELESS
    - inbound / outbound rules defined explicitly
    - useful for implementing security policies at subnet level

DEFAULT: NACLs allow all inbound / outbound traffic


VPC / subnets:  provide a way to define a private network on AWS

Route Table:    direct traffic within VPC to Internet 

Public Subnet:  point to Intenet Gateway

Intenet Gateway: allow resources withing public subnets to access Intenet

Private Subnet:  point to NAT Gateway

NAT Gateway:     enable instances to initiate outbound connections securely
NOTE: Route tables determine public / private access 

Security Group:  virtual firewall at instance level (EC2, RDS, LB)
                 control inbound / outbound traffic
                 they are stateful 

Network ACLs :   additional level of security at subnet level
                 stateless / explicit inbound and outbound rules
NACLs vs Security Groups:
    NACLs          @ subnet level 
    Security Group @ instance level 


Connectivity Issue Checklist:

1. Verify that VPC has Internet Gateway properly attached 

2. Verify that route tables have appropriate rules to direct traffic correctly

3. Verify that route tables associations with subnets are configured correctly
     i.e. don't want private routes associated with public subnets

4. Check security groups to make sure they have needed rules in place

5. Review network ACLs to confirm they allow necessary traffic

6. Double check instance configurations to ensure they are associated with correct 
   security groups and sbnets 




Route Table Exmple

 Customr Route Table 1
| Destination  | Target         |<--------- [Public Subnet 10.0.0/24]
| 10.0.0.0/16  | local          |
|  0.0.0.0/0   | internet-gw-id |

 Customr Route Table 2
| Destination  | Target         |<--------- [Public Subnet 10.1.0/24]
| 10.0.0.0/16  | local          |
|  0.0.0.0/0   | nat-gw-id      |


NACL: firewall that filters traffic to / from subnet
NOTE:  Rules are evaluated in ascending order (Rule 100, 110, 120, 800)


Endpoints:
    two types: private zone and public zone

    private zone services:  resources that need to be launced in a VPC 
        i.e. EC2, RDS, ELB, EFS, etc

    public zone services:  resources that DO NOT need to be launced in a VPC
        i.e. S3, DynamoDB, Lambda, Kinesis, Athena

Interface endpoints can be placed in a public or private subnet to allow resources to 
connect to AWS public resources

Gateway endpoints can be attached to a VPC to allow resources in the VPC to connect to 
S3 and DynamoDB

NOTE: S3 can also be reached using an interface endpoint but DynamoDB can only be 
      reached using gateway endpoint

Resources:
https://docs.aws.amazon.com/vpc/latest/userguide/default-vpc.html
https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Internet_Gateway.html
https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat.html
https://docs.aws.amazon.com/vpc/latest/userguide/infrastructure-security.html#VPC_Security_Comparison
https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html
https://docs.aws.amazon.com/en_us/console/ec2/security-groups/create
https://docs.aws.amazon.com/en_us/console/ec2/security-groups/reference
https://docs.aws.amazon.com/whitepapers/latest/aws-privatelink/what-are-vpc-endpoints.html
https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html





DB Connectivity Issues on AWS LAB





# RDS endpoint  por5-5432
de-c2w1a1-rds.cxo8cy4iqghl.us-east-1.rds.amazonaws.com

RDS connection:
    username: postgres
    password: postgrespwrd

## RDS endpoint from cli
de-c2w1a1-rds.cxo8cy4iqghl.us-east-1.rds.amazonaws.com

EC2 vpcid = 2e16
RDS vpcid =  de-c2w1a1 (vpc-....21f)




## Source Systems, Data Ingestion and Pipelines
## Course 2
## Week 2


Data Ingestion: get raw data from somewhere

# Data Ingestion on a Continuum


Unbounded Data: continuous stream of data (no beginning or end)

Stream ingestion:  ingest events one at a time
Batch  ingestion:  boundary on data and ingest the boundary of data as a unit
            - size threshold batch (100 MB)
            - # of records batch   (1000 )
            - time based batch (hourly, daily, weekly)
        - high frequency batch essentially becomes streaming


Ingestion Frequencies

          [Batch]           [Microbatch]        [Streaming]
    -------------------------------------------------------->>
       Semi-frequent        Frequent            Very Frequent       


File Ingestion 
    - manual file download
    - Secure File Transfer
        - SFTP
        - SCP

IoT Ingestion 

                                     Streaming Platform
[IoT] ->  [ Event Producer ]  -->>  [[ .............. ]]   -->> [ Event Consumer ]
                                       Message Queue

Batch Ingestion Tools:
    - AWS Glue ETL
        - performs ETL job using Apache Spark
            - distributes transformation workloads across compute nodes
        - serverless solution   
            - code-based ingestion and transformation

    - AWS EMR: managed platform to run Hadoop / Spark
        - can run serverless or provisioned

        Glue needs less confguration  / EMR more control 

    - AWS DMS: Data Migration Service
        - sync data from an existing database

    - AWS Snow family: transfer appliance to migrate large amounts of data
        - Snowball, Snowcone

    - AWS Transfer family: transfer files into and out of S3 using SFTP / FTP 

    - Other connectors: Airbyte, Matillion, Fivetran

Resources:
https://docs.aws.amazon.com/glue/latest/dg/components-key-concepts.html
https://docs.aws.amazon.com/prescriptive-guidance/latest/serverless-etl-aws-glue/aws-glue-etl.html
https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-what-is-emr.html

Streaming Ingestion Tools:
    - Kinesis
    - Apache Kafka MSK

Considrations Batch vs. Streaming ingestion

Use cases:  
    - what are you going to do with "real time" data over periodic batches
    - machine learning 
        - batch is good approach for training
    - dashboards / reporting:
        - benefit of "real time" dashboard. how will they act

Latency:  millisecond real-time data vs. micro batch ?

Cost   :  streaming can carry extra costs / complexities

Existing / Available system: 
        - can destination system handle real time data
        - what is impact of ingesting from live production source system 

Reliability / Availability:
        - streaming needs high availability of compute, batch does not


# ETL vs ELT

API souce -->> Batch ingetstion due to the number of call

Extract Transform Load
            - original batch ingestion proces 
            - potential data loss on transform

                      Extract       Transform       Load
       [Data Source] ----------> [Staging Area]  -------->  [Target]  

Extract Load Transform 
            - captures all data

                      Extract       Load      Transform
       [Data Source] ---------->   -------->  [Target]  

NOTE: Transform done in the target i.e. Datawarehouse

Advantages of ELT 
    - faster to implement
    - data available more quickly to end users
    - transformations can be done efficiently
        decide later to adopt different transformations 


Disadvantages:
    - can be come an Extract - Load pipeline
    - target can become a Data Swamp

EtLT: small t means transformation with limited scope
        - mask sensitive data, deduplicate rows


# REST API

API:  set of rules and specifications the allows you to programatically 
      communicate and exchange data with an application

API Features:
    - Metadata
    - Documentation
    - Authentication
    - Error handling

REST API:  Representational State Transfer API
           Use HTTP as basis for communication


## Spotify Lab

https://developer.spotify.com
https://developer.spotify.com/documentation/web-api
https://developer.spotify.com/documentation/web-api/concepts/authorization

each data item is a resource

HTTP Request        Action
GET                 retrieve a resource
POST                create a resource
PUT                 change / replace a resource
Delete              delete a resource


HTTP Request:  Endpoint + Access token

Access token: string that contains the permissions to access a given resource (valid 1 hr)
  - create spotify account
  - get client ID and client secret
        use them to generate access token


Get Playlist
Get Featured Playlist

Pagination: Extract the items chunk by chunk
    - using offset and limit

https://api.spotify,com/v1/me/shows?offset=0&limit=20
https://api.spotify,com/v1/me/shows?offset=20&limit=20
https://api.spotify,com/v1/me/shows?offset=40&limit=20
https://api.spotify,com/v1/me/shows?offset=60&limit=20
https://api.spotify,com/v1/me/shows?offset=80&limit=20

# use next field 
response.get('playlists').get('next')




authentication.py: contains scripts for get_token function returns access token

endpoint.py       1  paginated call to endpoint "Get featured playlists":.
                  2. paginated call to endpoint "Get playlist": 
                  3. TODO: generate new token when expired

main.py: get the ids of the featured playlists
         for each playlist id call second paginated call to get track info


# review these first before starting the lab:
https://developer.spotify.com
https://developer.spotify.com/documentation/web-api
https://developer.spotify.com/documentation/web-api/concepts/authorization


## Streaming Ingestion 

# Streaming Ingestion Deails


Streaming requirements from "Conversation with a Software Engineer"
-source 
-payload: session_id, products, 
   - size 100s bytes
   - rate  1000 events / second
   - ~ 1MB / sec
- data retention 
100 GB / day



Message Queue: buffer used to deliver messages asynchronously

[  Event   ]           [          Message Queue             ]           [  Event   ]
[ Producer ]  ---->>   [  { 4 } .. { 3 } .. { 2 }  .. { 1 } ]   ---->>  [ Consumer ]
                                        FIFO

NOTE: Once message read, it is deleted from queue



Event Streaming : Append-only persistent log

[  Event   ]           [          Streaming Platform        ]           [  Event   ]
[ Producer ]  ---->>   [  { 4 } .. { 3 } .. { 2 }  .. { 1 } ]   ---->>  [ Consumer ]
                                                                |
                                                                +--->>  [  Event   ]
                                                                        [ Consumer ]
NOTE: possible to replay or reprocess any events in the log


Kafka

┌────────────┐         ┌──────────────────────────────────────┐        ┌────────────┐
│  Event     │         │             Kafka Cluster            │        │  Event     │
│  Producer  │----+    │    (contains servers and brokers)    │   +--->│  Consumer  │
└────────────┘    |    │                                      │   |    └────────────┘
                  |    │  [-----]   [-----]  [-----]  [-----] │   |                  
┌────────────┐    |    │  [Topic]   [Topic]  [Topic]  [Topic] │   |    ┌────────────┐
│  Event     │    +--->│  [  1  ]   [  2  ]  [  3  ]  [  4  ] │ --+--->│  Event     │
│  Producer  │    |    │  [ IoT ]   [Alert]  [ data]  [ etc ] │   |    │  Consumer  │
└────────────┘    |    │  [-----]   [-----]  [-----]  [-----] │   |    └────────────┘
┌────────────┐    |    │                                      │   |    ┌────────────┐
│  Event     │----+    │                                      │   +--->│  Event     │
│  Producer  │         │                                      │        │  Consumer  │
└────────────┘         └──────────────────────────────────────┘        └────────────┘
           push messages                                     pull messages


Topics: Categories to hold related events

Partitions(logs): topics can be partitioned
                  Ordered immutable sequences of messages

NOTE: Event Consumers can be part of a Consumer Group with each Consumer consuming from 
a specific partition of the topic



Kinesis

┌────────────┐         ┌────────────────────────────────────────┐        ┌────────────┐
│  Event     │         │          Kinesis Data Streams          │        │  Event     │
│  Producer  │----+    │    (contains servers and brokers)      │   +--->│  Consumer  │
└────────────┘    |    │                                        │   |    └────────────┘
                  |    │ [------]  [------]  [------] [------]  │   |                  
┌────────────┐    |    │ [Stream]  [Stream]  [Stream] [Stream]  │   |    ┌────────────┐
│  Event     │    +--->│ [   1  ]  [  2   ]  [   3  ] [   4  ]  │ --+--->│  Event     │
│  Producer  │    |    │ [ IoT  ]  [Alert ]  [ data ] [  etc ]  │   |    │  Consumer  │
└────────────┘    |    │ [------]  [------]  [------] [------]  │   |    └────────────┘
┌────────────┐    |    │                                        │   |    ┌────────────┐
│  Event     │----+    │                                        │   +--->│  Event     │
│  Producer  │         │                                        │        │  Consumer  │
└────────────┘         └────────────────────────────────────────┘        └────────────┘
           push messages                                     pull messages


Stream: Categories to hold related events

Shard(logs): streams can be partitioned
             Ordered immutable sequences of messages



# Kinesis Data Streams Details



Event Producer sends data to a specific Stream
    - Stream is made up of shards (i.e. partitions)
        - Unit of Capaicity for the stream

NOTE: to scale up a stream, need to add more shards



How to size shards ?
    Need to know the size of payload and rate of read / write of pipeline

Read Operation
 - Each shard can support 5 read operations per second per shard
 - Max total read rate: 2 MB/s

Write Operation
 - Up to 1,000 records per shard
 - Max total write rate: 1 MB/s


Kineses "on-demand" Mode
    - Automatically manage the scaling of the shards 
       up or down as needed
    - Only charged for what you used
    - more conveenient


Kineses "Provisioned demand" Mode
    - Specify the number of shards necessary based on the 
       expected write and read request rate

    - Manually add more shards or re-shard when needed

    - good for more control or steady rate of system


Data Record made up of:
    - Partition Key
    - Sequence Number
    - Binary Large Object (BLOB)



Partition key used to determine which shard the data record is placed in 

Kenesis assigns a sequence number when data written to shard 
    - maintains order of record within the shard

Producer writes data to   shards
Consumer reads  data from shards
    - can have muliple consumers reading data from a shard
Shared   Fan-Out: Consumers share a shards read capacity == 2 MB/s  (default)
Enhanced Fan-Out: Consumers are able to read at full capacity of the shard (2 MB/s)


AWS Services that can process AWS Kinesis Data Streams:
    - AWS Lambda
    - AMS Apache Flink 
    - AWS Glue

Can write customer consumers using Amazaon Kinesis Client Library (KCL)

Consumers can send data to other AWS Services:
    - Firehose

# What is Change Data Capture (CDC)

Keeping data in sync across platforms:
    - Full snapshot / full load
    - incremental / differential:  only load what was updated from last run


CDC: only load data that has been inserted / updated since last run

Use cases: 
    - synchronize data from OLTP to Data Warehouse
    - capture on prem and apply to cloud databases
    - capture every historical change for auditing
    - enables microservices to track any change to source database 


Two CDC approaches:
    Push:
        - process to capture change on source db
        - source then pushes data updates to target
        - target updated with the latest data
        - can risk data loss if target unavailable

    Pull:
        - target continuously polls source db for changes
            - pulls in updates when that happens
        - usually results in a lag between updates

# CDC Implementation Patterns

    Batch-oriented or query-based CDC (pull-based): 
        query dbatabase to identify change in data 
        requires a field as : updated_at, last_update, last_modified
        identify updated rows since cetain specified time
        Computation intensive on target because each row scanned

    Continuous or log-based CDC (pull-based): 
        treat each updated to the db as an event using continuous CDC
        Database logs checked. DB logs every change to DB sequentially.  
        Can read events in DB log with Debezium and send them streaming to Kafka.
        No computation overhead of DB. 


    Trigger-based CDC (push-based): 
        Configure a trigger on specific column changes.  Trigger informs
        CDC of changes in source DB, relieves CDC from detecting change. 
        Caveat: too many triggers can negatively impact the write performance 
        of the source DB.


CDC Tools:
https://debezium.io/
https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Task.CDC.html
https://limadelrey.medium.com/kafka-connect-how-to-create-a-real-time-data-pipeline-using-change-data-capture-cdc-c60e06e5306a
https://airbyte.com/solutions/database-replication


# General Considerations for choosing ingestion tools

    - Data type and structure: 
        need to know type and structure so appropriate ingestion tool and 
        transformations can be handled

    - Data Volume:
        byte size of of existing data to ingest
        consider size of historical data
        for streaming ingestion, need to consider the message size.  
            Kinesis Data Streams supports max message size of 1MB
            Kafka defaults to 1 MB payload size but can be configured to 20 MB or more.
        what is size of future data will be.
        what is daily, monthly, yearly growth of data


    - Latency Requirements:
        daily ingestion or near real time
        how fast to process the ingested data once it reaches the pipeline 
        how quickly source data is generated
        CRITICAL:  Velocity of data will decide tools

    - Data quality 
        - can source data be used immediately at target
        - what post processing has to be done
        - data quality needs to be checked

    - Changes in schema:
        - need to detect and handle schama changes
        NOTE: Coordinate with upstream stakeholders

    - Reliability and Durability:
        reliability: systems functioning as designed
        durability : No data loss or corruption

NOTE: Evaluate tradeoff between the cost of losing data vs. building appropriate 
      level of redundancy.


LAB
de_c2w2lab1

# create buckets
!aws s3 ls
2024-09-29 14:46:20 de-c2w2lab1-471112674614-int
2024-09-29 14:46:19 de-c2w2lab1-471112674614-usa


# check if a stream is ready 
def is_stream_ready(stream_name: str) -> None:
    client = boto3.client("kinesis")
    response = client.describe_stream(StreamName=stream_name)
    return response["StreamDescription"]["StreamStatus"] == "ACTIVE"

# Check if the streams are ready
print(is_stream_ready(stream_name=USA_DATA_STREAM))
print(is_stream_ready(stream_name=INTERNATIONAL_DATA_STREAM))

# Week 2 Resources 

Kinesis: 
https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html#high-level-architecture
https://docs.aws.amazon.com/streams/latest/dev/service-sizes-and-limits.html


Kafka: 
https://kafka.apache.org/081/documentation.html#introduction
https://www.gentlydownthe.stream/

https://www.conduktor.io/blog/apache-kafka-vs-amazon-kinesis-comparing-across-five-dimensions/



## Source Systems, Data Ingestion and Pipelines
## Course 2
## Week 3

## DataOps Automation


DataOps:  Set of practices and cultural habits centered around building 
          robust data systems and deliverying high quality data products.


Three Pillars of DataOps:

    [ __________________ Data Ops ________________ ]
    [ Automation ]  [ Observability ]   [ Incident ]
                    [      &        ]   [ Resonse  ]
                    [ Monitoring    ]


Automation 
-------------

Continuous Ingetration:  build -> test -> integrate -> deploy 
and  Delivery  (CI/CD)       


Infrastructure as code:   Code that acts to deploy the resources required to 
                          run your data pipeline

Chris Burgh - Data Kitchen / DataOps

    - delivery data insights to customers
    - similar to LEAN
        "you are running a factory of data"
        "people are always asking for new things"
            - get it 80% done and then iterate 
                - feedback early


    test the things you produce 
        i.e. SQL >> how do you test to prove it works

DataOps Manifesto 


DataOps: 
    - Rapid innovation and experimentation delivering new insights to customers 
      with increasing velocity
    - Extremely high data quality and very low error rates
    - Collaboration across complex arrays of people, technology and environments
    - Clear measurement, monitoring and transparency of results


Deliver small chunks of work, small cycle times


# DataOps Automation

DevOps automation:
Continuous Ingetration and Delivery (CI/CD):  
    [ build ]  --->  [ test ]  --->  [ integrate ]  --->  [ deploy ] 


DataOps Automation:
    - CI/CD can be applied to code and data within your data pipeline 
        - code for database population
        - code for transformations
        - integrate new data

    - Automation of data pipeline running
        scheduling :  run stages of pipeline through a scheduler
        orchestrate:  utilize a directed acyclic graph (DAG) i.e. Airflow

    - Version control (git)
        - track code and data

    - Infrastructure as Code
        - maintain design of infrastructure as a codebase (Terraform)
        - version control your infrastructure

DataOps practices are borrowed from DevOps

    [Data Management] <-> [DataOps] <-> [Software Engineering]



# Infrastructure as code 

Roots in configuration management:

1970s - Conf Management     
        Chanllenge: config a serires of physical machines
        Solution  : use bash scripts to automate

2006 - AWS EC2

2010s- Development of IaC Tools 
            - AWS CloudFormation
            - Terraform
            - Ansible

Hashicorp Configuration Language (HCL)

# create / update vpc
resource "aws_vpc" "main" {
    cidr_block       = "10.0.0.0/16"
    instance_tenancy = "default"

    tags = {
        Name = "main"
    }
}

# create / update ec2 instance

resource "aws_instance" "web" {
    ami           = data.aws_ami.ubuntu.id
    instance_type = "t3.micro"

    tags = {
        Name = "HelloWorld"
    }
}

## GCP instance

resource "google_compute_instance" "default" {
    name         = "my-instance"
    machine_type = "n2-standard-2"
    zone         = "us-central1-a"

    tags = ["foo", "bar"]
    boot_disk {
        initialize_params {
            image = "debian-cloud/debian-11"
            labels = {
                my_label = "value"
            }
        }
    }
}

HCL is declaritive language:
    - just declare what infra looks like
    - desired end-state of the infrastructure
        - resources and configs
    - terraform will figure out intermediate steps to get to end state

    Highly idempotent
    - will make updates only to get to end state


# Terraform - Creating and EC2 instance

Terraform workflow: 

    1. Write config files to define resources

    2. Terraform prepares workspace
        a. installs files to communicate to cloud apis 
        b. creates exeuction plan
                create / update / destry
    
    3. User approves execution plan

    4. Terraform applies proposed steps


Target Infrastructure:

    +---------------------+
    | Region (us-east-1)  |
    |  +---------------+  | 
    |  | Default VPC   |  |
    |  |  +---------+  |  |
    |  |  |         |  |  |
    |  |  |   EC2   |  |  |
    |  |  |         |  |  |
    |  |  +---------+  |  |
    |  +---------------+  | 
    +---------------------+


5 sections to .tf file

# terraform settings
# providers
# resources
# input
# output

block of terraform code

keyword  labels {
    arguments 
    blocks
}

## See terraform/main.tf

provider is a plugin or binary file that Terraform will use to 
create your resources

for each provider need to provide:
    local  name
    source  location
    version constraint

can specifiy version constraint for terraform in terraform block 
  required_version = ">= 1.2.0"

# resources 
specify resouce followed by resource type and name; provider and resource separated by underscore
i.e. resource "aws_instance" "webserver" 

NOTE: instance and name is a unique name of resource that can be referenced in other blocks


# Steps to instantiate resource 
    terraform init          # installs provider in config file
                            # downloads aws provider and stores in .terraform
    terraform plan          # creates execution plan on what is planning to create / update / destroy 

    terraform apply         # shows plan again and then asks for approval 'yes'

    terraform outputs       # shows outputs
    terraform destroy       # destroy objects that were created, asks for approval 'yes'


# Terraform - Defining Variables and Outputs

variables created with variable keyword (duh)
   with three optional arguments: description, type, default

NOTE: if variable does not have a default then terraform 
      will prompt for one before creating

variable "region"  {
  description = "region for aws resources"
  type = string
  default = "us-east-1"

}

# output
variable "server_name"{
  description = "name of the server running the website"
  type = string

}


to access variable value use var.variable name 
i.e. var.region or var.server_name

two ways to initiate variables 

1. pass on cli
    terraform apply -var server_name=ExampleServer

2. add variable values in "*.tfvars"file 
    i.e. terraform.tfvars


To update with variables file use: 
    terraform apply

print out attributes of a resource using output values
    i.e. public_dns

    get via resource_type.resource_name.attribute

use terraform apply to add output variable
i.e. 

terraform apply

query outputs via 

terraform outputs 
or 
terraform outputs  output_name 
i.e. 
terraform outputs  server_arn
 
For sanity split main.tf to:
outputs.tf  :  contains all output variables # output
variables.tf:  contains all input  variables # input
providers.tf:  contains all providers  # providers
main.tf:       contains all resources  # resources
        NOTE: main.tf can be split by resources

Terraform will concatenate all .tf files 


# Terraform - Defining Data Sources and Modules


Data sources:
  Data blocks to reference resources created outside Terraform or in another 
  Terraform workspace

situation:
    launch EC2 instance in a public subnet in a VPC that was created outside the workspace

use data block to setup resources or query aws for info

data "aws_subnet" "selected_subnet" {
    id = "subnet-0597ece886ea42f19"
}

data "aws_ami" "latest_amazon_linux" {
    most_recent = true
    owners = ["amazon"]
    filter {
        name = "architecture"
        value=  ["x86_64"]
    } 
    filter {
        name = "name"
        valuevalues =  ["a1201*-ami-202*"]
    }
}

Modules: 
    a subdirectory inside main directory to group resources together
    For example, created website directory and put main.tf there

NOTE:  need to move outputs.tf, variables and main.tf to website directory
        because variables etc. are in website directory they are hidden from 
        root directory.  Fix this by adding main.tf in in root with a module reference 
        to source in website. 
main.tf:
module "website" {
    source = "./website"
}         


NOTE: whenever module blocks are added, removed, modified NEED TO rerun terraform init 
      to update changes to modules





Lab


de-c2w3lab1

bastion_host_dns = "ec2-100-25-255-135.compute-1.amazonaws.com"
db_host = "de-c2w3lab1-db.czc8w6gekhix.us-east-1.rds.amazonaws.com"
db_master_password = 
db_master_username = "postgres_admin"
db_port = 5432
project = "de-c2w3lab1"

terraform output db_host  "de-c2w3lab1-db.czc8w6gekhix.us-east-1.rds.amazonaws.com"   
terraform output db_port   5432

terraform output db_master_password    "
terraform output bastion_host_dns

NOTE: Terraform created an RDS instance and EC2 bastion host.
From the bastion host, able to ssh tunnel to to the RDS.postgresql instance with the below command 
Then able to run psql on bastion and have it connect to RDS via tunnel

## tunnel
ssh -i de-c2w3lab1-bastion-host-key.pem -L 5432:<RDS-HOST>:<DATABASE-PORT> ec2-user@<BASTION-HOST-DNS> -N -f
psql -h localhost -U postgres_admin -p 5432 -d postgres --password

Optional Notes:

    The ssh command uses the private key you generated using Terraform to authenticate the connection to the bastion host; the -i tag means identity_file and it specifies the file that contains the SSH private key.
    The L options means local port forwarding (-L LOCAL_PORT:DESTINATION_HOSTNAME:DESTINATION_PORT USER@SERVER_IP): this means that you're forwarding the connection from the bastion host to the RDS instance.
    The -f option means the command can be run in the background and -N means "do not execute a remote command".
    If you'd like learn more about SSH tunneling, you can check this article.

https://linuxize.com/post/how-to-setup-ssh-tunneling/#remote-port-forwarding



## Source Systems, Data Ingestion and Pipelines
## Course 2
## Week 3

## DataOps Observability


High Quality Data:                                  Low Quality Data:
 - Accurate                                             - Inaccurate 
 - Complete                                             - Incomplete 
 - Discoverable                                         - Hard to find
 - Available in a timely manner                         - Late

 Exactly what stakeholders expect:
    - well-defined schema
    - data definitions

Barr Moses 
Co-founder & CEO of Monte Carlo
Data Observability

    - Data State:
        - Data
        - Code
        - Infrastructure

    - KPI 
        - data downtime
        - time to detection
        - time to resolution


    - Is the data up-to-date
    - is the data complete
    - are fields within expected ranges
    - is the null rate higher or lower than it should be
    - has schema changed

Pillars:
    - Distribution / Internal quality
        % of nulls, % of unique elements, summary statistics, within expected range

    - Freshness
        how up-to-date is data, when last updated, how frequently updated 

    - Volume
        amount ingested and checking for dpikes or drops

    - Lineage
        trace data from source to destination, how transformed and where stored

    - Schema
        Data schema refers to monitoring changes to data structure

# Monitoring Data Quality

Data Quality Metrics:
    - Volume: Total # of Recs ingested in each batch or over some time interval
    - Distribution: range of values in a particular columns stays within a threshold 
    - Null values : Total number of null values in a table 
    - Freshness: difference between now and the most recent record in your data

NOTE: IDENTIFY MOST IMPORTANT METRICS 
    What do stakeholders care about

Testing Ingested Data
    - build in checks / tests to verify that shema and data types are consistent


Abe Gong 
Co-founder & CEO of Great Expectations
greatexpectations.io

Data Quality: is data fit for purpose

Stakeholders are key

Great Expectations
worfklow:
    1. Spcity data
    2. Define your expectations
    3. Validate your data against the expectations

Data Context:
    Instantiate a data context object
    entry point to Great Expectations API

Data Source:
    declare the data source
        - SQL DB, local file system, S3 bucket Pandas DataFrame 
    declare data asset 
        - collections of records within a data source
        - table in SQL db, file, join query, collections of file matching a pattern
    Batches 
        partition data from data asset
           - by date, column values 
        create a batch_request object
            - primary way to retrieve data from the data asset
            - need to provide batch_request object to the rest of ge components

Expectations:
    define expectations
    Expectation: statement that you can use to verify if your data meets certain conditions 
    predefined tests:
        expect_column_min_to_be_between
        expect_column_values_to_be_unique
        expect_column_values_to_be_null
    Expectation Suite: group expectation tests

Validator: 
    expects a batch_request and corresponding expectation suite 
    - manual interaction 
    - automate via checkpoint object

Checkpoints
    create checkpoint: takes batch_request and expectation suite and provides to Validator 
    

Metadata saved to backend stores
    - Expectation store
        - expectation suites
    - Validation store
        - objects created when validating data against the expectation squite 
    - Checkpoint store
        - checkpoint configs
    - Data docs store
        - reports on expectations
        - checkpoints
        - validation results
    Stores available via data conext object


Steps: 
    pip install great-expectations
    great_expectations init: start project
    launch jupyter notebook in great-expectations directory
        > jupyter notebook

great expectations lab




Chad Sanderson 
CEO gable.ai 

Data Contracts
    - interface for data, api for data
    - interface is a spec (specification)
        - structure: schema / logic 
        - data     : data quality rules, compliance, etc.

communication vs. techonology 
    - integration tests as enforcement
    - alerts on breaches


Data contract is enforced in the developer workflow
    - i.e. DevSecOps

source and consumer do not communicate on changes



## Source Systems, Data Ingestion and Pipelines
## Course 2
## Week 3


# Amazon CloudWatch


Amazon services will automatically start writing to CloudWatch 
     - Each AWS Service posts different metrics 

System Level Metrics:
 - CPU utilization
 - Disk I/O
 - Network traffic
 - Memory usage

Custom Metrics: monitor specific attributes of your application outside of system metrics
 - Number of txns processed
 - Response time of an API endpoint
 - Number of active users

CloudWatch Dashobards: visualize / monitor important metrics

CloudWatch Alarms: 
    - define thresholds for metrics
    - establish a baseline
        - measure performance of system under different loads and conditions

CloudWatch retains metrics data for up to 15 months


Common RDS Metrics:
    CPU Utilization
        - high value: instance might be under heavy load: scale up or optimize queries
        - values over 80 - 90% can lead to bottlenecks

    RAM Consumption
        - high RAM consumption can slow performance

    Disk Space: 
        - value consistently above 85%: may need to delete / archive data

    Database Connections: 
        - # of connections approaching max limit can lead to: 
            - connection errors
            - application failures

Third Party Monitoring:
    - DataDog
    - Splunk

Amazon CloudWatch lab

resources: 
https://developer.hashicorp.com/terraform/docs
https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/Welcome.html
https://docs.greatexpectations.io/docs/oss/
https://medium.com/@mikldd/incident-management-for-data-teams-5a14acd4e3d8
https://cloud.google.com/docs/security/incident-response
https://learning.oreilly.com/library/view/data-quality-fundamentals/9781098112035/
https://dataproducts.substack.com/p/the-rise-of-data-contracts
https://learning.oreilly.com/library/view/driving-data-quality/9781837635009/B19790_02.xhtml#_idParaDest-34
https://www.gable.ai/blog/data-contracts

## Source Systems, Data Ingestion and Pipelines
## Course 2
## Week 4


Orchestration, Monitoring and Automating Data Pipelines

Apache Airflow: pipeline as code

# Before Orchestration
    - cron
    1 8 4 2 0   command to be executed
    ^ ^ ^ ^ ^
    | | | | |
    | | | | +---- Week (0=Sun ... 6=Saturday) 
    | | | +------ Month (1 - 12)
    | | +-------- Day   (1 - 31)
    | +---------- hour  (0 - 23)
    +------------ Min   (0 - 59)

NOTE: * = no restriction on that value 


schedule a cron every night at midnight, python REST API
0 0 * * *   python ingest_from_rest_api.py       ## every midnight ingest from REST API
0 1 * * *   python transform_api_data.py         ## every 1 AM transform data from REST
0 0 * * *   python ingest_from_database.py       ## every midnight ingest from DB
0 2 * * *   python combine_api_and_database.py   ## every 2 AM combine API and DB data

## Failure is not good in a system like this


# Evolution of Orchestration Tools


late 2000s  Dataswarm (facebook)
2010s       Apache Oozie 
                - designed for hadoop cluster
                - difficult to use in heterogeneous environment
2014        Airflow (airbnb) / Open source project
2019        Airflow full Apache project



Advantages and Challenges of Airflow
Advantages:
    - written in python
    - very active project
    - available as a managed service:
        - AWS, GCP, astronomer.io

Challenges:
    - scalability challenge
    - ensuring data integrity
    - no support for streaming pipelines

Other Open Source Orchestration tools:
- luigi
- Conductor
- Prefect:  more scalable orchestration solution than Airflow
- dagster:  built-in capabilities for data quality testing
- Mage:     built-in capabilities for data quality testing


# Orchestration Basics







.