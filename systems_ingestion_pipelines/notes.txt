
## Source Systems, Data Ingestion and Pipelines
## Course 2
## Week 1

## Introduction to Source Systems


# Types of Source Systems


Structured Data     :  organized as table of rows and columns 
Semi-Structured Data:  Data that is not in tabular form but still has some structure
            i.e. JSON, XML

Unstructured Data:  Data that does not have any predefined structure 
        text, video, audio, images


General types of source systems

    Databases                   Files               Streaming Systems

stored in organized way      sequence of bytes       continuous flow of data

structured                     text                   semi-structured
semi-structured                images
                               audio                 [producer] -> [Message Q, streaming platform]
                               csv                                      |
   CRUD                                                                 +-->> [consumer] 
   DBMS                     structured                  IoT
    Relational              semi-structured             
    NoSQL                   unstructured



# Relational Databases
 
  - Online Transaction Processing: OLTP
  - Primary Key + Foreign Key


Data Normalization: 
    - minimize redundancy
    - ensure data integrity

                                    SQL Commands
                                         |
      +----------------------+-----------------------+---------------------+                                   
      |                      |                       |                     |
      |                      |                       |                     |
     Data                   Data                    Data                  Data  
    Cleaning               Joining               Aggregating            Filtering

    DROP                   INNER JOIN               SUM                  WHERE    
  TRUNCATE                 LEFT JOIN                AVG                   AND    
    TRIM                  RIGHT JOIN               COUNT                  OR    
   REPLACE                 FULL JOIN                MAX                   IS NULL
   SELECT DISTINCT           UNION                  MIN                   IS NOT NULL
                                                   GROUP BY               IN 
                                                                          LIKE 


# SQL Queries 
 

# NoSQL Databases 

NoSQL == Not Only SQL

- non-tabular values 
    - key value
    - document 
    - graph
    - wide column

- no predefined schemas
- more flexibility w/ data


Horizontal Scaling 
                                +---------> Secondary   (Eventual consistency: data read may not be up to date)
                                |                           
            client  ---->> [NoSQL DB]  ---> Primary


        NoSQL DB                        Relational DB
     -------------                    ----------------   
    Eventual Consistency              Strong Consistency

     speed is prioritized            read data only when all nodes
     system availability and           have been updated
      scalability are important

Not All NoSQL databases guarantee ACID:
    Atomicity
    Consistency         NOTE: mongoDB is ACID compliant
    Isolation
    Durability


Key-Value Database:
    - fast lookup: i.e. user session data

Document Store:
    - data stored in JSON-like documents
    - each document as unique key
    - documents organized into collections
        document    ==  row
        collection  ==  table

    - all information stored in one document
    - document stores don't support joins
    - flexible scheam

    use cases: content management, catalogs, sensor readings

NOTE: document databases can become a nightmare to manage and query


# Database ACID Compliance

    Relational Databases                NoSQL Databases
     Atomicity                           Not ACID compliant by default
     Consistency                         
     Isolation                           NOTE: mongoDB is ACID compliant
     Durability

ensures transactions are
processed reliably and 
accurately in an OLTP system

Atomicity:   transactions are atomic, treated as single indivisible unit
                all or no part of a transaction is done

Consistency: changes to data within a transaction follow the set of rules 
             or constraints defined by database schema

Isolation:   each transaction is executed independently in sequential order

Durability:  Once transaction completed, its effects are permanent and will survive 
             any subsequent system failures. (i.e. power loss)

ACID Principle guarantees that a database will maintain a consistent picture of the world
        - Strong Consistency:  Data consisten across the entire network 


# Interacting with DynamoDB

DynamoDB: Key-value Database
        - row has attributes of one item 
        - identified by a key
        - simple primary key == partition key
        - composite primary key == partition key + sort key
        - schemaless

Python Boto3:  python package that allows you to interact with AWS services

    CREATE:       create_table
    READ  :       scan / get_item / query
    UPDATE:       put_item /  write_batch_items /  update_item
    DELETE:       delete_item
                   
import boto3 
client = boto3.client('dynamodb')

KeySchema=[
   {
        'AttributeName': 'ForumName',
        'KeyType': 'HASH'
   },
   {
        'AttributeName': 'Subject',
        'KeyType': 'RANGE'
   },
]
HASH  == partition key
RANGE == sort key

def put_item_db( table_name: str, item: Dict[str, Any], **kwargs):
    ### START CODE HERE ### (~ 2 lines of code)
    client = boto3.client("dynamodb")
    response = client.put_item(TableName=table_name, Item=item, **kwargs)
    ### END CODE HERE ###

    return response

for dynamodb_tab in [product_catalog_table, thread_table]:
    file_name = dynamodb_tab['table_name'].split('-')[-1]    
    items = read_data(file_path=f'./data/aws_sample_data/{file_name}.json')
    
    for item in items[dynamodb_tab["table_name"]]:
        put_item_db(table_name=dynamodb_tab["table_name"], item=item['PutRequest']['Item'])

def batch_write_item_db(items: Dict[str, Any], **kwargs):
    ### START CODE HERE ### (~ 2 lines of code)
    client = boto3.client("dynamodb")
    response = client.batch_write_item(RequestItems=items, **kwargs)
    ### END CODE HERE ###
    
    return response

for dynamodb_tab in [reply_table, forum_table]:
    file_name = dynamodb_tab['table_name'].split('-')[-1]    
    items = read_data(file_path=f'./data/aws_sample_data/{file_name}.json')
    response = batch_write_item_db(items=items)
    print(response)


Queried data for table de-c2w1-dynamodb-ProductCatalog:

Read Capacity Unit  / consistent vs. strongly consistent reads: 

The previous request consumed 1.0 RCU because this item is less than 4KB. 
(RCU stands for Read Capacity Unit: "One read capacity unit represents one strongly consistent read per second, 
 or two eventually consistent reads per second, for an item up to 4 KB in size", reference).



The DynamoDB DeleteItem() method is used to delete an item. Deletes in DynamoDB are singleton operations. 
There is no single command you can run that would delete all the rows in the table. 



# Object Store

 - file treated as an individual object
 - object storage has no hierarchy
 - any type of data
    - semi-structured and unstructured
    - serving data for training ML models

 - each object has:
    -  Universal Unique Identifier (UUID key)
    -  Metadata: creation_date, file type, owner

NOTE: after initial write object is immutable
        - no random write or append

    - A write requires a new write with UUID pointing to new object
    - Can enable versioning: 
        - write metadata which specified version

Why Use Object Storage
    - store files of various formats without a file system structure
    - easily scale out to provide virtually limitless storage space
    - replicate data across several availability zones

        S3: 99.999999999% data durability (11 9's)

    - cheaper than other storage options
    - ideal for data lakes and data lakehouses


Object Store Lab

s3_client.create_bucket(Bucket=bucket_name)
s3_client.upload_file(local_file_path, bucket_name, object_key)
s3_client.select_object_content(bucket_name, object_key)
s3_client.download_file(bucket_name, object_key, local_file_path)

NOTE: can apply a SQL expression to apply on the return from select_object_content()

# aws cli 
aws s3 ls
aws s3 ls de-c2w1lab3-265729830485/csv/

# set query parameters
# 
file_s3_key = 'csv/ratings_ml_training_dataset.csv'
kwargs = {'ExpressionType': 'SQL',
          'Expression': """SELECT * FROM s3object AS s WHERE s.\"productline\" = 'Trains' LIMIT 20""",
          'InputSerialization': {'CSV': {"FileHeaderInfo": "Use"}, 'CompressionType': 'NONE'},
          'OutputSerialization': {'CSV': {}},
}

response = s3_select_object_content(bucket_name=BUCKET_NAME, object_key=file_s3_key, **kwargs)


NOTE: by default S3 buckets and objects in it are private

# below updates bucket's public access settings
# 
    s3_client.put_public_access_block(
        Bucket=bucket_name,
        PublicAccessBlockConfiguration=public_access_block_configuration
    )

# Define the public access settings  
public_access_configuration = {
    'BlockPublicAcls': False,
    'IgnorePublicAcls': False,
    'BlockPublicPolicy': False,
    'RestrictPublicBuckets': False
}

s3_public_access_setup(bucket_name=BUCKET_NAME,  
                       public_access_block_configuration=public_access_configuration)
                       
# apply a policy to a bucket
response = s3_client.put_bucket_policy(Bucket=bucket_name, Policy=json.dumps(policy))

# policy to the bucket to allow anyone on the internet to have reading access to the 
#  objects whose key starts with images/

policy = { 
    "Version": "2012-10-17", 
    "Statement": [
        {
            "Effect": "Allow",
            "Principal": "*",
            "Action": "s3:GetObject",
            "Resource": f"arn:aws:s3:::{BUCKET_NAME}/images/*"
        }
    ]
}

# update bucket versioning
# 
response = s3_client.put_bucket_versioning(
        Bucket=bucket_name,
        VersioningConfiguration=versioning_config
    )

# versioning config
# 
versioning_config = {'Status': 'Enabled'}

response = configure_bucket_versioning(bucket_name=BUCKET_NAME, 
                                       versioning_config=versioning_config)


## S3 select_object_content
https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3/client/select_object_content.html

## S3 query data
https://aws.amazon.com/blogs/storage/querying-data-without-servers-or-databases-using-amazon-s3-select/


## S3 pub_public_access_block
https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3/client/put_public_access_block.html




# Logs

Log: apend only sequence of records ordered by time, capturing info about events

       Rich data source                  downstream use case
    ------------------------          ------------------------
    Web Server Logs        --------->>     Analysis of user behavior patterns
    Database Sysetem Logs  --------->>     Track changes in source database
    Security Sysetem Logs  --------->>     Machine Learning anomaly detection


Log Level:  tag to categorize the event
    - debug         - info
    - warn          - error
    - fatal          

# Streaming Systems



Event:
Something happened in the world or change to a system

Message:
A record of info about an event

Stream: 
A sequence of messages



Straming System Components


[  Event   ]  ----->>   [   Event Router /  ]  --------->>  [  Event   ]
[ Producer ]            [  Streaking Broker ]     |         [ Consumer ]
                                                  |
 - IoT                    - acts as buffer        |         [  Event   ]
 - Mobile App               to filter and         +=---->>  [ Consumer ]
 - API                      distribute messages
 - Website                - decouples producer 
                            and consumer

                            - Message Queues  (FIFO / SQS)
                            - Event Streaming (Append only log: Kinesis, Kafka)




## Source Systems, Data Ingestion and Pipelines
## Course 2
## Week 1

## Connecting to Source Systems


- boto3
- cloud9
- jupyter

- API Connector 
    - JDBC
    - ODBC

# Connecting to an Amazon RDS MySQL Database

https://aws.amazon.com/getting-started/hands-on/create-mysql-db/

# Access RDS via CloudShell

## MySql Connect
https://dev.mysql.com/doc/refman/8.0/en/connecting.html

## Pgsql Connect
https://www.postgresql.org/docs/9.1/app-psql.html



mysql --host=[hostname]            --port=[port number] 
      --user=[database user name]  --password=[database user password]

mysql> show databases;
mysql> use  <db_name>;
mysql> show tables;
mysql> select * from <tbl_name>;
mysql> exit


get the endpoint and port via cli

aws rds describe-db-instances --filters "Name=engine,Values=mysql" --query "*[].[DBInstanceIdentifier,Endpoint.Address,Endpoint.Port,MasterUsername]"


# connect via python
import boto3

access_key_id="AXXXXXXXXXXXXH"
secret_access_key="bXXXXXXXXXXXXZ"
region_name = "us-east-1"

session = boto3.Session(aws_access_key_id=access_key_id, aws_secret_access_key=secret_access_key, region_name=region_name) 
rds=session.client("rds)
dbInstance = rds.describe_db_instances()['DBInstances'[0]]

import pymysql

try: 
    conn = pymysql.connect(host=ENDPOINT, user=USER, passwd=toekn, port=PORT, database=DBNAME)
    cur = conn.cursor()
    cur.execute("""SELECT * from pet""")
    query_results = curr.fetchall()
    print(query_results)
 except Exception as e:
    print("Database connection failed due to {}".format(e))   


# Basics of IAM and Permissions


IAM: framework for managing permissions
    - permissions define which actions an identity (person, app)
      can perform on a specific set of resources



AWS Identity and Access Management


  - Policies used to grant identities access to resources

                                              Resources
[------------]                              [------------]                          
[            ]                              [    [S3]    ]       
[ Identities ]  --->>  [Policies ] --->>    [    [RDS]   ]
[            ]                              [    [EC2]   ]       
[------------]                              [------------]                          

Types of Identities:
Root User:   Has unrestricted access to all resources

IAM  User:   Has specific permissions to certain resources
                - Username and password
                - Access Key

IAM  Group:  A collection of users that inherit the same permission  
             from the group policy
                    i.e.  DB Users or DB Admins

IAM  Role :  A user, application or services that's been granted 
             temporary permissions


        [EC2]     -----XX---->   [S3]
        EC2 be default cannot access S3


    [ROLE] [EC2]     ----------->   [S3]
        Role allows EC2 instance to access S3



# Basics of AWS IAM 

IAM: web servcies that helps you manage and securely control access to AWS resources and services.


Root User:  full access to all AWS resources and services in an account

IAM User:   person or service that interacts with AWS resources.
                - define what resources IAM user can access
                - what actions they can perform
                - credentials generated:
                    - username / password
                    - access keys for apps
                - grant access to AWS resources via policies

Policy:     specifies what actions are allowed or denied for a resource
                - read only, write only, full access
            policy can be attached to several users
            a user can have several policies

When a request is made, AWS evaluates policies to determine if request is allowed

IAM Group:  collection of users, can attach policy to group i.e. Data Scientists or DBAs
                - each user in group inherit's group's permissions 
                - groups can have multiple users
                - user can have no group, one, group or multiple groups (up to 10)
                NOTE: groups CANNOT be nested

IAM Role:   specific permissions with short-term credentials
            Roles can be assumed by entities: people, applications, AWS resources 
            No long-term credentials:  temporary security credentials provided 
               for duration of the role sessions.
            1. Create IAM role   
            2. Attach a policy to it 
            3. Specify which resource can assume this role

            Example: EC2 instance needs to read from S3. 
                     Default EC2 instance does not have read permisssion from S3
                     DON'T transfer credentials to EC2 to read S3
                     Create a role, attach policy to read from S3
                     Assign role to EC2 instance

            Example: Glue job needs to write to S3. 
                     Create role with S3 write perms and assign to Glue job


IAM Policy:  an object in AWS that defines the permissions of the attached user or role.
             can manage access in AWS by creating policies and attaching them to 
                users, groups, roles

             Stored in AWS as JSON doc
             Create custom policy with AWS-managed policies

IAM Documentation
https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html

ex. 
{
    "Version": "2012-10-17", 
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "s3:*",
                "s3-object-lambda:*"
            ],
            "Resource": "*"
        }
    ] 
}

Version  : specialty the version of policy language
Statement: container of the details of given perms or denials.
           can include more than one statement in a policy
           multiple statements: AWS applies locial OR across statements

    Sid:      optional statement id to differntiate between statements
    Effect:   Use Allow or Deny wheter policy allows or denies access
    Action:   list of actions policy allows / denies. 
                eg. s3:*  menas all read and write actions on S3 allowed
    Resource: object or list of objects which the actions apply. 
              i.e. can specify a specific S3 bucket or all resources "*"    

ex. 2

{
    "Version": "2012-10-17", 
    "Statement": [
        {
            Sid: "allow_statement"
            "Effect": "Allow",
            "Action": [
                "s3:*"
            ],
            "Resource": "*"
        },
        {
            Sid: "deny_statement"
            "Effect": "Deny",
            "Action": [
                "s3:DeleteBucket",
            ],
            "Resource": "arn:aws:s3:::confidential"
        }
    ] 
}

IAM / Policy Resources

https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-bucket-policies.html
https://docs.aws.amazon.com/IAM/latest/UserGuide/access.html
https://docs.aws.amazon.com/IAM/latest/UserGuide/id.html


# Basics of Networking in the Cloud

- Region considerations:
    - legacl compliance
    - latency
    - availability
    - cost

VPC:  smaller network that spans multiple availability zones within a region



                                                                              
                                                                              
   VPC                                                                
 ┌────────────────────────────────────────────────────────────┐       
 │   ┌──────────────────┐                                     │       
 │   │Availability Zone │           [172.16.0.0]              │       
 │   │                  │           [172.16.0.0]              │       
 │   │                  │           [172.16.0.0]              │       
 │   │  ┌────────────┐  │              ^                      │       
 │   │  │            │  │              |                      │       
 │   │  │   Public   │ <---->   [ACL]--+                      │       
 │   │  │            │  │              |                      │       
 │   │  │   Subnet   │  │              |                      │       
 │   │  │            │  │              |                      │ [ Internet ]      
 │   │  └────────────┘  │              +------------------>   │ [ Gateway  ] <--> Internet      
 │   │                  │              |                      │       
 │   │  ┌────────────┐  │              |                      │       
 │   │  │            │  │              |                      │       
 │   │  │   Private  │ <---->   [ACL]--+                      │       
 │   │  │            │  │              |                      │       
 │   │  │   Subnet   │  │              v                      │       
 │   │  │            │  │           [172.16.0.0]              │       
 │   │  └────────────┘  │           [172.16.1.0]              │       
 │   │                  │           [172.16.2.0]              │       
 │   └──────────────────┘                                     │       
 └────────────────────────────────────────────────────────────┘       
                                                                              
Each Subnet has it's own Network Access Control List (NACL)                                                                              
                                                                              
Routing configs in Internet Gateway                                                                              
                                                                              
                                                                              
                                                                              

















.