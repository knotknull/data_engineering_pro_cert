
## Source Systems, Data Ingestion and Pipelines
## Course 2
## Week 1

## Introduction to Source Systems


# Types of Source Systems


Structured Data     :  organized as table of rows and columns 
Semi-Structured Data:  Data that is not in tabular form but still has some structure
            i.e. JSON, XML

Unstructured Data:  Data that does not have any predefined structure 
        text, video, audio, images


General types of source systems

    Databases                   Files               Streaming Systems

stored in organized way      sequence of bytes       continuous flow of data

structured                     text                   semi-structured
semi-structured                images
                               audio                 [producer] -> [Message Q, streaming platform]
                               csv                                      |
   CRUD                                                                 +-->> [consumer] 
   DBMS                     structured                  IoT
    Relational              semi-structured             
    NoSQL                   unstructured



# Relational Databases
 
  - Online Transaction Processing: OLTP
  - Primary Key + Foreign Key


Data Normalization: 
    - minimize redundancy
    - ensure data integrity

                                    SQL Commands
                                         |
      +----------------------+-----------------------+---------------------+                                   
      |                      |                       |                     |
      |                      |                       |                     |
     Data                   Data                    Data                  Data  
    Cleaning               Joining               Aggregating            Filtering

    DROP                   INNER JOIN               SUM                  WHERE    
  TRUNCATE                 LEFT JOIN                AVG                   AND    
    TRIM                  RIGHT JOIN               COUNT                  OR    
   REPLACE                 FULL JOIN                MAX                   IS NULL
   SELECT DISTINCT           UNION                  MIN                   IS NOT NULL
                                                   GROUP BY               IN 
                                                                          LIKE 


# SQL Queries 
 

# NoSQL Databases 

NoSQL == Not Only SQL

- non-tabular values 
    - key value
    - document 
    - graph
    - wide column

- no predefined schemas
- more flexibility w/ data


Horizontal Scaling 
                                +---------> Secondary   (Eventual consistency: data read may not be up to date)
                                |                           
            client  ---->> [NoSQL DB]  ---> Primary


        NoSQL DB                        Relational DB
     -------------                    ----------------   
    Eventual Consistency              Strong Consistency

     speed is prioritized            read data only when all nodes
     system availability and           have been updated
      scalability are important

Not All NoSQL databases guarantee ACID:
    Atomicity
    Consistency         NOTE: mongoDB is ACID compliant
    Isolation
    Durability


Key-Value Database:
    - fast lookup: i.e. user session data

Document Store:
    - data stored in JSON-like documents
    - each document as unique key
    - documents organized into collections
        document    ==  row
        collection  ==  table

    - all information stored in one document
    - document stores don't support joins
    - flexible scheam

    use cases: content management, catalogs, sensor readings

NOTE: document databases can become a nightmare to manage and query


# Database ACID Compliance

    Relational Databases                NoSQL Databases
     Atomicity                           Not ACID compliant by default
     Consistency                         
     Isolation                           NOTE: mongoDB is ACID compliant
     Durability

ensures transactions are
processed reliably and 
accurately in an OLTP system

Atomicity:   transactions are atomic, treated as single indivisible unit
                all or no part of a transaction is done

Consistency: changes to data within a transaction follow the set of rules 
             or constraints defined by database schema

Isolation:   each transaction is executed independently in sequential order

Durability:  Once transaction completed, its effects are permanent and will survive 
             any subsequent system failures. (i.e. power loss)

ACID Principle guarantees that a database will maintain a consistent picture of the world
        - Strong Consistency:  Data consisten across the entire network 


# Interacting with DynamoDB

DynamoDB: Key-value Database
        - row has attributes of one item 
        - identified by a key
        - simple primary key == partition key
        - composite primary key == partition key + sort key
        - schemaless

Python Boto3:  python package that allows you to interact with AWS services

    CREATE:       create_table
    READ  :       scan / get_item / query
    UPDATE:       put_item /  write_batch_items /  update_item
    DELETE:       delete_item
                   
import boto3 
client = boto3.client('dynamodb')

KeySchema=[
   {
        'AttributeName': 'ForumName',
        'KeyType': 'HASH'
   },
   {
        'AttributeName': 'Subject',
        'KeyType': 'RANGE'
   },
]
HASH  == partition key
RANGE == sort key

def put_item_db( table_name: str, item: Dict[str, Any], **kwargs):
    ### START CODE HERE ### (~ 2 lines of code)
    client = boto3.client("dynamodb")
    response = client.put_item(TableName=table_name, Item=item, **kwargs)
    ### END CODE HERE ###

    return response

for dynamodb_tab in [product_catalog_table, thread_table]:
    file_name = dynamodb_tab['table_name'].split('-')[-1]    
    items = read_data(file_path=f'./data/aws_sample_data/{file_name}.json')
    
    for item in items[dynamodb_tab["table_name"]]:
        put_item_db(table_name=dynamodb_tab["table_name"], item=item['PutRequest']['Item'])

def batch_write_item_db(items: Dict[str, Any], **kwargs):
    ### START CODE HERE ### (~ 2 lines of code)
    client = boto3.client("dynamodb")
    response = client.batch_write_item(RequestItems=items, **kwargs)
    ### END CODE HERE ###
    
    return response

for dynamodb_tab in [reply_table, forum_table]:
    file_name = dynamodb_tab['table_name'].split('-')[-1]    
    items = read_data(file_path=f'./data/aws_sample_data/{file_name}.json')
    response = batch_write_item_db(items=items)
    print(response)


Queried data for table de-c2w1-dynamodb-ProductCatalog:

Read Capacity Unit  / consistent vs. strongly consistent reads: 

The previous request consumed 1.0 RCU because this item is less than 4KB. 
(RCU stands for Read Capacity Unit: "One read capacity unit represents one strongly consistent read per second, 
 or two eventually consistent reads per second, for an item up to 4 KB in size", reference).



The DynamoDB DeleteItem() method is used to delete an item. Deletes in DynamoDB are singleton operations. 
There is no single command you can run that would delete all the rows in the table. 



# Object Store

 - file treated as an individual object
 - object storage has no hierarchy
 - any type of data
    - semi-structured and unstructured
    - serving data for training ML models

 - each object has:
    -  Universal Unique Identifier (UUID key)
    -  Metadata: creation_date, file type, owner

NOTE: after initial write object is immutable
        - no random write or append

    - A write requires a new write with UUID pointing to new object
    - Can enable versioning: 
        - write metadata which specified version

Why Use Object Storage
    - store files of various formats without a file system structure
    - easily scale out to provide virtually limitless storage space
    - replicate data across several availability zones

        S3: 99.999999999% data durability (11 9's)

    - cheaper than other storage options
    - ideal for data lakes and data lakehouses


Object Store Lab

s3_client.create_bucket(Bucket=bucket_name)
s3_client.upload_file(local_file_path, bucket_name, object_key)
s3_client.select_object_content(bucket_name, object_key)
s3_client.download_file(bucket_name, object_key, local_file_path)

NOTE: can apply a SQL expression to apply on the return from select_object_content()

# aws cli 
aws s3 ls
aws s3 ls de-c2w1lab3-265729830485/csv/

# set query parameters
# 
file_s3_key = 'csv/ratings_ml_training_dataset.csv'
kwargs = {'ExpressionType': 'SQL',
          'Expression': """SELECT * FROM s3object AS s WHERE s.\"productline\" = 'Trains' LIMIT 20""",
          'InputSerialization': {'CSV': {"FileHeaderInfo": "Use"}, 'CompressionType': 'NONE'},
          'OutputSerialization': {'CSV': {}},
}

response = s3_select_object_content(bucket_name=BUCKET_NAME, object_key=file_s3_key, **kwargs)


NOTE: by default S3 buckets and objects in it are private

# below updates bucket's public access settings
# 
    s3_client.put_public_access_block(
        Bucket=bucket_name,
        PublicAccessBlockConfiguration=public_access_block_configuration
    )

# Define the public access settings  
public_access_configuration = {
    'BlockPublicAcls': False,
    'IgnorePublicAcls': False,
    'BlockPublicPolicy': False,
    'RestrictPublicBuckets': False
}

s3_public_access_setup(bucket_name=BUCKET_NAME,  
                       public_access_block_configuration=public_access_configuration)
                       
# apply a policy to a bucket
response = s3_client.put_bucket_policy(Bucket=bucket_name, Policy=json.dumps(policy))

# policy to the bucket to allow anyone on the internet to have reading access to the 
#  objects whose key starts with images/

policy = { 
    "Version": "2012-10-17", 
    "Statement": [
        {
            "Effect": "Allow",
            "Principal": "*",
            "Action": "s3:GetObject",
            "Resource": f"arn:aws:s3:::{BUCKET_NAME}/images/*"
        }
    ]
}

# update bucket versioning
# 
response = s3_client.put_bucket_versioning(
        Bucket=bucket_name,
        VersioningConfiguration=versioning_config
    )

# versioning config
# 
versioning_config = {'Status': 'Enabled'}

response = configure_bucket_versioning(bucket_name=BUCKET_NAME, 
                                       versioning_config=versioning_config)


## S3 select_object_content
https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3/client/select_object_content.html

## S3 query data
https://aws.amazon.com/blogs/storage/querying-data-without-servers-or-databases-using-amazon-s3-select/


## S3 pub_public_access_block
https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3/client/put_public_access_block.html




# Logs

Log: apend only sequence of records ordered by time, capturing info about events

       Rich data source                  downstream use case
    ------------------------          ------------------------
    Web Server Logs        --------->>     Analysis of user behavior patterns
    Database Sysetem Logs  --------->>     Track changes in source database
    Security Sysetem Logs  --------->>     Machine Learning anomaly detection


Log Level:  tag to categorize the event
    - debug         - info
    - warn          - error
    - fatal          

# Streaming Systems



Event:
Something happened in the world or change to a system

Message:
A record of info about an event

Stream: 
A sequence of messages



Straming System Components


[  Event   ]  ----->>   [   Event Router /  ]  --------->>  [  Event   ]
[ Producer ]            [  Streaking Broker ]     |         [ Consumer ]
                                                  |
 - IoT                    - acts as buffer        |         [  Event   ]
 - Mobile App               to filter and         +=---->>  [ Consumer ]
 - API                      distribute messages
 - Website                - decouples producer 
                            and consumer

                            - Message Queues  (FIFO / SQS)
                            - Event Streaming (Append only log: Kinesis, Kafka)




## Source Systems, Data Ingestion and Pipelines
## Course 2
## Week 1

## Connecting to Source Systems


- boto3
- cloud9
- jupyter

- API Connector 
    - JDBC
    - ODBC

# Connecting to an Amazon RDS MySQL Database

https://aws.amazon.com/getting-started/hands-on/create-mysql-db/

# Access RDS via CloudShell

## MySql Connect
https://dev.mysql.com/doc/refman/8.0/en/connecting.html

## Pgsql Connect
https://www.postgresql.org/docs/9.1/app-psql.html



mysql --host=[hostname]            --port=[port number] 
      --user=[database user name]  --password=[database user password]

mysql> show databases;
mysql> use  <db_name>;
mysql> show tables;
mysql> select * from <tbl_name>;
mysql> exit


get the endpoint and port via cli

aws rds describe-db-instances --filters "Name=engine,Values=mysql" --query "*[].[DBInstanceIdentifier,Endpoint.Address,Endpoint.Port,MasterUsername]"


# connect via python
import boto3

access_key_id="AXXXXXXXXXXXXH"
secret_access_key="bXXXXXXXXXXXXZ"
region_name = "us-east-1"

session = boto3.Session(aws_access_key_id=access_key_id, aws_secret_access_key=secret_access_key, region_name=region_name) 
rds=session.client("rds)
dbInstance = rds.describe_db_instances()['DBInstances'[0]]

import pymysql

try: 
    conn = pymysql.connect(host=ENDPOINT, user=USER, passwd=toekn, port=PORT, database=DBNAME)
    cur = conn.cursor()
    cur.execute("""SELECT * from pet""")
    query_results = curr.fetchall()
    print(query_results)
 except Exception as e:
    print("Database connection failed due to {}".format(e))   


# Basics of IAM and Permissions


IAM: framework for managing permissions
    - permissions define which actions an identity (person, app)
      can perform on a specific set of resources



AWS Identity and Access Management


  - Policies used to grant identities access to resources

                                              Resources
[------------]                              [------------]                          
[            ]                              [    [S3]    ]       
[ Identities ]  --->>  [Policies ] --->>    [    [RDS]   ]
[            ]                              [    [EC2]   ]       
[------------]                              [------------]                          

Types of Identities:
Root User:   Has unrestricted access to all resources

IAM  User:   Has specific permissions to certain resources
                - Username and password
                - Access Key

IAM  Group:  A collection of users that inherit the same permission  
             from the group policy
                    i.e.  DB Users or DB Admins

IAM  Role :  A user, application or services that's been granted 
             temporary permissions


        [EC2]     -----XX---->   [S3]
        EC2 be default cannot access S3


    [ROLE] [EC2]     ----------->   [S3]
        Role allows EC2 instance to access S3



# Basics of AWS IAM 

IAM: web servcies that helps you manage and securely control access to AWS resources and services.


Root User:  full access to all AWS resources and services in an account

IAM User:   person or service that interacts with AWS resources.
                - define what resources IAM user can access
                - what actions they can perform
                - credentials generated:
                    - username / password
                    - access keys for apps
                - grant access to AWS resources via policies

Policy:     specifies what actions are allowed or denied for a resource
                - read only, write only, full access
            policy can be attached to several users
            a user can have several policies

When a request is made, AWS evaluates policies to determine if request is allowed

IAM Group:  collection of users, can attach policy to group i.e. Data Scientists or DBAs
                - each user in group inherit's group's permissions 
                - groups can have multiple users
                - user can have no group, one, group or multiple groups (up to 10)
                NOTE: groups CANNOT be nested

IAM Role:   specific permissions with short-term credentials
            Roles can be assumed by entities: people, applications, AWS resources 
            No long-term credentials:  temporary security credentials provided 
               for duration of the role sessions.
            1. Create IAM role   
            2. Attach a policy to it 
            3. Specify which resource can assume this role

            Example: EC2 instance needs to read from S3. 
                     Default EC2 instance does not have read permisssion from S3
                     DON'T transfer credentials to EC2 to read S3
                     Create a role, attach policy to read from S3
                     Assign role to EC2 instance

            Example: Glue job needs to write to S3. 
                     Create role with S3 write perms and assign to Glue job


IAM Policy:  an object in AWS that defines the permissions of the attached user or role.
             can manage access in AWS by creating policies and attaching them to 
                users, groups, roles

             Stored in AWS as JSON doc
             Create custom policy with AWS-managed policies

IAM Documentation
https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html

ex. 
{
    "Version": "2012-10-17", 
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "s3:*",
                "s3-object-lambda:*"
            ],
            "Resource": "*"
        }
    ] 
}

Version  : specialty the version of policy language
Statement: container of the details of given perms or denials.
           can include more than one statement in a policy
           multiple statements: AWS applies locial OR across statements

    Sid:      optional statement id to differntiate between statements
    Effect:   Use Allow or Deny wheter policy allows or denies access
    Action:   list of actions policy allows / denies. 
                eg. s3:*  menas all read and write actions on S3 allowed
    Resource: object or list of objects which the actions apply. 
              i.e. can specify a specific S3 bucket or all resources "*"    

ex. 2

{
    "Version": "2012-10-17", 
    "Statement": [
        {
            Sid: "allow_statement"
            "Effect": "Allow",
            "Action": [
                "s3:*"
            ],
            "Resource": "*"
        },
        {
            Sid: "deny_statement"
            "Effect": "Deny",
            "Action": [
                "s3:DeleteBucket",
            ],
            "Resource": "arn:aws:s3:::confidential"
        }
    ] 
}

IAM / Policy Resources

https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-bucket-policies.html
https://docs.aws.amazon.com/IAM/latest/UserGuide/access.html
https://docs.aws.amazon.com/IAM/latest/UserGuide/id.html


# Basics of Networking in the Cloud

- Region considerations:
    - legacl compliance
    - latency
    - availability
    - cost

VPC:  smaller network that spans multiple availability zones within a region


# AWS Networking Overview: VPC & Subnets
                                                                              
                                                                              
   VPC                                                                
 ┌────────────────────────────────────────────────────────────┐       
 │   ┌──────────────────┐                                     │       
 │   │Availability Zone │           [172.16.0.0]              │       
 │   │                  │           [172.16.0.0]              │       
 │   │                  │           [172.16.0.0]              │       
 │   │  ┌────────────┐  │              ^                      │       
 │   │  │            │  │              |                      │       
 │   │  │   Public   │ <---->   [ACL]--+                      │       
 │   │  │            │  │              |                      │       
 │   │  │   Subnet   │  │              |                      │       
 │   │  │            │  │              |                      │ [ Internet ]      
 │   │  └────────────┘  │              +------------------>   │ [ Gateway  ] <--> Internet      
 │   │                  │              |                      │       
 │   │  ┌────────────┐  │              |                      │       
 │   │  │            │  │              |                      │       
 │   │  │   Private  │ <---->   [ACL]--+                      │       
 │   │  │            │  │              |                      │       
 │   │  │   Subnet   │  │              v                      │       
 │   │  │            │  │           [172.16.0.0]              │       
 │   │  └────────────┘  │           [172.16.1.0]              │       
 │   │                  │           [172.16.2.0]              │       
 │   └──────────────────┘                                     │       
 └────────────────────────────────────────────────────────────┘       
                                                                              
Each Subnet has it's own Network Access Control List (NACL)                                                                              
                                                                              
Routing configs in Internet Gateway                                                                              
                                                                              
                                                                              
                                                                              
# AWS Networking: VPC and Subnets

Scenario: 
    - EC2 instace running web app that queries RDS



                                    User
                                      ^                                       
                                      |                                       
       Region                         |                                       
     ┌────────────────────────────────|───────────────────────────────────┐   
     │     VPC                        |                                   │   
     │   ┌────────────────────────────|───────────────────────────────┐   │   
     │   │                            |                               │   │   
     │   │   ┌──────────────────┐     |      ┌─────────────────┐      │   │   
     │   │   │      AZ 1        │     |      │     AZ 2        │      │   │   
     │   │   │  ┌────────────┐  │     |      │  ┌───────────┐  │      │   │   
     │   │   │  │   Public   │  │     |      │  │   Public  │  │      │   │   
     │   │   │  │   Subnet   │  │     |      │  │   Subnet  │  │      │   │   
     │   │   │  │            │  │     |      │  │           │  │      │   │   
     │   │   │  │ [  NAT   ] │  │     |      │  │ [  NAT  ] │  │      │   │   
     │   │   │  │ [Gateway ] │  │     |      │  │ [Gateway] │  │      │   │   
     │   │   │  └────────────┘  │     |      │  └───────────┘  │      │   │   
     │   │   │                  │     |      │                 │      │   │   
     │   │   │  ┌────────────┐  │     |      │  ┌───────────┐  │      │   │   
     │   │   │  │  Private   │  │     |      │  │  Private  │  │      │   │   
     │   │   │  │  Subnet    │  │     |      │  │  Subnet   │  │      │   │   
     │   │   │  │            │  │     |      │  │           │  │      │   │   
     │   │   │  │   [RDS]    │  │     v      │  │    [RDS]  │  │      │   │   
     │   │   │  │   [EC2] <--│--│---[ALB]----│--│--> [EC2]  │  │      │   │   
     │   │   │  │            │  │            │  │           │  │      │   │   
     │   │   │  └────────────┘  │            │  └───────────┘  │      │   │   
     │   │   └──────────────────┘            └─────────────────┘      │   │   
     │   └────────────────────────────────────────────────────────────┘   │   
     └────────────────────────────────────────────────────────────────────┘   
NOTE: VPC can span all AZs in a Region
        - can have multiple VPCs in a Region 
        - resources in same VPC can communicate
        - cannot by default communicate accross VPCs (needs to be configured)

NOTE: Default VPC in each Region in an AWS account 
        - includes public subnet in each AZ in region  + Internet Gateway
            -  can launch public facing EC2 instances quickly



Network creation steps:
1.   AWS Console -> VPC -> [Create VPC]
        - Name                                                                              
        - Private IP address range: 10.0.0.0/16 (CIDR: Classless Inter-Domain Rouding)
                / notation is the size of the network 
                /16 means network is 16 bits in length (first two digits in dot notation)

        - Region: i.e. US-East-1
    [Create Subnet]
        - Select VPC subnet will reside in
        - Subnet Name
        - Choose Availability Zone
        - Enter subnet CIDR block (must be a subnet of VPC)
                eg. 10.0.1.0/24

        - Add new subnet

   NOTE:  subnet created in specific AZ
            - usually 1 private and 1 public subnet in each AZ

subnet                 CIDR                 AZ
public_subnet_1       10.0.1.0/24      us-east-1a
private_subnet_1      10.0.2.0/24      us-east-1a

public_subnet_2       10.0.3.0/24      us-east-1b
private_subnet_3      10.0.4.0/24      us-east-1b


# Internet Gateway & NAT Gateway

Per scenario:
    - EC2 instances would need internet access to download updates and patches
    - need away to submit requests web app on EC2 instance (via ALB)

Attach Internet Gateway to VPC
    - allow public subnets to connect to internet


NAT Gateway: Network Address Translation Gateway
    - allow resources in private subnet to connect to 
      Internet or other AWS services

    - prevents Internet from initiating connections to those resources


Application Load Balancer (ALB): 
    - Distributes incoming application traffic across multiple backend targets
    - Entry point for external users
    - Handles load and ensures app is responsive and available
    - Keeps EC2 instances private


# Create Internet Gateway
AWS Console -> VPC -> Internet Gateways -> [Create Internet Gateway]
    - Name:
    - [Create Internet Gateway]


NOTE: A VPC can have one Internet Gateway  
        - Internet Gateway can only be attached to one VPC at aa time

# Attach Intenet Gateway to VCP
AWS Console -> VPC -> Internet Gateways -> new Internet gateway -> Actions -> [Attach to VPC]
    - select VPC from dropdown
    - [Attach Internet gateway]

# Create NAT Gateway
AWS Console -> VPC -> NAT Gateways -> [Create NAT Gateway]
    - Name:
    - select subnet from dropdown
    - connectivity type: Public
    - [ Allocate Elastic IP ]  <static IP address> 
    - [Create NAT Gateway]

NOTE: repeat for each public subnet 
      best practice, create NAT Gateway each AZ  


# Route Tables

Rotue Tables: direct network traffice within VPC
    - Each Subnet can have a route table 
    - route table is a set of rules / routes of where network traffic is directed

NOTE: when VPC created, default route table created

Default route table allows internal communication within VPC
        - resources in different subnets can communicate

Public  subnets will direct all internet bound traffic to Internet Gateway
Private subnets will direct all internet bound traffic to NAT Gateway in Public Subnet
    - private subnet resources can make outbound Internet calls
        and prvent inbound connections from Internet



# Create Route Tables
AWS Console -> VPC -> Route tables -> [Create Route Table]
    - Name:
    - VPC: [select VPC from dropdown]
    - [Create Route Table] 


# Associate Subnet to Route Table
AWS Console -> VPC -> Route tables 
    - Choose Route Table
    - Actions: [Edit Subnet Associations]  <<-- dropdown
    - select subnet
    - [Save associations]

# Create Routes
    - public  subnets to Intenet Gateway
    - private subnets to NAT     Gateway


AWS Console -> VPC -> Route tables 
    - Choose Route Table
        NOTE: choose PUBLIC route table
    - [Routes]  << Tab
    - [Edit Routes] 
    - [Add Route] 
        - Destination: 0.0.0.0/0    << ANY IP ADDRESS
        - Target: [Internet Gateway]   from dropdown
            [select Internet Gateway for this VPC]
            [save changes]

Now private subnet route table setup
    - Choose Route Table
        NOTE: choose PRIVATE route table
    - [Routes]  << Tab
    - [Edit Routes] 
    - [Add Route] 
        - Destination: 0.0.0.0/0    << ANY IP ADDRESS
        - Target: [NAT Gateway]   from dropdown
            [select NAT Gateway associated for the subnet the route table is in]
            [save changes]


# Network ACLSs and Security Groups

Security Groups: Instance level virtual firewalls 
                    control inbound and outbound traffic

            Default:  deny all inbound traffic / allow all outbound traffic


Security Groups Are Stateful:
    - allow inbound traffic to an instance automatically allows the return traffic
        DON'T NEED EXPLICIT OUTBOUND RULE
    - security groups can reference other security groups

Resources placed in VPCs use Security Groups
    - i.e. EC2, RDS, LBs



Security Group Chaining

    [ALB]                       [EC2]                           [RDS]

sec grp id: sg-123  <<------+   sec grp id: sg-456  <<----+  sec grp id: sg-789
Source    |Protocol| Port   |   Source |Protocol| Port    |  Source  |Protocol| Port
----------|--------| ----   |   -------|--------| ----    |  --------|--------| ----       
0.0.0.0/0 | HTTP   | 80     +-- sg-123 | HTTP   | 80      +-- sg-456 | TCP    | 3306           
0.0.0.0/0 | HTTPS  | 443        sg-123 | HTTPS  | 443           


# Create Security Group (for ALB)

AWS Console -> VPC -> Security -> Security Groups -> [Create Security Group]
    - Name: alb-sg
    - VPC: select from drop down 
    Inbound Rule:  [Add Rule]
        Type: HTTP
        Source: 0.0.0.0/0
    Inbound Rule:  [Add Rule]
        Type: HTTPS
        Source: 0.0.0.0/0
    [Create Security Group]


Network Access Control Lists (NACL)
    - Provide additional layer of security at subnet level
    - NACLs are STATELESS
    - inbound / outbound rules defined explicitly
    - useful for implementing security policies at subnet level

DEFAULT: NACLs allow all inbound / outbound traffic


VPC / subnets:  provide a way to define a private network on AWS

Route Table:    direct traffic within VPC to Internet 

Public Subnet:  point to Intenet Gateway

Intenet Gateway: allow resources withing public subnets to access Intenet

Private Subnet:  point to NAT Gateway

NAT Gateway:     enable instances to initiate outbound connections securely
NOTE: Route tables determine public / private access 

Security Group:  virtual firewall at instance level (EC2, RDS, LB)
                 control inbound / outbound traffic
                 they are stateful 

Network ACLs :   additional level of security at subnet level
                 stateless / explicit inbound and outbound rules
NACLs vs Security Groups:
    NACLs          @ subnet level 
    Security Group @ instance level 


Connectivity Issue Checklist:

1. Verify that VPC has Internet Gateway properly attached 

2. Verify that route tables have appropriate rules to direct traffic correctly

3. Verify that route tables associations with subnets are configured correctly
     i.e. don't want private routes associated with public subnets

4. Check security groups to make sure they have needed rules in place

5. Review network ACLs to confirm they allow necessary traffic

6. Double check instance configurations to ensure they are associated with correct 
   security groups and sbnets 




Route Table Exmple

 Customr Route Table 1
| Destination  | Target         |<--------- [Public Subnet 10.0.0/24]
| 10.0.0.0/16  | local          |
|  0.0.0.0/0   | internet-gw-id |

 Customr Route Table 2
| Destination  | Target         |<--------- [Public Subnet 10.1.0/24]
| 10.0.0.0/16  | local          |
|  0.0.0.0/0   | nat-gw-id      |


NACL: firewall that filters traffic to / from subnet
NOTE:  Rules are evaluated in ascending order (Rule 100, 110, 120, 800)


Endpoints:
    two types: private zone and public zone

    private zone services:  resources that need to be launced in a VPC 
        i.e. EC2, RDS, ELB, EFS, etc

    public zone services:  resources that DO NOT need to be launced in a VPC
        i.e. S3, DynamoDB, Lambda, Kinesis, Athena

Interface endpoints can be placed in a public or private subnet to allow resources to 
connect to AWS public resources

Gateway endpoints can be attached to a VPC to allow resources in the VPC to connect to 
S3 and DynamoDB

NOTE: S3 can also be reached using an interface endpoint but DynamoDB can only be 
      reached using gateway endpoint

Resources:
https://docs.aws.amazon.com/vpc/latest/userguide/default-vpc.html
https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Internet_Gateway.html
https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat.html
https://docs.aws.amazon.com/vpc/latest/userguide/infrastructure-security.html#VPC_Security_Comparison
https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html
https://docs.aws.amazon.com/en_us/console/ec2/security-groups/create
https://docs.aws.amazon.com/en_us/console/ec2/security-groups/reference
https://docs.aws.amazon.com/whitepapers/latest/aws-privatelink/what-are-vpc-endpoints.html
https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html





DB Connectivity Issues on AWS LAB





# RDS endpoint  por5-5432
de-c2w1a1-rds.cxo8cy4iqghl.us-east-1.rds.amazonaws.com

RDS connection:
    username: postgres
    password: postgrespwrd

## RDS endpoint from cli
de-c2w1a1-rds.cxo8cy4iqghl.us-east-1.rds.amazonaws.com

EC2 vpcid = 2e16
RDS vpcid =  de-c2w1a1 (vpc-....21f)




## Source Systems, Data Ingestion and Pipelines
## Course 2
## Week 2


Data Ingestion: get raw data from somewhere

# Data Ingestion on a Continuum


Unbounded Data: continuous stream of data (no beginning or end)

Stream ingestion:  ingest events one at a time
Batch  ingestion:  boundary on data and ingest the boundary of data as a unit
            - size threshold batch (100 MB)
            - # of records batch   (1000 )
            - time based batch (hourly, daily, weekly)
        - high frequency batch essentially becomes streaming


Ingestion Frequencies

          [Batch]                      [Streaming]
    -------------------------------------------------------->>
       Semi-frequent                  Very Frequent       


File Ingestion 
    - manual file download
    - Secure File Transfer
        - SFTP
        - SCP

IoT Ingestion 

                                     Streaming Platform
[IoT] ->  [ Event Producer ]  -->>  [[ .............. ]]   -->> [ Event Consumer ]
                                       Message Queue

Batch Ingestion Tools:
    - AWS Glue ETL
        - performs ETL job using Apache Spark
            - distributes transformation workloads across compute nodes
        - serverless solution   
            - code-based ingestion and transformation

    - AWS EMR: managed platform to run Hadoop / Spark
        - can run serverless or provisioned

        Glue needs less confguration  / EMR more control 

    - AWS DMS: Data Migration Service
        - sync data from an existing database

    - AWS Snow family: transfer appliance to migrate large amounts of data
        - Snowball, Snowcone

    - AWS Transfer family: transfer files into and out of S3 using SFTP / FTP 

    - Other connectors: Airbyte, Matillion, Fivetran

Resources:
https://docs.aws.amazon.com/glue/latest/dg/components-key-concepts.html
https://docs.aws.amazon.com/prescriptive-guidance/latest/serverless-etl-aws-glue/aws-glue-etl.html
https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-what-is-emr.html

Streaming Ingestion Tools:
    - Kinesis
    - Apache Kafka MSK

Considrations Batch vs. Streaming ingestion

Use cases:  
    - what are you going to do with "real time" data over periodic batches
    - machine learning 
        - batch is good approach for training
    - dashboards / reporting:
        - benefit of "real time" dashboard. how will they act

Latency:  millisecond real-time data vs. micro batch ?

Cost   :  streaming can carry extra costs / complexities

Existing / Available system: 
        - can destination system handle real time data
        - what is impact of ingesting from live production source system 

Reliability / Availability:
        - streaming needs high availability of compute, batch does not


# ETL vs ELT

API souce -->> Batch ingetstion due to the number of call

Extract Transform Load
            - original batch ingestion proces 
            - potential data loss on transform

                      Extract       Transform       Load
       [Data Source] ----------> [Staging Area]  -------->  [Target]  

Extract Load Transform 
            - captures all data

                      Extract       Load      Transform
       [Data Source] ---------->   -------->  [Target]  

NOTE: Transform done in the target i.e. Datawarehouse

Advantages of ELT 
    - faster to implement
    - data available more quickly to end users
    - transformations can be done efficiently
        decide later to adopt different transformations 


Disadvantages:
    - can be come an Extract - Load pipeline
    - target can become a Data Swamp

EtLT: small t means transformation with limited scope
        - mask sensitive data, deduplicate rows


# REST API

API:  set of rules and specifications the allows you to programatically 
      communicate and exchange data with an application

API Features:
    - Metadata
    - Documentation
    - Authentication
    - Error handling

REST API:  Representational State Transfer API
           Use HTTP as basis for communication


## Spotify Lab

https://developer.spotify.com
https://developer.spotify.com/documentation/web-api
https://developer.spotify.com/documentation/web-api/concepts/authorization

each data item is a resource

HTTP Request        Action
GET                 retrieve a resource
POST                create a resource
PUT                 change / replace a resource
Delete              delete a resource


HTTP Request:  Endpoint + Access token

Access token: string that contains the permissions to access a given resource (valid 1 hr)
  - create spotify account
  - get client ID and client secret
        use them to generate access token


Get Playlist
Get Featured Playlist

Pagination: Extract the items chunk by chunk
    - using offset and limit

https://api.spotify,com/v1/me/shows?offset=0&limit=20
https://api.spotify,com/v1/me/shows?offset=20&limit=20
https://api.spotify,com/v1/me/shows?offset=40&limit=20
https://api.spotify,com/v1/me/shows?offset=60&limit=20
https://api.spotify,com/v1/me/shows?offset=80&limit=20

# use next field 
response.get('playlists').get('next')




authentication.py: contains scripts for get_token function returns access token

endpoint.py       1  paginated call to endpoint "Get featured playlists":.
                  2. paginated call to endpoint "Get playlist": 
                  3. TODO: generate new token when expired

main.py: get the ids of the featured playlists
         for each playlist id call second paginated call to get track info


MAP LAST HERE
# review these first before starting the lab:
https://developer.spotify.com
https://developer.spotify.com/documentation/web-api
https://developer.spotify.com/documentation/web-api/concepts/authorization



https://www.coursera.org/learn/source-systems-data-ingestion-and-pipelines/gradedLti/xp17c/graded-programming-assignment-2-batch-data-processing-from-an-api







.