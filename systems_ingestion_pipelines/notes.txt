
## Source Systems, Data Ingestion and Pipelines
## Course 2
## Week 1

## Introduction to Source Systems


# Types of Source Systems


Structured Data     :  organized as table of rows and columns 
Semi-Structured Data:  Data that is not in tabular form but still has some structure
            i.e. JSON, XML

Unstructured Data:  Data that does not have any predefined structure 
        text, video, audio, images


General types of source systems

    Databases                   Files               Streaming Systems

stored in organized way      sequence of bytes       continuous flow of data

structured                     text                   semi-structured
semi-structured                images
                               audio                 [producer] -> [Message Q, streaming platform]
                               csv                                      |
   CRUD                                                                 +-->> [consumer] 
   DBMS                     structured                  IoT
    Relational              semi-structured             
    NoSQL                   unstructured



# Relational Databases
 
  - Online Transaction Processing: OLTP
  - Primary Key + Foreign Key


Data Normalization: 
    - minimize redundancy
    - ensure data integrity

                                    SQL Commands
                                         |
      +----------------------+-----------------------+---------------------+                                   
      |                      |                       |                     |
      |                      |                       |                     |
     Data                   Data                    Data                  Data  
    Cleaning               Joining               Aggregating            Filtering

    DROP                   INNER JOIN               SUM                  WHERE    
  TRUNCATE                 LEFT JOIN                AVG                   AND    
    TRIM                  RIGHT JOIN               COUNT                  OR    
   REPLACE                 FULL JOIN                MAX                   IS NULL
   SELECT DISTINCT           UNION                  MIN                   IS NOT NULL
                                                   GROUP BY               IN 
                                                                          LIKE 


# SQL Queries 
 

# NoSQL Databases 

NoSQL == Not Only SQL

- non-tabular values 
    - key value
    - document 
    - graph
    - wide column

- no predefined schemas
- more flexibility w/ data


Horizontal Scaling 
                                +---------> Secondary   (Eventual consistency: data read may not be up to date)
                                |                           
            client  ---->> [NoSQL DB]  ---> Primary


        NoSQL DB                        Relational DB
     -------------                    ----------------   
    Eventual Consistency              Strong Consistency

     speed is prioritized            read data only when all nodes
     system availability and           have been updated
      scalability are important

Not All NoSQL databases guarantee ACID:
    Atomicity
    Consistency         NOTE: mongoDB is ACID compliant
    Isolation
    Durability


Key-Value Database:
    - fast lookup: i.e. user session data

Document Store:
    - data stored in JSON-like documents
    - each document as unique key
    - documents organized into collections
        document    ==  row
        collection  ==  table

    - all information stored in one document
    - document stores don't support joins
    - flexible scheam

    use cases: content management, catalogs, sensor readings

NOTE: document databases can become a nightmare to manage and query


# Database ACID Compliance

    Relational Databases                NoSQL Databases
     Atomicity                           Not ACID compliant by default
     Consistency                         
     Isolation                           NOTE: mongoDB is ACID compliant
     Durability

ensures transactions are
processed reliably and 
accurately in an OLTP system

Atomicity:   transactions are atomic, treated as single indivisible unit
                all or no part of a transaction is done

Consistency: changes to data within a transaction follow the set of rules 
             or constraints defined by database schema

Isolation:   each transaction is executed independently in sequential order

Durability:  Once transaction completed, its effects are permanent and will survive 
             any subsequent system failures. (i.e. power loss)

ACID Principle guarantees that a database will maintain a consistent picture of the world
        - Strong Consistency:  Data consisten across the entire network 


# Interacting with DynamoDB

DynamoDB: Key-value Database
        - row has attributes of one item 
        - identified by a key
        - simple primary key == partition key
        - composite primary key == partition key + sort key
        - schemaless

Python Boto3:  python package that allows you to interact with AWS services

    CREATE:       create_table
    READ  :       scan / get_item / query
    UPDATE:       put_item /  write_batch_items /  update_item
    DELETE:       delete_item
                   
import boto3 
client = boto3.client('dynamodb')

KeySchema=[
   {
        'AttributeName': 'ForumName',
        'KeyType': 'HASH'
   },
   {
        'AttributeName': 'Subject',
        'KeyType': 'RANGE'
   },
]
HASH  == partition key
RANGE == sort key

def put_item_db( table_name: str, item: Dict[str, Any], **kwargs):
    ### START CODE HERE ### (~ 2 lines of code)
    client = boto3.client("dynamodb")
    response = client.put_item(TableName=table_name, Item=item, **kwargs)
    ### END CODE HERE ###

    return response

for dynamodb_tab in [product_catalog_table, thread_table]:
    file_name = dynamodb_tab['table_name'].split('-')[-1]    
    items = read_data(file_path=f'./data/aws_sample_data/{file_name}.json')
    
    for item in items[dynamodb_tab["table_name"]]:
        put_item_db(table_name=dynamodb_tab["table_name"], item=item['PutRequest']['Item'])

def batch_write_item_db(items: Dict[str, Any], **kwargs):
    ### START CODE HERE ### (~ 2 lines of code)
    client = boto3.client("dynamodb")
    response = client.batch_write_item(RequestItems=items, **kwargs)
    ### END CODE HERE ###
    
    return response

for dynamodb_tab in [reply_table, forum_table]:
    file_name = dynamodb_tab['table_name'].split('-')[-1]    
    items = read_data(file_path=f'./data/aws_sample_data/{file_name}.json')
    response = batch_write_item_db(items=items)
    print(response)


Queried data for table de-c2w1-dynamodb-ProductCatalog:

Read Capacity Unit  / consistent vs. strongly consistent reads: 

The previous request consumed 1.0 RCU because this item is less than 4KB. 
(RCU stands for Read Capacity Unit: "One read capacity unit represents one strongly consistent read per second, 
 or two eventually consistent reads per second, for an item up to 4 KB in size", reference).



The DynamoDB DeleteItem() method is used to delete an item. Deletes in DynamoDB are singleton operations. 
There is no single command you can run that would delete all the rows in the table. 



# Object Store

 - file treated as an individual object
 - object storage has no hierarchy
 - any type of data
    - semi-structured and unstructured
    - serving data for training ML models

 - each object has:
    -  Universal Unique Identifier (UUID key)
    -  Metadata: creation_date, file type, owner

NOTE: after initial write object is immutable
        - no random write or append

    - A write requires a new write with UUID pointing to new object
    - Can enable versioning: 
        - write metadata which specified version

Why Use Object Storage
    - store files of various formats without a file system structure
    - easily scale out to provide virtually limitless storage space
    - replicate data across several availability zones

        S3: 99.999999999% data durability (11 9's)

    - cheaper than other storage options
    - ideal for data lakes and data lakehouses


Object Store Lab

s3_client.create_bucket(Bucket=bucket_name)
s3_client.upload_file(local_file_path, bucket_name, object_key)
s3_client.select_object_content(bucket_name, object_key)
s3_client.download_file(bucket_name, object_key, local_file_path)

NOTE: can apply a SQL expression to apply on the return from select_object_content()

# aws cli 
aws s3 ls
aws s3 ls de-c2w1lab3-265729830485/csv/

# set query parameters
# 
file_s3_key = 'csv/ratings_ml_training_dataset.csv'
kwargs = {'ExpressionType': 'SQL',
          'Expression': """SELECT * FROM s3object AS s WHERE s.\"productline\" = 'Trains' LIMIT 20""",
          'InputSerialization': {'CSV': {"FileHeaderInfo": "Use"}, 'CompressionType': 'NONE'},
          'OutputSerialization': {'CSV': {}},
}

response = s3_select_object_content(bucket_name=BUCKET_NAME, object_key=file_s3_key, **kwargs)


NOTE: by default S3 buckets and objects in it are private

# below updates bucket's public access settings
# 
    s3_client.put_public_access_block(
        Bucket=bucket_name,
        PublicAccessBlockConfiguration=public_access_block_configuration
    )

# Define the public access settings  
public_access_configuration = {
    'BlockPublicAcls': False,
    'IgnorePublicAcls': False,
    'BlockPublicPolicy': False,
    'RestrictPublicBuckets': False
}

s3_public_access_setup(bucket_name=BUCKET_NAME,  
                       public_access_block_configuration=public_access_configuration)
                       
# apply a policy to a bucket
response = s3_client.put_bucket_policy(Bucket=bucket_name, Policy=json.dumps(policy))

# policy to the bucket to allow anyone on the internet to have reading access to the 
#  objects whose key starts with images/

policy = { 
    "Version": "2012-10-17", 
    "Statement": [
        {
            "Effect": "Allow",
            "Principal": "*",
            "Action": "s3:GetObject",
            "Resource": f"arn:aws:s3:::{BUCKET_NAME}/images/*"
        }
    ]
}

# update bucket versioning
# 
response = s3_client.put_bucket_versioning(
        Bucket=bucket_name,
        VersioningConfiguration=versioning_config
    )

# versioning config
# 
versioning_config = {'Status': 'Enabled'}

response = configure_bucket_versioning(bucket_name=BUCKET_NAME, 
                                       versioning_config=versioning_config)


## S3 select_object_content
https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3/client/select_object_content.html

## S3 query data
https://aws.amazon.com/blogs/storage/querying-data-without-servers-or-databases-using-amazon-s3-select/


## S3 pub_public_access_block
https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3/client/put_public_access_block.html




# Logs

MAP LAST HERE
https://www.coursera.org/learn/source-systems-data-ingestion-and-pipelines/lecture/p2cpV/logs



.