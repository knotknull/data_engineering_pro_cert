Andrew Ng

Joe Reis
Oreilly: Fundamentals of Data Engineering




## Intro to Data Engineering
## Week 1

Data Centric AI: discipline of systematically engineering the data used to build an AI system



# Syllabus

Intermediate Python skills:
    - w3 School Pandas tutorial: https://www.w3schools.com/python/pandas/default.asp
    - Kaggle Pandas tutorials  : https://www.kaggle.com/learn/pandas

Basic SQL skills:
    - SQLBolt Tutorials course: https://sqlbolt.com/


AWS skills :
    - AWS Cloud Practioner Essentials: https://www.coursera.org/learn/aws-cloud-practitioner-essentials
    - AWS Cloud Technical Essentials : https://www.coursera.org/learn/aws-cloud-technical-essentials


## Digial copy of Fundamentals of Data Engineering
https://go.redpanda.com/fundamentals-of-data-engineering


# DeepLearning AI Community link
https://community.deeplearning.ai/invites/RsdEnVWirt




# Data Engineering Defined

    - originally seen as byproduct or "exhaust"
    - data has business value
    - data engineering work:

        [ ingestion ] [ transformation ] [ serving ]
        --------------------------------------------
                      [ data storage ] 

Data engineering is the development, implementation, and maintenance of systems and processes
that take in raw data and produce high quality consistent information that supports downstream 
use cases such as analysis and machine learning.

Data engineering is the intersection of:
         security, data management, DataOps,
        data architecture, orchestration, and software engineering


# Data Engineering Lifecycle                                                   
                    + -----------------------------------------------------+       +-------------------+
+------------+      |                                                      |       |                   |
|            |      | [ ingestion ] ->  [ transformation ] ->  [ serving ] |       |  Analytics        |
| Generation | -->> | -----------------------------------------------------| --->> |  Machine Learning |
|            |      | [[  ............    data storage   ............   ]] |       |  Reverse ETL      |
+------------+      |                                                      |       |                   |
      ^             +------------------------------------------------------+       +-------------------+
      |                                                                                    |
      |                                                                                    |
      +-----------------------------------Reverse ETL--------------------------------------+

==========================================================================================================

  [Security]  [Data Mangement]  [DataOps]  [Data Architecture]  [Orchestration]  [Software Engineering]




Data Pipeline: combination of architecture, systems, and processes that move data through the 
               stages of the data engineering lifecycle

in short: get raw data, turn in into something useful and then deliver



  Downstream                                      Data                        Upstream
Stakeholders                 --+                 Engineer                     Stakeholders
  - Analysts                   |               +----------+                     - Software 
  - Data Scientists            | How often <-> |          | <-> Volume             Engineers
  - Machine Learning Engineers | What Info <-> |   Data   | <-> Frequency
  - Salespeople                | How much  <-> |  Engineer| <-> Format
  - Marketing Professionals    |  latency      |          | <-> Data Security
  - Executives               --+               +----------+ <-> Reg Compliance


Look for business value!!
    - don't get hung up on the technology
    - focus on the value that the tech can provide !!!!


# System Requirements
    - Business Requirements
            - revenue growth
            - increase user base
    - Stakeholder Requirements
            - needs of individual in orgranization
    - System Requirements
            - Functional Requirements
                - "The What"
                    - Updates to a DB
                    - Alert a user about data anomaly
            - Non-Functional Requirements
                - "The How"
                    - Tech specs of ingestion or orchestration 
                    - How you'l meed end user's needs

# Reguirements Gathering:                    --+                           +--
    - Business & Stakeholder Requirements    |                           | High Level       Downstream
    - Features and Attributes                |  <<-- Data Engineer  -->> | Goals &     <<-- Stakeholders 
    - Memory & Storage Capacity              |                           | needs
    - Costs & Security Constraints           |                           |
                                           --+                           +--

Jordan Morrow: Founder of Data Literacy 
    - ability to read, work with and analyze data

    - Understand audience
    - Understand business goals of company
    - Understand goals of stakeholders

Work on People Skills
Know how business operates

# Reguirements Gathering Conversation 

- what do they need
- how do they will use it
- when do they need it
    - is there value in real time ?
- format and time scope

# Stakeholder Needs into Specific Requirements

# Key Elements:
    - Learn what existing data systems or solutions are in place
    - Learn what pain points or problems there are with the existing solutions
    - Learn what actions stakeholders plan to take with the data
        NOTE: Repeat what you learned back to your stakeholders
    - Identify any other stakeholders you'll need to talk to if you're still missing information

Think like a Data Engineer:

1. Identify business goals &  stakeholder needs
    A. Identify business goals and stakeholders you will serve 
    B. Explore existing systems and stakeholder needs
    C. Ask stakeholders what actions they will take with the data product

2. Define System Requirements
    A. Translate stakeholder needs to Functional Requirements
    B. Define non-functional requirements
    C. Document and confirm requirements with stakeholders

3. Choose tools & technologies
    A. Itentify tools & tech to meet non-functional requirements
    B. Perform cost / benefity analysis and choose between comparable tools & tech
    C. Prototype and test your system, alaign with stakeholder needs

4. Build, evaluate, iterate & evolve
    A. Build & deploy production data system
    B. Monitor, evaluate and iterate on you system to improve it
    C. Evolve your system basedon stakeholder needs

     +-->  [1] -> [2] -> [3] -> [4] ---+
     |                                 |
     +---------------------------------+



community.deeplearning.ai 

## Data Engineering on the Cloud
## 


# Intro to AWS Cloud

- compute
    - EC2
    - Container hosting services (EKS)
    - Serverless functions
- storage
    - Simple Storage Service (S3)
    - Elastic Block Store
    - Database Services
- networking
    - Virtual Private Clouse

# AWS Regions and Availability Zones

AWS Regions: collections of data centers within geographical areas where you can use AWS Services 
    - high reliability, availability, resilience
    - named for geographical name and region code: us-east-1 
            Region: us-east-1     AZ: us-east-1a, 1b, 1c 
            Region: eu-central-1  AZ: eu-central-1a, eu-central-1b
    - Each region contains at least 3 isolated and physically separated AZs  **


Availability Zones: Each Region comprised of multiple AZs 
    - multiple data centers make up Availability Zone
    - multiple AZs make up an AWS Region
    - physically separated by a meaningul distance from any other AZ 
        - all within 100km of eachother **

34 AWS Regions  **
108 Availability Zones  **

NOTE: Costs may differ between regions
      Certain regulations may rquire hosting your data in specific geographical regions
      Service availability no available in all regions


https://aws.amazon.com/about-aws/global-infrastructure/regions_az/

https://aws.amazon.com/about-aws/global-infrastructure/


# Intro to AWS Core Services

Compute: 
    - Elastic Compute Cloud (EC2) 
        - VM  / EC2 instance
    - Lambda
    - Elastic Container Service (ECS)
    - Elastic Kubernetes Service (EKS)


Network: 
    - Virtual Private Network (VPC): Private network you can crate and place resources into 
        - isolated from other networks
        - choose size of private IP space
        - subnets: partition into smaller networks
    NOTE: VPC spans AZs within a region
            - cannot span across regions
            - create a VPC in every region to operate in
        
        - data and resources don't leave region unless you build solution to do so

      Region                                                                       
    ┌────────────────────────────────────────────────────────────┐         
    │     VPC                                                    │         
    │   ┌────────────────────────────────────────────────────┐   │         
    │   │    AZ 1                   AZ 2                     │   │         
    │   │   ┌──────────────────┐    ┌─────────────────┐      │   │         
    │   │   │  ┌────────────┐  │    │  ┌───────────┐  │      │   │         
    │   │   │  │   Public   │  │    │  │   Public  │  │      │   │         
    │   │   │  │   Subnet   │  │    │  │   Subnet  │  │      │   │         
    │   │   │  └────────────┘  │    │  └───────────┘  │      │   │         
    │   │   │  ┌────────────┐  │    │  ┌───────────┐  │      │   │         
    │   │   │  │   Private  │  │    │  │   Private │  │      │   │         
    │   │   │  │   Subnet   │  │    │  │   Subnet  │  │      │   │         
    │   │   │  └────────────┘  │    │  └───────────┘  │      │   │         
    │   │   └──────────────────┘    └─────────────────┘      │   │         
    │   └────────────────────────────────────────────────────┘   │         
    └────────────────────────────────────────────────────────────┘         
                                                                                   
                                                                                   
Storage: 
    Object Storage:  S3 / unstructured data

    Block Storage :  EBS / db storage, vm file system                     
                        - attatch EBS as volumes to EC2 instance                                                                 

    File Storage  :  Data organized into files and directories  / Elastic File System (EFS)                                                                                    
                                                                                   
                                                                                   
    Relational Data Service :  RDS / cloud-based relational database service
    Redshift :  data warehouse services to store transform and serve data 


Security: 
    Shared responsiblity model
        AWS  is responsible for secrurity OF the cloud
            - hardware to hypervisor
        User is responsible for secrurity IN the cloud
            - VM, OS, Firewall Config, Software, Access and Data





Resources:


https://go.redpanda.com/fundamentals-of-data-engineering

https://www.linkedin.com/pulse/what-recovering-data-scientist-joe-reis/

https://datastackshow.com/podcast/29-the-present-and-future-of-data-engineering-with-joe-reis-and-matthew-housley-from-ternary-data/

https://www.listennotes.com/podcasts/monday-morning/154-sol-rashidi-getting-b2Bjy6Fr-4P/#google_vignette


## AWS
https://aws.amazon.com/getting-started/

https://www.coursera.org/learn/aws-cloud-practitioner-essentials

https://www.coursera.org/learn/aws-cloud-technical-essentials

https://aws.amazon.com/certification/certified-data-engineer-associate/


## Intro to Data Engineering
## Data Engineering Lifecycle
## Week 2


# Data Generation in Source Systems

Common Source Systems  
 - Databases: Relational / NoSQL
 - Files    : text , audio , video
 - API      : request / response
 - Data Sharing Platform
 - IoT Devices

Source systems are unreliable:
    - can go down
    - format / schema changes

Data Engineer needs to work with Source Systems 
    - How they generate data
    - How data changes over time
    - How changes will impact downstream systems

# Ingestion

ingestion: moving raw data from source systems into data pipeline for processing

    - frequency:  batch / streaming

    streaming: near real-time (data available shortly after it is produced)
        - event streaming platform / message queue

    - What would be done with the data in real-time

# Storage

Raw Hardware 
    Solid-State Storage
    Magnetic Storage
    RAM

Process components
    Networking          Serialization
    CPU                 Compression
    Caching

Storage Systems
    DBMS                    Object Store
    Apache Iceberg          Cache / Memory based storage
    Streaming Storage

Storage Abstraction
    Data Warehouse      Data Lake
    Data Lakehouse

    choose config patterns: Latency, Scalability, Cost

Storage Hierarchy
 [ Storage Abstractions]
 [   Storage Systems   ]
 [     Raw Hardware    ]


# Data Transformation / Data Engineering Lifecycle

transformation:  turn raw data into useful data / valuable

queries, modeling, transformation

query: issue a request to read records from data source

data modeling: choosing a coherent structure for data to make it useful for business

data transformation: data manipulated, enhanced and saved for downstream use


# Serving Data

Giving stakeholders the opportunity to extract business value from data

analytics:  process of identifying key insights and patterns within data
        - business intelligence: 
                explore historical and business data to discover insights

        - operational analytics: 
                monitoring real-time data for immediate action
        
        - embedded analytics
                external or customer-facing analytics
                    - dashboards, etc

machine learning: 
        - model training
        - real-time inference
        - track data history and lineagoe

reverse ETL learning:  take transformed data, analytics, machine learnig model output 
                       and feed back to source systems


## Data Engineering Lifecycle
## Data Undercurrents
## Week 2

Data Undercurrents:
    Security
    Data Management
    DataOps
    Data Architecture
    Orchestration
    Software Engineering

# Security 

Least Privilege:  Give users or apps access to only the essentail data / resources they need 
                  for only the duration required

Data Sensitivity: Give user access to data only when necessary


Security in the Cloud:
    - Identity and Access Management
    - Encryption Methods
    - Networking Protocols

defensive mindset
    - be cautious with sensitive data
    - design for potential attacks

# Data Management  

    Data Management Association International (DAMA)
        - provides Data Management Book of Knowledge (DMBOK)

DMBOK defines data management as the development, execution, and supervision of plans,
programs, and practices that deliver, control, protect, and enhance the value of
data and information assets throughout their life cycles


Data governance: data management fucntion to ensure the quality, integrity, security and
                 usability of the data collecte by an orgranization
                 
Data quality
    High Quality Data                       Low Quality Data
    -----------------------                 --------------------
        Accurate                                Inaccurate
        Complete                                Incomplete
        Discoverable                            Hard to find
        Available in a timely manner            Late
    ( exactly what stakeholders expect )        (unusable)


# Data Architecture  

 The design of systems to support the evolving data needs 
 of an enterprise, achived by flexible and reversible decisions 
 reached through a careful evaluation of trade-offs

    - ongoing effort
    - evolving

Principle of Good Data Architecture 
    1. Choose common components wisely
    2. Plan for failure
    3. Architect for scalability
    4. Architecture is leadership 
    5. Always be architecting
    6. Build loosely coupled systems
    7. Make reversible decisions
    8. Prioritize security (but it is toward the bottom)
    9. Embrace FinOps: Finance + DataOps


# Data Ops  

DataOps: Improves the dev process and quality of data products
         set of cultural habits
            - Communication and Collaboration
            - Continuous Improvement
            - Rapit Iteration

DataOps from Agile Methodology

Pillars of DataOps:
    - Automation
    - Observability & Monitoring
    - Incident Response

Goal: Provide high-quality data products 

Automation:
    Automated Change management:
        Code /  Config / Environment 
        Pipelines
        Data 
    Scheduling
    Orchestration framework (Airflow)

Observability & Monitoring:

Incident Response
    - root cause
    - quick resolution
    - identify tech and tools
    - coordinate efforts of data team


# Orchestration

Orchestration Frameworks:
    - Airflow
    - Dagster
    - Prefect
    - Mage

  - Automate pipeline with complex dependencies
  - Monitor pipeline

Directed Acyclic Graph (DAG)
    - how data flows thru pipeline
    - flow of data in one direction (directed)
    - acyclic (no loops)
    - graph: composed of nodes and edges
        i.e. flowchart

# Software Engineering

design, development, deployment and maintenance of software apps

    - open source fameworks
    - Infrastructure as code
    - Pipeline as code
    - Problem solving


## Practical Examples on AWS

# Data Engineering Lifecycle on AWS

# Data Engineering Lifecycle                                                   
                    + -----------------------------------------------------+       +-------------------+
+------------+      |                                                      |       |                   |
|            |      | [ ingestion ] ->  [ transformation ] ->  [ serving ] |       |  Analytics        |
| Generation | -->> | -----------------------------------------------------| --->> |  Machine Learning |
|            |      | [[  ............    data storage   ............   ]] |       |  Reverse ETL      |
+------------+      |                                                      |       |                   |
      ^             +------------------------------------------------------+       +-------------------+
      |                                                                                    |
      +-----------------------------------Reverse ETL--------------------------------------+

Source Systems
 Databases
    - Amazon Relational Database Service (RDS)
        - managed db
        - mysql / postgresql

    - Amazon DynamoDB
        - Serverless NoSQL
        - stand-alone tables (virtually unlimited in size)
        - flexible schema
        - low latency / large volumes (gaming, IoT)
 
 Streaming
    - Amazon Kinesis Data Streams
    - Amazon Simple Queue Service (SQS)
    - Amazon Managed Streaming for Apache Kafka (MSK)
        - AWS manages infrastructure

 Ingestion
   From a DB    
    - Amazon Database Migration Services (DMS)
        - migration / replicate data from source to target automated

    - AWS Glue
        - supports data integration processes

   From Streaming    
    - Amazon Kinesis Data Streams
    - Amazon Data Firehose
    - Amazon SQS
    - Amazon MSK

 Storage
   Traditional Data Warehouse
    - Amazon Redshift
    - Amazon S3
          +-->> Lakehouse: access structured data in data warehouse 
                           and unstructured data in an object store data lake 

 Transformation
   Data Processing Tools
    - AWS Glue
    - Apache Spark
    - dbt


 Serving
   Business Intelligence / Analytics 
    - Amazon Athena   : query structured / unstructured data
    - Amazon Redshift

   Dashboarding Tools
    - Amazon QuickSight 
    - Apache Superset 
    - Apache Metabase 

   AI / Machine Learning 
    - batch data for model training
    - vector databases

# Undercurrents on AWS

Undercurrents (get it, they are under the dashed line ....)

==========================================================================================================
  [Security]  [Data Mangement]  [DataOps]  [Data Architecture]  [Orchestration]  [Software Engineering]



 Security
   Shared Responsibility Model
    - AWS      responsible for data center and services they provide
    - Customer responsible for security of systems built with AWS resources 

    - Identity and Access Management (IAM)
        - roles and permissions to access AWS resources

       IAM Roles:
        - give users / apps access to temporary credentials
        - provide appropriate AWS API permissions to various tools / data storage areas

    - Amazon Virtual Private Cloud 
        - private network

    - Security Groups
        - Instance level firewalls


 Data Management
    - AWS Glue               +   | Discover, create, manage metadata for data stored in 
    - AWS Glue Crawler       | --| S3 or other storage and database systems
    - AWS Glue Data Catalog  +   |

    - AWS Lake Formation:  Centrally manage and scale fine-grained data access permissions

 DataOps
    - AWS CloudWatch: Collects metrics and provides monitroing features for cloud resources, applications, 
                      and on-premises resources

    - AWS CloudWatch Logs: Sore and analyze operational logs

    - AWS Simple Notification Service (SNS):  setup notifications between apps or 
                  via text / email that are triggered by events within your system
 
    OpenSource monitoring: Monte Carlo, Bigeye

 Orchestration 
    - Apache Airflow
        - alternatives: Dagster, Prefect, Mage

 Architecture 
    - AWS Well-Architected
        Operactional Excellence    Performance Efficiency
        Security                   Cost Optimization
        Reliability                Sustainability

 Software Engineering 
    - AWS Cloud9: IDE for development within a browser
        [Cloud9]  ---hosted on ---> [EC2]

    - AWS CodeDeploy: Automated Code Deployment

    - repos: git / github



NOTE: Community Forum of lab issues: 
https://community.deeplearning.ai/c/course-q-a/data-engineering-specialization/de-course-1/467


Lab Scenario:

RDBMS: product, productlines, employees, offices, customers, orders, orderdetails, payments
  |
  +--> Transformed data: fact_orders, dim_customers, dim_products, dim_locations


Architectural Diagram for Lab
                                                      +------------->  User
 +--VPC------------------------+                      |
 |                             |         +------> [ Athena ]
 |    [ RDS ]    [ EC2  ]      |         |
 |      |        [Cloud9]      |         |
 |      |                      |         |
 |     +---> [ Glue ETL ] -----|------> [S3] <--- [Glue Crawler] 
 |                             |
 |                             |
 +-----------------------------+

Lab Walkthru:
- Setup Lap
- Launch Cloud9 
- Setup Jupyter Notebook 

NOTE : Once finished with lab, go back to vocareum and "Submit" lab

LAB: Passed 100 / 100


## Intro to Data Engineering
## Data Architecture
## Week 3


## Data Architecture


Enterprise Arechitecture:  the design of systems to support change in an enterprise, 
    achieved by flexible and reversible decisions reached through a careful evaluation of trade-offs
    

Data Arechitecture      :  the design of systems to support the evolving data needs of 
    an enterprise.  Achieved by flexible and reversible decisions reached through a careful 
    evaluation of trade-offs. 

// VERY SIMILAR !!

+--- Enterprice Architecture ---+
|                               |
|   [Business Architecture]     | <<-- Product / service strategy and business model
|   [Application Architecture]  | <<-- Structure and interaction of key applications
|   [Technical Architecture]    | <<-- Interaction of software and hardware components
|   [Data Architecture]         | <<-- Supporting the evolving needs of data
|                               |
+-------------------------------+

NOTE: Change management and reversible decisions are key to Enterprise Architecture

## Conway's Law

"Any organization that designs a system will produca a design whose structure is a copy
of the organization's communication structure." 
  -- Melvin Conway

WARNING:  DO NOT BUILD A SYSTEM THAT CLASHES WITH COMPANY'S COMMUNICATION STRUCTURE



## Principle's of Good Data Architecture

1. Choose common components wisely
2. Plan for failure
3. Architect for scalability
4. Architect is leadership
5. Always be architecting
6. Build loosely coupled systems
7. Make reversible decisions
8. Prioritize security (second to last)
9. Embrace FinOps


# Grouped by
                                   +
1. Choose common components wisely | <<-- How data arch impacts other teams 
4. Architect is leadership         +      and individuals 

5. Always be architecting          +
6. Build loosely coupled systems   | <<-- Data arch is an ongoing process 
7. Make reversible decisions       +

2. Plan for failure                +
3. Architect for scalability       | <<-- Unspoken but understood priorities 
8. Prioritize security             |
9. Embrace FinOps                  +


# Common Components
    - Object Storage
    - version control system
    - Observability & Monitoring
    - Processing Engines

    can facilitate team collaboration / break down silos
    Wise choice:
        - identify tools that benefit all teams
        - Avoid a one-size-fits-all approach

## Always Architecting



                    [ Always be architecting ]
                        ^              ^
                       /                \
                      /                  \
                     /                    \
     [ Make reversible decisions]      [ Build loosely coupled systems]



## When Systems Fail 

# Plan for Failure

    - Availability: percent of time an IT service is expected to be in an operable state
    - Realiability: probability of a service performing its' intended function during a 
                    particular time interval
    - Durability  : ability of a storage system to withstand data loss due to hardware failure, 
                    software errors, or natural disasters

Recovery Time Objective (RTO) : Maximum acceptable time for a service or system outage
                    ex. impact to customers

Recovery Point Objective (RTO): Acceptable state after recovery
                    ex. maximum acceptable data loss

# Prioritize Security

 - Zero-trust security 
    Every action requires authentication


# Architect for scalability &  Embrace FinOps
    - Large unforseen costs (cloud)
    - Missed opportunities for revenues
    - Manage cloud costs

## Batch Architectures

- processed in chunks
- real-time analysis is not critical

                                                        +------------------------+
                                                        |                        |
                                 +------------+         |   +----------------+   |
[ Data ] [ Data ] [ Data ] -->>  | Ingestion  | -->>    |   | Transformation |   |
                                 +------------+         |   +----------------+   |
   - Extract Transform Load                             |                        |
                                                        |   +----------------+   |
   - Extract Load Transform                             |   |    Storage     |   |
                                                        |   +----------------+   |
                                                        |                        |
                                                        +------------------------+

Data Mart: subset of a Data Warehouse (concentrating on Dept, Function, Business Area)

            - analytics and reporting 
                    - Sales
                    - Marketing
                    - Ops

FinOps: Cost Benefit Analsysis
        - Performance vs Cost


## Streaming Architectures

Data: stream of events

    [ Event Producer ]   [D] [D] [D]     [Event Consumer]  data available shortly after produced
                       continuous,                              (< 1s ) 
                       near real-time 
                       ingestion

    [ Event  ]    [Streaming]    [  Event  ]                  Downstream 
    [Producer]    [  Broker ]    [ Consumer] -------------->> Use Case 
       |                              |                         |
       |                              |                         |
     Data Source                Data processor            Analytics / ML




# Streaming Frameworks
    - Kafka:  Event Streaming 
    - Storm:  Streaming / Real-time Analytics
    - Samza:  Streaming / Real-time Analytics  


# Lambda Architecture

                   [ Batch Processing ]     -->> [Data Warehouse] 
                /                                                 \  [Serving] 
[Source System]                                                      [Layer  ]
                \                                                 /
                   [ Streaming Processing ] -->> [NoSQL DB ]     


# Kappa Architecture
    - 

[Source System]   -->>  [ Stream Processing ]  -->> [Serving Layer ]                                                       [Layer  ]

Unified Batch & Streaming

    Google Dataflow 
    Apache Beam 
    Apache Flink 

All data are events:
    - unbounded data: real-time event streams
    -   bounded data: data in batches


Batch is a "special case" of streaming


## Architecting for Compliance

    General Data Protection Regulation  (GDPR)
        - personal data
            - personal identifiable info
            - other data


    Health Insurance Portability and  Accountability Act (HIPAA)

    Sarbanes Oxley Act (SOX)



## Choosing the Right Technologies 
## 


Data Arch   --->>>   Tools
---------           ---------
 What                How
 Why
 When

Considerations: 
    Location: onprem, cloud, hybrid
    Cost Optimization
    build vs. buy
    team size / capabilities


# Location

on prem: company owns / maintains data stack
cloud  : cloud provider builds maintains hardware
hybrid : on prem + cloud


# Monolithic vs Modular

monolithic: tightly-coupled components
    - single codebase
    - one language / one technology
    - hard to maintain


monolithic: losely-coupled components
    - interoperability (can be integrated with other tools i.e. using parquet)
    - flexible & reversible decisions
    - continuous improvement


# Cost optimization and Business Value

Total Cost of Ownership (TCO): total estimated cost of a solution, project or initiative over its lifecycle
    direct costs                        indirect costs
        - salaries                      - netowrk downtime
        - cloud bills                   - IT support
        - software subscriptions        - loss of productivity


Capital Expenses   (CapEx): payment to purchase long-term fixed assets    [ On Prem]

Operational Expenses (OpEx): Expense assocaites with running day to day operations  [ Cloud ]



Total Opportunity Cost of Ownership (TOCO):  cost of lost opportunities incurred in choosing a particular tool / technology

Minimize TOCO 
  - build flexible systems
  - loosely-coupled components

Recognize components that are likely to change: 
    - Immutable tech
        - Object storage, Networking, SQL

    - Transitory tech
        - Stream processing, Orchestration, AI


FinOps:  Minimize TCO / TOCO
         Maximize revenue generation opportunities

    Cloud:  OpEx first
            Pay as you go tech
            modular options

# Build vs buy

Build Benefits: 
    - get exactly what is needed
    - avoid licensing fees
    - avoid being at mercy of a vendor

Existing Solutions: 
    - open source (community)
    - commercial open source (vendor)
    - proprietary non-open source
Considerations: 
    Team
        - does team have bandwidth to implement open source 
        - can proprietary service free up time
    Cost
        - cost of licensing fees
        - what is total cost to build and maintain
    Business Value
        - advantage by building own vs buy
        - avoiding work with no payback

# Server, Conatiner and Serverless Compute Options

        Server                          Container                                Serverless
- setup & manage                        - packaged code                         - no server to setup / maintain 
- update OS                                and dependencies                     - automatic scaling
- install / update                      - lightweight & portable                - availability & fault-tolerance
  packages                              - setup app code & deps                 - pay as you go
- patch software                                                                - AWS Lambda
- netowrk, scaling, security                                                    - run as needed / 
                                                                                  pay a little bit each time

When to User Serverless Services 
    - what is cloud cost 
        - high event rate can be costly
    - model and monitor
        - event rates / duration / cost
    - limitations
        - limits on execution frequency
            concurncy and duration

Best for     : simple / discrete tasks
Not Good when: alot of compute or memory

ELSE 
    containers / Kubernetes



# How Undercurrents Impact Your Desicisions

Security 
    - what are security features of the tool
        - only from reputable sources

Data Management 
    - how are data governance practices handled by provider
        - Data Breach
        - Compliance
        - Data Quality

DataOps 
    - What features are offered to 
        - Automation 
        - Monitoring
        - Service Leval Agreement (SLA)

Data Architecture
    - How does tool provide modularity and interoperabilty 
        - Loosely-coupled components

Orchestration 
    - Airflow
    - Dagster
    - Prefect
    - Mage

Software Engineering  
    - How much do you want to do?
        - team's capabilities
        - business value
        - build vs. buy
        - avoiding work with no payback  ( Avoid undifferentiated heafy lifting )



## Investigating Your Architecture on AWS
## 

# AWS Well-Architected Framework

6 Pillars: 

Operattional Excellence         Performance Efficiency   
Security                        Cost Optimization
Reliability                     Sustainability


Operational Excellence
    - how to develop and run workloads on AWS more effectively
    - monitor your systems to gain insght into your operations
    - continuously improve your porcesses and procedures to deliver business value

Security
    - how to take advantage of cloud technologies to protect your data, systems and assets

Reliability
    - everything: designing for reliability to planning for failure and adapting to change 

Performance Efficiency   
    - data driven approach to buile high-perfromance architecture
    - evaluate ability of resource to efficiently meet system requirements 
    - maintain efficiency as demand changes and tech evolves

Cost Optimization
    - build systems to deliver max biz value at lowest possible price point
    - AWS Cost Explorer / Cost Optimzation Hub

Sustainability
    - consider environmenta impact of the workloads your running on the cloud
    - reduce energy consumption and increase efficiency across all system components



Lens: extension of Well-Architected Framework that focuses on area, industry or tech stack.
      and provides guideance to those contexts

Data Analytics Lens: AWS > Documentation > AWS Well-Architected > AWS Well-Architected Framework


Apache Benchmark for web stress testing


LAB: 

ALB / ELB 
de-c1w3-alb-1436900260.us-east-1.elb.amazonaws.com


.