Andrew Ng

Joe Reis
Oreilly: Fundamentals of Data Engineering




## Intro to Data Engineering
## Week 1

Data Centric AI: discipline of systematically engineering the data used to build an AI system



# Syllabus

Intermediate Python skills:
    - w3 School Pandas tutorial: https://www.w3schools.com/python/pandas/default.asp
    - Kaggle Pandas tutorials  : https://www.kaggle.com/learn/pandas

Basic SQL skills:
    - SQLBolt Tutorials course: https://sqlbolt.com/


AWS skills :
    - AWS Cloud Practioner Essentials: https://www.coursera.org/learn/aws-cloud-practitioner-essentials
    - AWS Cloud Technical Essentials : https://www.coursera.org/learn/aws-cloud-technical-essentials


## Digial copy of Fundamentals of Data Engineering
https://go.redpanda.com/fundamentals-of-data-engineering


# DeepLearning AI Community link
https://community.deeplearning.ai/invites/RsdEnVWirt




# Data Engineering Defined

    - originally seen as byproduct or "exhaust"
    - data has business value
    - data engineering work:

        [ ingestion ] [ transformation ] [ serving ]
        --------------------------------------------
                      [ data storage ] 

Data engineering is the development, implementation, and maintenance of systems and processes
that take in raw data and produce high quality consistent information that supports downstream 
use cases such as analysis and machine learning.

Data engineering is the intersection of:
         security, data management, DataOps,
        data architecture, orchestration, and software engineering


# Data Engineering Lifecycle                                                   
                    + -----------------------------------------------------+       +-------------------+
+------------+      |                                                      |       |                   |
|            |      | [ ingestion ] ->  [ transformation ] ->  [ serving ] |       |  Analytics        |
| Generation | -->> | -----------------------------------------------------| --->> |  Machine Learning |
|            |      | [[  ............    data storage   ............   ]] |       |  Reverse ETL      |
+------------+      |                                                      |       |                   |
      ^             +------------------------------------------------------+       +-------------------+
      |                                                                                    |
      |                                                                                    |
      +-----------------------------------Reverse ETL--------------------------------------+

==========================================================================================================

  [Security]  [Data Mangement]  [DataOps]  [Data Architecture]  [Orchestration]  [Software Engineering]




Data Pipeline: combination of architecture, systems, and processes that move data through the 
               stages of the data engineering lifecycle

in short: get raw data, turn in into something useful and then deliver



  Downstream                                      Data                        Upstream
Stakeholders                 --+                 Engineer                     Stakeholders
  - Analysts                   |               +----------+                     - Software 
  - Data Scientists            | How often <-> |          | <-> Volume             Engineers
  - Machine Learning Engineers | What Info <-> |   Data   | <-> Frequency
  - Salespeople                | How much  <-> |  Engineer| <-> Format
  - Marketing Professionals    |  latency      |          | <-> Data Security
  - Executives               --+               +----------+ <-> Reg Compliance


Look for business value!!
    - don't get hung up on the technology
    - focus on the value that the tech can provide !!!!


# System Requirements
    - Business Requirements
            - revenue growth
            - increase user base
    - Stakeholder Requirements
            - needs of individual in orgranization
    - System Requirements
            - Functional Requirements
                - "The What"
                    - Updates to a DB
                    - Alert a user about data anomaly
            - Non-Functional Requirements
                - "The How"
                    - Tech specs of ingestion or orchestration 
                    - How you'l meed end user's needs

# Reguirements Gathering:                    --+                           +--
    - Business & Stakeholder Requirements    |                           | High Level       Downstream
    - Features and Attributes                |  <<-- Data Engineer  -->> | Goals &     <<-- Stakeholders 
    - Memory & Storage Capacity              |                           | needs
    - Costs & Security Constraints           |                           |
                                           --+                           +--

Jordan Morrow: Founder of Data Literacy 
    - ability to read, work with and analyze data

    - Understand audience
    - Understand business goals of company
    - Understand goals of stakeholders

Work on People Skills
Know how business operates

# Reguirements Gathering Conversation 

- what do they need
- how do they will use it
- when do they need it
    - is there value in real time ?
- format and time scope

# Stakeholder Needs into Specific Requirements

# Key Elements:
    - Learn what existing data systems or solutions are in place
    - Learn what pain points or problems there are with the existing solutions
    - Learn what actions stakeholders plan to take with the data
        NOTE: Repeat what you learned back to your stakeholders
    - Identify any other stakeholders you'll need to talk to if you're still missing information

Think like a Data Engineer:

1. Identify business goals &  stakeholder needs
    A. Identify business goals and stakeholders you will serve 
    B. Explore existing systems and stakeholder needs
    C. Ask stakeholders what actions they will take with the data product

2. Define System Requirements
    A. Translate stakeholder needs to Functional Requirements
    B. Define non-functional requirements
    C. Document and confirm requirements with stakeholders

3. Choose tools & technologies
    A. Itentify tools & tech to meet non-functional requirements
    B. Perform cost / benefity analysis and choose between comparable tools & tech
    C. Prototype and test your system, alaign with stakeholder needs

4. Build, evaluate, iterate & evolve
    A. Build & deploy production data system
    B. Monitor, evaluate and iterate on you system to improve it
    C. Evolve your system basedon stakeholder needs

     +-->  [1] -> [2] -> [3] -> [4] ---+
     |                                 |
     +---------------------------------+



community.deeplearning.ai 

## Data Engineering on the Cloud
## 


# Intro to AWS Cloud

- compute
    - EC2
    - Container hosting services (EKS)
    - Serverless functions
- storage
    - Simple Storage Service (S3)
    - Elastic Block Store
    - Database Services
- networking
    - Virtual Private Clouse

# AWS Regions and Availability Zones

AWS Regions: collections of data centers within geographical areas where you can use AWS Services 
    - high reliability, availability, resilience
    - named for geographical name and region code: us-east-1 
            Region: us-east-1     AZ: us-east-1a, 1b, 1c 
            Region: eu-central-1  AZ: eu-central-1a, eu-central-1b
    - Each region contains at least 3 isolated and physically separated AZs  **


Availability Zones: Each Region comprised of multiple AZs 
    - multiple data centers make up Availability Zone
    - multiple AZs make up an AWS Region
    - physically separated by a meaningul distance from any other AZ 
        - all within 100km of eachother **

34 AWS Regions  **
108 Availability Zones  **

NOTE: Costs may differ between regions
      Certain regulations may rquire hosting your data in specific geographical regions
      Service availability no available in all regions


https://aws.amazon.com/about-aws/global-infrastructure/regions_az/

https://aws.amazon.com/about-aws/global-infrastructure/


# Intro to AWS Core Services

Compute: 
    - Elastic Compute Cloud (EC2) 
        - VM  / EC2 instance
    - Lambda
    - Elastic Container Service (ECS)
    - Elastic Kubernetes Service (EKS)


Network: 
    - Virtual Private Network (VPC): Private network you can crate and place resources into 
        - isolated from other networks
        - choose size of private IP space
        - subnets: partition into smaller networks
    NOTE: VPC spans AZs within a region
            - cannot span across regions
            - create a VPC in every region to operate in
        
        - data and resources don't leave region unless you build solution to do so

      Region                                                                       
    ┌────────────────────────────────────────────────────────────┐         
    │     VPC                                                    │         
    │   ┌────────────────────────────────────────────────────┐   │         
    │   │    AZ 1                   AZ 2                     │   │         
    │   │   ┌──────────────────┐    ┌─────────────────┐      │   │         
    │   │   │  ┌────────────┐  │    │  ┌───────────┐  │      │   │         
    │   │   │  │   Public   │  │    │  │   Public  │  │      │   │         
    │   │   │  │   Subnet   │  │    │  │   Subnet  │  │      │   │         
    │   │   │  └────────────┘  │    │  └───────────┘  │      │   │         
    │   │   │  ┌────────────┐  │    │  ┌───────────┐  │      │   │         
    │   │   │  │   Private  │  │    │  │   Private │  │      │   │         
    │   │   │  │   Subnet   │  │    │  │   Subnet  │  │      │   │         
    │   │   │  └────────────┘  │    │  └───────────┘  │      │   │         
    │   │   └──────────────────┘    └─────────────────┘      │   │         
    │   └────────────────────────────────────────────────────┘   │         
    └────────────────────────────────────────────────────────────┘         
                                                                                   
                                                                                   
Storage: 
    Object Storage:  S3 / unstructured data

    Block Storage :  EBS / db storage, vm file system                     
                        - attatch EBS as volumes to EC2 instance                                                                 

    File Storage  :  Data organized into files and directories  / Elastic File System (EFS)                                                                                    
                                                                                   
                                                                                   
    Relational Data Service :  RDS / cloud-based relational database service
    Redshift :  data warehouse services to store transform and serve data 


Security: 
    Shared responsiblity model
        AWS  is responsible for secrurity OF the cloud
            - hardware to hypervisor
        User is responsible for secrurity IN the cloud
            - VM, OS, Firewall Config, Software, Access and Data





Resources:


https://go.redpanda.com/fundamentals-of-data-engineering

https://www.linkedin.com/pulse/what-recovering-data-scientist-joe-reis/

https://datastackshow.com/podcast/29-the-present-and-future-of-data-engineering-with-joe-reis-and-matthew-housley-from-ternary-data/

https://www.listennotes.com/podcasts/monday-morning/154-sol-rashidi-getting-b2Bjy6Fr-4P/#google_vignette


## AWS
https://aws.amazon.com/getting-started/

https://www.coursera.org/learn/aws-cloud-practitioner-essentials

https://www.coursera.org/learn/aws-cloud-technical-essentials

https://aws.amazon.com/certification/certified-data-engineer-associate/


## Intro to Data Engineering
## Data Engineering Lifecycle
## Week 2


# Data Generation in Source Systems

Common Source Systems  
 - Databases: Relational / NoSQL
 - Files    : text , audio , video
 - API      : request / response
 - Data Sharing Platform
 - IoT Devices

Source systems are unreliable:
    - can go down
    - format / schema changes

Data Engineer needs to work with Source Systems 
    - How they generate data
    - How data changes over time
    - How changes will impact downstream systems

# Ingestion

ingestion: moving raw data from source systems into data pipeline for processing

    - frequency:  batch / streaming

    streaming: near real-time (data available shortly after it is produced)
        - event streaming platform / message queue

    - What would be done with the data in real-time

# Storage

Raw Hardware 
    Solid-State Storage
    Magnetic Storage
    RAM

Process components
    Networking          Serialization
    CPU                 Compression
    Caching

Storage Systems
    DBMS                    Object Store
    Apache Iceberg          Cache / Memory based storage
    Streaming Storage

Storage Abstraction
    Data Warehouse      Data Lake
    Data Lakehouse

    choose config patterns: Latency, Scalability, Cost

Storage Hierarchy
 [ Storage Abstractions]
 [   Storage Systems   ]
 [     Raw Hardware    ]


# Data Transformation / Data Engineering Lifecycle

transformation:  turn raw data into useful data / valuable

queries, modeling, transformation

query: issue a request to read records from data source

data modeling: choosing a coherent structure for data to make it useful for business

data transformation: data manipulated, enhanced and saved for downstream use


# Serving Data

Giving stakeholders the opportunity to extract business value from data

analytics:  process of identifying key insights and patterns within data
        - business intelligence: 
                explore historical and business data to discover insights

        - operational analytics: 
                monitoring real-time data for immediate action
        
        - embedded analytics
                external or customer-facing analytics
                    - dashboards, etc

machine learning: 
        - model training
        - real-time inference
        - track data history and lineagoe

reverse ETL learning:  take transformed data, analytics, machine learnig model output 
                       and feed back to source systems


## Data Engineering Lifecycle
## Data Undercurrents
## Week 2

Data Undercurrents:
    Security
    Data Management
    DataOps
    Data Architecture
    Orchestration
    Software Engineering

# Security 

Least Privilege:  Give users or apps access to only the essentail data / resources they need 
                  for only the duration required

Data Sensitivity: Give user access to data only when necessary


Security in the Cloud:
    - Identity and Access Management
    - Encryption Methods
    - Networking Protocols

defensive mindset
    - be cautious with sensitive data
    - design for potential attacks

# Data Management  

    Data Management Association International (DAMA)
        - provides Data Management Book of Knowledge (DMBOK)

DMBOK defines data management as the development, execution, and supervision of plans,
programs, and practices that deliver, control, protect, and enhance the value of
data and information assets throughout their life cycles


Data governance: data management fucntion to ensure the quality, integrity, security and
                 usability of the data collecte by an orgranization
                 
Data quality
    High Quality Data                       Low Quality Data
    -----------------------                 --------------------
        Accurate                                Inaccurate
        Complete                                Incomplete
        Discoverable                            Hard to find
        Available in a timely manner            Late
    ( exactly what stakeholders expect )        (unusable)


# Data Architecture  

 The design of systems to support the evolving data needs 
 of an enterprise, achived by flexible and reversible decisions 
 reached through a careful evaluation of trade-offs

    - ongoing effort
    - evolving

Principle of Good Data Architecture 
    1. Choose common components wisely
    2. Plan for failure
    3. Architect for scalability
    4. Architecture is leadership 
    5. Always be architecting
    6. Build loosely coupled systems
    7. Make reversible decisions
    8. Prioritize security (but it is toward the bottom)
    9. Embrace FinOps: Finance + DataOps


# Data Ops  

DataOps: Improves the dev process and quality of data products
         set of cultural habits
            - Communication and Collaboration
            - Continuous Improvement
            - Rapit Iteration

DataOps from Agile Methodology

Pillars of DataOps:
    - Automation
    - Observability & Monitoring
    - Incident Response

Goal: Provide high-quality data products 

Automation:
    Automated Change management:
        Code /  Config / Environment 
        Pipelines
        Data 
    Scheduling
    Orchestration framework (Airflow)

Observability & Monitoring:

Incident Response
    - root cause
    - quick resolution
    - identify tech and tools
    - coordinate efforts of data team


# Orchestration

Orchestration Frameworks:
    - Airflow
    - dagster
    - Prefect
    - Mage

  - Automate pipeline with complex dependencies
  - Monitor pipeline

Directed Acyclic Graph (DAG)
    - how data flows thru pipeline
    - flow of data in one direction (directed)
    - acyclic (no loops)
    - graph: composed of nodes and edges
        i.e. flowchart

# Software Engineering

design, development, deployment and maintenance of software apps

    - open source fameworks
    - Infrastructure as code
    - Pipeline as code
    - Problem solving


## Practical Examples on AWS

# Data Engineering Lifecycle on AWS

# Data Engineering Lifecycle                                                   
                    + -----------------------------------------------------+       +-------------------+
+------------+      |                                                      |       |                   |
|            |      | [ ingestion ] ->  [ transformation ] ->  [ serving ] |       |  Analytics        |
| Generation | -->> | -----------------------------------------------------| --->> |  Machine Learning |
|            |      | [[  ............    data storage   ............   ]] |       |  Reverse ETL      |
+------------+      |                                                      |       |                   |
      ^             +------------------------------------------------------+       +-------------------+
      |                                                                                    |
      +-----------------------------------Reverse ETL--------------------------------------+

Source Systems
 Databases
    - Amazon Relational Database Service (RDS)
        - managed db
        - mysql / postgresql

    - Amazon DynamoDB
        - Serverless NoSQL
        - stand-alone tables (virtually unlimited in size)
        - flexible schema
        - low latency / large volumes (gaming, IoT)
 
 Streaming
    - Amazon Kinesis Data Streams
    - Amazon Simple Queue Service (SQS)
    - Amazon Managed Streaming for Apache Kafka (MSK)
        - AWS manages infrastructure

 Ingestion
   From a DB    
    - Amazon Database Migration Services (DMS)
        - migration / replicate data from source to target automated

    - AWS Glue
        - supports data integration processes

   From Streaming    
    - Amazon Kinesis Data Streams
    - Amazon Data Firehose
    - Amazon SQS
    - Amazon MSK

 Storage
   Traditional Data Warehouse
    - Amazon Redshift
    - Amazon S3
          +-->> Lakehouse: access structured data in data warehouse 
                           and unstructured data in an object store data lake 

 Transformation
   Data Processing Tools
    - AWS Glue
    - Apache Spark
    - dbt


 Serving
   Business Intelligence / Analytics 
    - Amazon Athena   : query structured / unstructured data
    - Amazon Redshift

   Dashboarding Tools
    - Amazon QuickSight 
    - Apache Superset 
    - Apache Metabase 

   AI / Machine Learning 
    - batch data for model training
    - vector databases

# Undercurrents on AWS

Undercurrents (get it, they are under the dashed line ....)

==========================================================================================================
  [Security]  [Data Mangement]  [DataOps]  [Data Architecture]  [Orchestration]  [Software Engineering]



 Security
   Shared Responsibility Model
    - AWS      responsible for data center and services they provide
    - Customer responsible for security of systems built with AWS resources 

    - Identity and Access Management (IAM)
        - roles and permissions to access AWS resources

       IAM Roles:
        - give users / apps access to temporary credentials
        - provide appropriate AWS API permissions to various tools / data storage areas

    - Amazon Virtual Private Cloud 
        - private network

    - Security Groups
        - Instance level firewalls


 Data Management
    - AWS Glue               +   | Discover, create, manage metadata for data stored in 
    - AWS Glue Crawler       | --| S3 or other storage and database systems
    - AWS Glue Data Catalog  +   |

    - AWS Lake Formation:  Centrally manage and scale fine-grained data access permissions

 DataOps
    - AWS CloudWatch: Collects metrics and provides monitroing features for cloud resources, applications, 
                      and on-premises resources

    - AWS CloudWatch Logs: Sore and analyze operational logs

    - AWS Simple Notification Service (SNS):  setup notifications between apps or 
                  via text / email that are triggered by events within your system
 
    OpenSource monitoring: Monte Carlo, Bigeye

 Orchestration 
    - Apache Airflow
        - alternatives: Dagster, Prefect, Mage

 Architecture 
    - AWS Well-Architected
        Operactional Excellence    Performance Efficiency
        Security                   Cost Optimization
        Reliability                Sustainability

 Software Engineering 
    - AWS Cloud9: IDE for development within a browser
        [Cloud9]  ---hosted on ---> [EC2]

    - AWS CodeDeploy: Automated Code Deployment

    - repos: git / github



NOTE: Community Forum of lab issues: 
https://community.deeplearning.ai/c/course-q-a/data-engineering-specialization/de-course-1/467


Lab Scenario:

RDBMS: product, productlines, employees, offices, customers, orders, orderdetails, payments
  |
  +--> Transformed data: fact_orders, dim_customers, dim_products, dim_locations














.