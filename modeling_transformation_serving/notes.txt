## Data Modeling, Transformation and Serving
## Course 4
## Week 1

# Introduction to Data Modeling for Analytics

Overview

Data Modeling: A data model organizes and standardizes data in a precise structured representation 
              to enable and guide human and machine behavior, inform decision-making and facilitate actions 




Define the structure, relastionshiops and meaning of data.

Structure the data in a way that connects back to the organization
    - Data is understandable and valuable


Good Data Models 
    - Reflect the business goals and logic while incorporating business rules
    - Ensure compliance with operational standards and legal requirements
    - Outline the relationships between business processes
    - Serve as a powerful communication too, creating a "shared language"

ex. what is "an active users"



Poor Data Models 
    - Don't reflect how the busienss operates
    - Create more problems than they solve
    - Provide stakeholders with innaccurate information 
        and create confusion 


Targeted Data Modeling Approach
- Focus on specific business domains 

    Marketing:  Better understand customer behavior and campaign effectiveness
    Finance  :  Analyze spending patterns and identify cost-saving opportunities
    Machine Learning  :  Drive better decision-making and impactful AI models

# Conceptual, Logical and Physical Data Modeling


Conceptual:
    - describes business entities, relationships and attributes
    - business logic and rules
        - Entity - Relationshiop Diagram (ER)  (1 to 1, 1 to Many)

Logical 
    - details about the implementation of the conceptual model
        - column types, primary / foreign keys

Physical 
    - Details about the implementation of the logical model in a specific DBMS
    - Configuration details 
        - Data storage approach
        - Partitioning details
        - Replication details


# Normalization 


Normalization:  A data modeling practice typically applied to relational databases
                to remove the redundancy of data within a database and ensure referential 
                integrity between tables.

Edgar Codd's Objectives of Normalization 

    - To free the collections of relations from undesireable insertion, 
       update and deletion dependencies

    - To reduce the need for restructuring the collection of relations as 
       new types of data are intruduced



    Reduce Data Redundancy: Eliminating duplicate data saves storage space and ensures consistency across the database.
    Improve Data Integrity: Ensuring that each piece of data is stored in only one place reduces the likelihood of data anomalies and maintains the accuracy of the data.
    Enhance Update/Delete Query Performance




SalesOrders 
Denormalized Form 
 - contains redundant data 
 - contains nested data

OrderID         OrderItems              CustomerID      Customer Name   address     OrderDate

 101           [{                             5               Joe Reis    1st. St     1/08/2024
                "sku":1,
                "price":50,
                "quantity":1,
                "name": "Thigamajig",
               },{
                "sku":2,
                "price":50,
                "quantity":1,
                "name": "Whatchmacallit",
               }]

 102           [{                             5               Matt Housley    1st. St     1/08/2024
                "sku":3,
                "price":75,
                "quantity":1,
                "name": "Whoozeewhatzit",
               },{
                "sku":1,
                "price":50,
                "quantity":1,
                "name": "Thigamajig",
               }]
 

First Normal Form (1NF): 
    - Each column must be 
        - unique
        - have a single value
    - Unique Primary Key

  composite key
+---------------+
OrderID   Item Number  sku    price   quantity    name          CustomerID  CustomerName        address     OrderDate
100         1           1       50      1       Thingamajig         5           Joe Reis        1st St.     1/8/2024
100         2           2       25      2       Watchamcallit       5           Joe Reis        1st St.     1/8/2024
101         1           3       75      1       Whoozeewhatzit      7           Matt Housely    2nd Ave.    1/8/2024
101         2           2       25      3       Watchamcallit       7           Matt Housely    2nd Ave.    1/8/2024
---                                                                    +----------------------------------------------+
 |                                                                                           | 
 +------------------------------------------------------------------------------------------+
                                                                         Non-key columns that depend on OrderID 
                                                                    i.e. if you know an OrderID you would know the customer                        
Second Normal Form (2NF):
    - Requirements of 1NF must be met  
    - Partial dependencies should be removed

NOTE: 
Partial Dependency: A subset of non-key columns that depend on some columns in the composite key

Split to two tables: OrderItems and Orders

OrderItems                                                       Orders                                                       
+---------------------------------------------------------+     +----------------------------------------------------+
OrderID   Item Number  sku    price   quantity    name          OrderID CustomerID  CustomerName        address     OrderDate
100         1           1       50      1       Thingamajig       100       5           Joe Reis        1st St.     1/8/2024 
100         2           2       25      2       Watchamcallit     101       7           Matt Housely    2nd Ave.    1/8/2024  
101         1           3       75      1       Whoozeewhatzit      
101         2           2       25      3       Watchamcallit       


These two tables now have a Transitive Dependency 
Transitive Dependency:
  - A non-key column depends on another non-key column 

    OrderItems:
        price and name depend on sku

    Orders:
        CustomerName and address depend on CustomerID 


Third Normal Form (3NF):
    - Requirements of 2NF must be met  
    - Transitive dependencies should be removed


OrderItems                                  Orders                           Customers
+------------------------------------+     +----------------------------+   +------------------------------------+
OrderID   Item Number  sku    quantity      OrderID CustomerID  OrderDate    CustomerID  CustomerName     address
100         1           1       50            100       5        1/8/2024     5           Joe Reis        1st St.  
100         2           2       25            101       7        1/8/2024     7           Matt Housely    2nd Ave. 
101         1           3       75     
101         2           2       25    

                                                                                Items                                  
                                                                               +----------------------------+  
                                                                                sku   price    name
                                                                                1       50      Thingamajig 
                                                                                2       25      Watchamcallit   
                                                                                3       75      Whoozeewhatzit      




# Dimensin Modeling: Star Schema
                                                                         
                                                                [Dim]  [Dim]  [Dim]
    Fact Table     :  business measures                            \     |     /
    Dimension Table:  Contextual Informtion                         \    |    /
                                                                    \   |   / 
                                                            [Dim]----- [Fact] -----[Dim] 
                                                                         |
                                                                         |
                                                                       [Dim]

Fact Table: contains quantitative business measurements that result from a business event or process
       - Each row contins the facts of a particular business event 
       - Data in Fact table is immutable (append-only)
       - Typically narrow and long
            - not alot of columns but alot of rows


Business Event          Facts                          Grain (detail of Fact)                       Dimensions
order a ride share      Trip duration, trip price      - all rides by all customers in one day,     - customers
                        tip paid, trip delays, etc.    - all rides by one customer on one day       - drivers
                                                            - one ride by one customer              - trip locations

Atomic Grain: most detailed level of a business proces


Dimension Table: Provide the reference data, attributes and relational context for the events in the fact table
        - Describe the events' what, who, where and when  
        - Typically wide and short
            - many columns fewer rows 



Conformed Dimension:  Dimension table used with multiple fact tables

                [Dim]  [Dim]  [Dim]                [Dim]  [Dim]  [Dim]
                  \     |     /                      \     |     /
                   \    |    /                        \    |    /
                    \   |   /                          \   |   /
           [Dim]----- [Fact] ----------[Dim]------------ [Fact] -----[Dim] 
                        |                                   |
                        |                                   |
                      [Dim]                               [Dim]

Each DIM   has a PKey
Fact table has a PKey
Fact connected to Dim via FKey

Best practice:  create a surrogate key 
                    - used to combine data from differnt source systems with 
                      natural primary keys that are in different formats

                    - used to decouple the primary key of the star schema from source systems


Analytical Queries:
    - Apply aggregate queries to find the sum, average, maximum of fact
    - Use dim tables to filter or group facts


         [ dim_customers ]                   [ fact_orders ]               [ dim_products ]
         customerNumber (pk)+--+            OrderLineNumber (pk)    +---- productCode (pk)
         customerName          |            OrderNumber     (pk)    |     productName
         contactLastName       +-----------+customerNumber  (fk)    |     productLine
         contactFirstName                   postalCode      (fk)-+  |     productScale
         phone                              productCode     (fk)-|- +     productVendor
         addressLine1                       orderDate            |        productDescription
         addressLine2                       quantityOrdered      |        productLineDescription
                                            priceEac             |
                                            buyPrice             |  
                                            orderAmount          | 
                                            MSRP                 | 
                                                                 | 
                                            [dim_location]       | 
                                            postalCode (PK) +----+
                                            city 
                                            state 
                                            country 


ex. Find the total sales amount for each product line within the usa
SELECT  
  dim_product.productLine, 
  SUM(fact_orders.orderAmount) AS total_sales
FROM fact_orders 
JOIN dim_product ON 
  fact_orders.productCode =       
  dim_products.productCode       
JOIN dim_locations ON 
  fact_orders.postalCode =       
  dim_locations.postalCode
WHERE dim_locations.country = 'USA' 
GROUP by dim_products.productLine         


Star Schema vs 3NF

 - Star Schema organizes data so it's easier for business users to understand, navigate and use
 - Star Schema results in simpler queries with fewer joins



## Data Modeling Techniques 

# Inmon vs kimball Data Modeling Approaches for Data Warehouses


Inmon Data Modeling Approach

  datawarehouse: A subject-oriented, integrated, nonvolatile and time-variant collection of data 
                 in support of management's decisions.

The data warehouse contains granular corporate data.  Data in the data warehouse is able to be used
for many different purposes, including sitting and waiting for future requirements which are unknown today.


             +-------------------- Data Warehouse ----------------+    +----> [Sales: Data Mart]
 [Source1]-+   Major Subject Areas              Subject Details        |       (star schema)            +---------------+
 [Source2]-+>  [Products]  [Orders]           - business keys          |                                |   Reports     | 
 [Source3]-+   [Customers] [Shipments]        - relationships       ---+----> [Marketing Data Mart]     |     &         |
                                              - attributes             |                                |   Analysis    |
                      (Highly normalized (3NF))                        |                                |               |
                                                                       |                                |               |
                                                                       +----> [Purchasing Data Mart]    +---------------+


Kimball Data Modeling Approach:
  Kimball's approach effectively allows you to server data that's structured as star schemas 
  (or similar variants) directly from the data warehouse
      - faster modeling and iteration 
      - more data redundancy and duplications

                [Dim]  [Dim]  [Dim]                [Dim]  [Dim]  [Dim]                [Dim]  [Dim]  [Dim]      +--------------+
                  \     |     /                      \     |     /                      \     |     /          |              |
[Source1]+         \    |    /                        \    |    /                        \    |    /      -->> |  Reports     |
         |          \   |   /                          \   |   /                          \   |   /            |     &        |
[Source1]+> [Dim]----- [Fact] ----------[Dim]------------ [Fact] ----------[Dim]----------- [Fact] -----[Dim]  |  Analyssis   |
         |              |                                   |                                 |                |              |
[Source1]+              |                                   |                                 |           -->> |              |
                      [Dim]                               [Dim]                             [Dim]              +--------------+


Kimball Data Modeling Approach:
  - Quick insights are your highest priority
  - Rapid implementation and iteration

Inmon Data Modeling Approach
  -  Data quality is your highest priority
  -  The analysis requiremetns are not defined


# From Normalized Model to Star Shema






Normalized Data

OrderItems                     Items                                  
+-------------------+         +---------+  
order_id         (pk)           sku    (pk)
item_line_number (pk)           price    
itme_sku         (fk)           name
item_quantity                   brand

Orders                         Stores                                  
+-------------------+         +----------------+   
order_id         (pk)          store_id     (pk)
customer_id      (fk)          store_name
store_id         (fk)          store_city
order_date                     store_zipcode

Customers
+-------------------+
customer_id      (pk)
customer_name     
customer_zipcode     

                                                                               
                                                                               
Understand the needs of the business
1. select the business process   -->> 2. Declare the grain   -->> 3. Identify the dims  -->> 4. Identify the facts


User Needs:
- analyze sales data 
  - which products are sellin in which stores on a given day
  - differences in the sales between the stores
  - which product brands are most popular


Business process:  Company's sales transactions

Atomic Grain:  Individual product item in a sales transaction


Dims:   analysts wants stores, dates, brands 
    [ dim_stores ]    [ dim_items ]   [ dim_date ]

[dim_stores]
store_key (PK)           ## NOTE MD5 is a has function to create a surrogate key
store_id                 SELECT MD5(store_id) as store_key,    
store_name                      store_id, store_name,   store_city,  store_zipcode
store_city               FROM stores;
store_zipcode 



[dim_items]               SELECT MD5(sku) as item_key, sku, price,   grand
item_key(pk)              FROM dim_items;        
sku                       
price 
brand

Date Table 
Date            Year    Quarter     Month   Day-of-wek 
2022-03-01      2022       1          3       Tuesday
2022-03-02      2022       1          3       Wednesday

[dim_date]          ## NOTE: using pgSQL generate_series()  
date_key (pk)       select date_key, 
day_of_week           EXTRACT(DAY     FROM date_key) as day_of_week,
month                 EXTRACT(MONTH   FROM date_key) as month,
quarter               EXTRACT(Quarter FROM date_key) as quarter,
Year                  EXTRACT(year    FROM date_key) as YEAR
                      FROM generate_series ('2020-01-01'::date, '2025-01-01'::date, '1 day'::interval)  as date_key


[fact_order_items]          
fact_order_key (pk)
order_id                SELECT MD5(CONCAT(OrderItems.order_id, OrderItems.item_line_number)) as fact_order_key,
item_line_number              OrderItems.order_id,
store_key                     OrderItems.item_line_number,
item_key                      MD5(OrderItems.store_id) as store_key,
date_key                      MD5(OrderItems.item_sku) as item_key,
item_quantity                 Orders.order_date as date_key, 
item_price                    OrderItems.item_quantity,
                              Items.price as item_price
                        FROM OrderItems
                              JOIN Orders on Orders.order_id = OrderItems.order_id
                              JOIN Items  on Items.sku       = OrderItems.item_sku


Model data with dbt
  - connects to data warehouse
  - transforms and validates your data with the data warehouse
  - generates the sql code behind the scense to transform your data
  - CAN'T join together data from different sources or move transformed data to another target system

AWS Glue 
  - can connect to differnt sources, apply transformations and store processed data somewhere else




# Conversation about dbt with Drew Banin


dbt: applies business logic rules to data to become information (?)

    - setup rules with SQL and python
    - instruct data warehouse (db) to transform (t) in place 


pre-dbt:  sql scripts,  wildwest etc.



post-dbt:  version control code
           community of practice:  analytics engineer, between engineers and business

dbt: "ruby on rails for data"
  there is a "dbt way"

Things to standardize around dbt: 
 - use a sql style code 
 - column names
 - how logic is transformed to: 
    - staging tables 
    - intermediate transformations
    - marts / dimensional models
 - software engineering best practices 
    - modular code
    - test as you go / unit testing
    - code review 
    - CI/CD etc.


# Data Vault


3 layers to Data Vault

                      [Enterprise]                  [ Information ]
[ Staging ]           [Data      ]                  [ Delivery    ]
                      [Warehouse ]

                     Data Vault Model                 
insert only         - hubs, links, satellites         Data delivered to Data Marts

                   separate business objects and 
                   their relationshipsfrom their 
                   descriptive attributes


- Only change the structure in which the data is stored: 
    - Allows you to trace the data back to its source
    - Helps you avoid restructuring the data when business requirements change 


Data Vault Model: Three types of tables 

Hub : Stores a unique list of business keys to represent a core business concept: 
        Customers, products, employees, vendors


Link: Connects two or more hubs. Represents relationship, transaction, event between 
      two or more business concepts 


Satellite: Contains attributes that provide context for hubs and links  
        eg. User will query a hub which will link to a satellite table containing 
        the query's relevant attributes.


                  Data Vault Model 

+-----------------------------------------------------------------------+
  [ Hub ]-------[ Link ]-------[ Hub ]-------[ Link ]-------[ Hub ]
     |             |              |              |             | 
  [Satellite]   [Satellite]    [Satellite]   [Satellite]    [Satellite]


+-----------------------------------------------------------------------+


Data Vault Steps 

OrderItems                     Items                                  
+-------------------+         +---------+  
order_id         (pk)           sku    (pk)
item_line_number (pk)           price    
itme_sku         (fk)           name
item_quantity                   brand

Orders                         Stores                                  
+-------------------+         +----------------+   
order_id         (pk)          store_id     (pk)
customer_id      (fk)          store_name
store_id         (fk)          store_city
order_date                     store_zipcode

Customers
+-------------------+
customer_id      (pk)
customer_name     
customer_zipcode     

                                                                               
Step 1:  Model the Hubs, which contain business keys 

        Identify business keys: 
          - what is the identifiable business element ? 
          - How do users commonly look for data ?
          - business key: 
              - colmn(s) used by the business to identify and locate the data
              - not be a key generated in or tied to particular source system 

      
    [  Customer   ]             [  Order   ]
     customer_hash_key (PK)     order_hash_key (PK)
     customer_id                order_id
     load_date                  load_date
     resource_source            resource_source 

    [  item   ]                 [  Store   ]
     item_hash_key (PK)         store_hash_key (PK)
     sku                        store_id
     load_date                  load_date
     resource_source            resource_source 


  Hubs:  should contain 
    - The business key 
    - The hash key
        - Calculated as a hash of the business key 
        - Used as the Hub primary key
    - The load date    : date on which the business key was first loaded
    - The record source: the source of the businexx key 


Step 2:  Model the links, link table to connect 2 or more hubs



                    (link table)                                   (link table)
[  item   ]---------[Item_Order ]------------[  Order   ]---------[Order_Customer] -------------[  Customer   ]             
item_hash_key (PK)  item_order_hash_key(pk)  order_hash_key (PK)   customer_order_hash_key(pk)  customer_hash_key (PK)     
sku                 item_hash_key            order_id              order_hash_key               customer_id                
load_date           order_hash_key           load_date             customer_hash_key            load_date                  
resource_source     sku                      resource_source       order_id                     resource_source            
  |                 order_id                      |                customer_id
  |                 load_date                (link table)          load_date
  |                 record_source            [Order_Store]         record_source
  |                       |                  order_store_hash_key
 (satellite)        (satellite)              order_hash_key
[item]              [item_order]             store_hash_key 
item_hash_key(pk)                            order_id
load_date(pk)                                store_id
price                                        load_date
name                                         record_source
brand                                             |                     (satelite)
record_source                                [  Store   ]-------------[  Store   ]
                                             store_hash_key (PK)      store_hash_key (PK)
                                             store_id                 load_date(PK)
                                             load_date                store_name
                                             resource_source          store_zipcode
                                                                      record_source

Each table must contain must containt the primary and business keys from its parent hubs,
the load date of a row, and the source for the record.

For each table, the primary key consists of a hash calculated based on the business keys of the parent hubs.

With link tables, you can easily add new relationships or update a current relationship
without having to re engineer the Data Vault


Step 3:  Satellites
satelite table has additional information with hash_key and load date and record source


# One Big Table (OBT)


All data into a single wide table 
  - thousands of columns
  - column can be single or nested  
  - highly denormalized and flexible

example of a wide table 

OrderID         OrderItems              CustomerID      Customer Name   address     OrderDate

 101           [{                             5               Joe Reis    1st. St     1/08/2024
                "sku":1,
                "price":50,
                "quantity":1,
                "name": "Thigamajig",
               },{
                "sku":2,
                "price":50,
                "quantity":1,
                "name": "Whatchmacallit",
               }]

 102           [{                             5               Matt Housley    1st. St     1/08/2024
                "sku":3,
                "price":75,
                "quantity":1,
                "name": "Whoozeewhatzit",
               },{
                "sku":1,
                "price":50,
                "quantity":1,
                "name": "Thigamajig",
               }]
 

- Can have hundreds or more columns
- combines varous data types
- no need for complex joins 
- supports fast analytical queries
- wide table contains ALL the data 

Why OBT becoming popular: 
  - Low cost of cloud storage
  - Nested data allows for flexible schemas

Columnar storage helps optimiase the storage and processing of OBTs
- wide tables are sparse (expensive in an RDBMS) 
- columnar database reads only columns selected in a query and 
   reading nulls is esentially free

Cons:
  - You might lose the business logic in your analytics
  - Your need complex data structures to store nested data

  - Can have poor update and aggregation performance


# Demo: Transforming Data with dbt  pt 1
# Demo: Transforming Data with dbt  pt 2



# Lab: Data Modeling with DBT


source lab-venv/bin/activate

dbt --version



# Initiate the `classicmodels_modeling` project with the `init` command

dbt init classicmodels_modeling




(lab-venv) voclabs:~/environment $ ls -ltR classicmodels_modeling/
-rw-r--r--. 1 ec2-user ec2-user 1280 Oct 11 12:47 dbt_project.yml
-rw-r--r--. 1 ec2-user ec2-user  571 Oct 11 12:41 README.md
drwxr-xr-x. 2 ec2-user ec2-user   22 Oct 11 12:41 analyses
drwxr-xr-x. 2 ec2-user ec2-user   22 Oct 11 12:41 macros
drwxr-xr-x. 3 ec2-user ec2-user   21 Oct 11 12:41 models
drwxr-xr-x. 2 ec2-user ec2-user   22 Oct 11 12:41 seeds
drwxr-xr-x. 2 ec2-user ec2-user   22 Oct 11 12:41 snapshots
drwxr-xr-x. 2 ec2-user ec2-user   22 Oct 11 12:41 tests
classicmodels_modeling/analyses/
classicmodels_modeling/macros/
classicmodels_modeling/models
classicmodels_modeling/models/example/
-rw-r--r--. 1 ec2-user ec2-user 475 Oct 11 12:41 my_first_dbt_model.sql
-rw-r--r--. 1 ec2-user ec2-user 115 Oct 11 12:41 my_second_dbt_model.sql
-rw-r--r--. 1 ec2-user ec2-user 447 Oct 11 12:41 schema.yml
classicmodels_modeling/seeds/
classicmodels_modeling/snapshots/
classicmodels_modeling/tests/




cp ./scripts/packages.yml ./classicmodels_modeling/
cd classicmodels_modeling


## fetch latest versions of tools / libraries specified in packages.yml
dbt deps

scripts/profile.yaml contains connection informatino 

cp ../scripts/profiles.yml ~/.dbt/profiles.yml 

# attempt to connect to profile source
dbt debug


Resources: 
https://www.amazon.com/Building-Data-Warehouse-W-Inmon/dp/0764599445

https://www.amazon.com/Data-Warehouse-Toolkit-Definitive-Dimensional/dp/1118530802

https://www.oreilly.com/library/view/building-a-scalable/9780128026489/

https://vertabelo.com/blog/data-vault-series-data-vault-2-0-modeling-basics/



## Data Modeling, Transformation and Serving
## Course 4
## Week 2
## Data Modeling and Transformation for Machine Learning

# Modeling and Processing Tabular Data for Machine Learning 
# Overview 

Machine Learning Project Lifecycle Framework 


[--- Scoping --]  [------------ Data ------]  [---Modeling/Algo Dev ----]  [------- Deployment ------]
[Define Project]  [define data/] [label and]  [select and ] [Perform err]  [Deploy in ] [Monitor &   ]     
                  [est baseline] [org data ]  [train model] [analysis   ]  [production] [maintain sys]    


# Machine Learning Overview



ML System 1 : Predict customer churn                               Learn            
                                                        ----------->>                 categorical label
[cust        ] [browsing ] [hist sales  ] [customer  ]  [Supervised]   [ML    ] -->> [ Predict churn]
[demographics] [ history ] [product data] [churn data]  [Learning  ]   [System]      [ / not churn  ]
+---------------------------------------+ +----------+
                  features                    label     (Classification)

Supervised Learning:  Labels subervised what needs to be learned from the features
Classification:  value given is a classification of customer who will or will not churn


ML System 2 : Predict sales for the next new year holiday    Learn
                                                        ----------->>                 numerical label
[cust        ] [browsing ] [hist sales  ] [historical]  [Supervised]   [ML    ] -->> [ Predict churn]
[demographics] [ history ] [product data] [sales data]  [Learning  ]   [System]      [ / not churn  ]
+---------------------------------------+ +----------+   
                  features                    label     (Regression)
                  
Regression:  value given is a number




ML System 3 : segment customers into groups based on similar purchasing behaviors
                                                            Learn
                                                        ------------->> 
[cust        ] [browsing ] [hist sales  ] [  NO   ]  [Unsupervised]   [ML    ] -->> [ Predict churn]
[demographics] [ history ] [product data] [LABELS ]  [  Learning  ]   [System]      [ / not churn  ]
+---------------------------------------+ 
                  features only                   
                  
Unsupervised Learning:   No labels, algo has to figure out grouping based on similarities





[--- Scoping --]  [------------ Data ------]  [---Modeling/Algo Dev ----]  [------- Deployment ------]
[Define Project]  [define data/] [label and]  [select and ] [Perform err]  [Deploy in ] [Monitor &   ]     
                  [est baseline] [org data ]  [train model] [analysis   ]  [production] [maintain sys]    

Scoping:
  - ML Engineer / Data Scientist defines project and decide which business problems to apply ML to 

Data: 
  - ML Eng will work with Data Eng to determine which features and labels are needed to train ML algo

Algo Development:
  - Select and train model and perform err analaysis
  - ML Eng takes data provided and uses it to train ML algo
    - split data into training and test set
    - use training set to train several ML algos with different configs

    - Classical ML algorithms:
      - Linear regression
      - Logistic regression
      - Decision trees
      - Random  foreset / boosted trees 

    - Classical ML algos expect tabluar data
      - as data increases, Classical ML algos will reach a "plateau"

    - Complex ML algorithms:
      - Deep neural networks
        - can work with tabular data
      - Convolutional neural networks
        - can work with image data
      - Recurrent neural networks
        - can work with timeseries data
      - Large language models
        - can work with text data

  - select the best model through cross-validation 
    - Evaluate the model performance using the test set

  - May go back to data engineer to:
    - fix something in collected data
    - add more features
    - collect more data
    - go back to Data step

Deployment:
  - ML Eng / Data Scientists check to make sure the system's performance is good an reliable
  - Write software to put system in to production
  - Monitor the system, track data and maintain system 
  - Data Engineer may:
    - prepare and serve the data that is needed for the deployed model
    - serve an updated set of data to re-train and update the model


# Modeling Data for Traditional ML Algorithms

Traditional ML Algos expect tabular Numerical data

ex.  customer
# of Items   Date of          Customer    Minutes on    Account     
purchased    last purchase     income      Platform     Type       Churned
   14         7/5/2024         $50,000       15         Family      No
    9         3/4/2024         $40,000       13         Platinum    Yes
 Null         8/12/2024         null         null       Null        Yes
    2         8/24/2024         null         35         Basic       No

                                               derived field -----+
customer as numerical                                             |
# of Items   Days since        Customer    Minutes on  Account  Purchase   
purchased    last purchase     income      Platform     Type    per min      Churned
   0.93         0.90           0.5          0.24         1       0.93           0
   0.57         0.29           0.4          0.20         2       0.69           1   Churn
   0.07         0.35           0.35         0.64         0       0.06           0   Not Churn
+---------------------------------------------------------------------+    +----------+   
                                    features                                  labels

- No missing valaues or duplicat rows 
- Each column consists of numerical values that are within a similar range



Feature Engineering:
  - any change or processing done to a raw column and any creation of new features
        - handling missing values
        - feature scaling 
        - converting categorical columns into numerical ones
        - creating new columns by combining or modifying existing ones

Handling missing values:
  - delete the entire column or row (if there is no risk of losing valuable data)
  - impute the missing values with summary statistics 
      - replace missing values with the column mean or median
      - replace missing values with values from a similar record

Scaling Numerical Features:
  - scale features so that the values of each feature end up within a similar range 
  - Training an ML algo is based on solving an optimization problem:
      - if values vary drastically, it could take longer for the optimizaiton algo to converge
  - Certain ML algo are based on distinct metrics
      - accuracies can be affected by different ranges of values

  Standardiztion scaling 
    value - column mean / column standard deviation 
        - reulting value has mean of 0 and variance of 1

  Min-Max scaling 
    value - column min / (column max - molumn min)
        - reulting value is between 0 and 1 


How to handle non-numerical values (Family, Basic, Platinum, etc.)

One Hot Encoding:  break out values into columns and place a one in the column that it is

Acct Type     Basic   Family  Platinum
---------     -----   ------- ----------
Family          0       1        0 
Platinum        0       0        1
Basic           1       0        0

Ordinal Encoding:  assign a value to each "level" of acct type

Acct Type     Acct Type
---------     ----------
Family           2            middle  
Platinum         3            most expensive
Basic            1            least


NOTE:  Preprocessing of data == Feature Engineering 





# Conversation with Wes McKinney (creator of Pandas)

Apache Arrow 
ibis-project.org  <<-- look at this 
polars            <<-- look at this 



# Processing Tabular Data for Classical Machine Learning Algorithms  
# Using Scikit-Learn pt 1.


utilize sci-kit learn for pre-processing 



Two processing methods:
  - Standardiztion for the numerical columns
  - One-hot encoding for the categorical columns



0. download data set
https://www.kaggle.com/datasets/muhammadshahidazeem/customer-churn-dataset


1. start jupyter
jupyter notebook 

import pandas as pd 
data = pd.read_csv("customer_churn_dataset-training-master.csv")
data.shape
data.head(5)
data.describe()
data.isnull().sum()
data[data['CustomerID'].isnull()]   ## this retrieves the row that the CustomerID is null
data = data.dropna()   ## drop null row
data.isnull().sum()    ## verify it is gone

for col in ['Subscription Type', 'Contract Length']:   ## returns each column value and % or rows for each value
    print(col)
    print(data[col].value_counts(normalize=True))
    print('\n')


features = data.iloc[:,0:-1     ## gets features columns, all columns but last one on right]
labels = data.iloc[:,-1]        ## gets labels column (Churn), the last column on right

Preparing Data for Training a Machine Learning Model 

1. Split the data into training and test sets

2. Process the training data
  a. Numerical columns   -->> standardize
  b. Categorical columns -->> one hot encoding
  c. Combine processed columns with the Customer ID into a Pandas data frame
  d. Convert Pandas data frame into a parquet file

3. Process the test data
  a. Numerical columns   -->> standardize
  b. Categorical columns -->> one hot encoding
  c. Combine processed columns with the Customer ID into a Pandas data frame
  d. Convert Pandas data frame into a parquet file


Use the same computed statistics (i.e. standardize, one hot encoding) on test set that was 
done on the training set 



# Processing Tabular Data for Classical Machine Learning Algorithms  
# Using Scikit-Learn pt 2.


Preparing Data for Training a Machine Learning Model 


use scikit learn tran_test_split 

import sklearn.model_selection import train_test_split


# x == features, y == labels for each train and test 
# 
X_train, X_test, y_train, y_test = train_test_split(features, 
                                                    labels, 
                                                    test_size = 0.2,
                                                    random_state=32)

from sklearn.preprocessing import StandardScaler

numerical_columns = ['Age', 'Tenure', 'Usage Frequency', 'Support Calls', 
                     'Payment Delay', 'Total Spend', 'Last Interaction']

X_train_numerical = X_train[numerical_columns]  ## Just pull out numerical cols from X_train 

scaler = StandardScaler()


## NOTE:  
  scaler.fit()        computes  the mean and the std dev of each column
  scaler.transform()  uses the mean and std deviation to scale the values of each column

scaler.fit(X_train_numerical)                         # first part of transforming values
X_train_scaled = scaler.transform(X_train_numerical)  # scale numerical values accordingly

## below creates a data frame with X_train_scale as data and pass in index and columns
## 
X_train_scaled_df = pd.DataFrame(data=X_train_scaled, index=X_train.index, columns=numerical_columns)

X_train_scaled_df.head(5) shows all features have been scaled to lay within a similar range 

 	        Age 	    Tenure 	    Usage Frequency 	Support Calls 	Payment Delay 	Total Spend 	Last Interaction
21498 	-0.751860 	-0.882836 	-1.375749 	      0.129657 	      1.578870 	      -1.703974 	  -1.336266
302478 	0.291831 	  -0.477252 	0.254667 	        -1.172992 	    -1.207105 	    0.661850 	    0.525345
253281 	-0.912428 	1.376846 	  -0.909916 	      -1.172992 	    0.973223 	      1.232594 	    1.339801
177014 	-0.189873 	0.913322 	  -1.259290 	      1.106644 	      0.488706 	      0.320443 	    -0.754513
308214 	0.291831 	  1.492727 	  -0.327624 	      -0.521667 	    -0.964846 	    1.315442 	    0.059943


from sklearn.preprocessing import OneHotEncoder     ## handles One Hot Encoding

categorical_columns = ['Subscription Type', 'Contract Length']  # set column names for categories / One Hot

X_train_categorical = X_train[categorical_columns]  ## now get the training set just for these columns

## NOTE:  
  encoder.fit()   - check the unique values within each categorical column 
                  - prepares the labels of the output columns

X_train_encoded = encoder.transform(X_train_categorical)  ## returns encoded columns as compressed matrix

print(type(X_train_encoded))
<class 'scipy.sparse._csr.csr_matrix'>       #csr = compresses sparse row


print(X_train_encoded)          # returns only locations where 1's are stored

<Compressed Sparse Row sparse matrix of dtype 'float64'
	with 705330 stored elements and shape (352665, 6)>
  Coords	Values
  (0, 2)	1.0
  (0, 3)	1.0
  (1, 0)	1.0
.....
  (352663, 0)	1.0
  (352663, 3)	1.0
  (352664, 1)	1.0
  (352664, 4)	1.0


X_train_encoded.todense()       ## convert to regular matrix
matrix([[0., 0., 1., 1., 0., 0.],
        [1., 0., 0., 0., 0., 1.],
        [1., 0., 0., 0., 0., 1.],
        ...,
        [1., 0., 0., 1., 0., 0.],
        [1., 0., 0., 1., 0., 0.],
        [0., 1., 0., 0., 1., 0.]])

encoder.get_feature_names_out()   ## column names of One Hot Columns
array(['Subscription Type_Basic', 'Subscription Type_Premium',
       'Subscription Type_Standard', 'Contract Length_Annual',
       'Contract Length_Monthly', 'Contract Length_Quarterly'],
      dtype=object)



## now create data frame with encoded categorical columns

X_train_encoded_df = pd.DataFrame(X_train_encoded.todense(),
                                  index=X_train.index,
                                  columns = encoder.get_feature_names_out())

 	    Subscription Type_Basic 	Subscription Type_Premium 	Subscription Type_Standard 	Contract Length_Annual 	Contract Length_Monthly 	Contract Length_Quarterly
21498 	0.0 	0.0 	1.0 	1.0 	0.0 	0.0
302478 	1.0 	0.0 	0.0 	0.0 	0.0 	1.0
253281 	1.0 	0.0 	0.0 	0.0 	0.0 	1.0
177014 	0.0 	1.0 	0.0 	0.0 	0.0 	1.0
308214 	0.0 	1.0 	0.0 	1.0 	0.0 	0.0


## Now concatenate the CustomerID, the scaled df and the encoded df, axis=1 means concat horizontally
## 

X_train_transf = pd.concat([X_train['CustomerID'], 
                            X_train_scaled_df,
                            X_train_encoded_df], axis=1)

X_train_transf.head(5)


 	CustomerID 	Age 	Tenure 	Usage Frequency 	Support Calls 	Payment Delay 	Total Spend 	Last Interaction 	Subscription Type_Basic 	Subscription Type_Premium 	Subscription Type_Standard 	Contract Length_Annual 	Contract Length_Monthly 	Contract Length_Quarterly
21498 	21507.0 	-0.751860 	-0.882836 	-1.375749 	0.129657 	1.578870 	-1.703974 	-1.336266 	0.0 	0.0 	1.0 	1.0 	0.0 	0.0
302478 	309592.0 	0.291831 	-0.477252 	0.254667 	-1.172992 	-1.207105 	0.661850 	0.525345 	1.0 	0.0 	0.0 	0.0 	0.0 	1.0
253281 	260395.0 	-0.912428 	1.376846 	-0.909916 	-1.172992 	0.973223 	1.232594 	1.339801 	1.0 	0.0 	0.0 	0.0 	0.0 	1.0
177014 	181825.0 	-0.189873 	0.913322 	-1.259290 	1.106644 	0.488706 	0.320443 	-0.754513 	0.0 	1.0 	0.0 	0.0 	0.0 	1.0
308214 	315328.0 	0.291831 	1.492727 	-0.327624 	-0.521667 	-0.964846 	1.315442 	0.059943 	0.0 	1.0 	0.0 	1.0 	0.0 	0.0


## scale the numerical columns in test set
## 
X_test_scaled = scaler.transform(X_test[numerical_columns])
X_test_scaled_df = pd.DataFrame(data=X_test_scaled, 
                                index=X_test.index, 
                                columns = numerical_columns)

## encoding the categorical columns in testing set
## 
X_test_encoded = encoder.transform(X_test[categorical_columns])
X_test_encoded_df = pd.DataFrame(data=X_test_encoded.todense(), 
                                 index=X_test.index,
                                 columns = encoder.get_feature_names_out())


## concatenate the two dataframes 
## 
X_test_trans = pd.concat([X_test['CustomerID'],
                          X_test_scaled_df, 
                          X_test_encoded_df], axis=1)

X_test_trans.head(5)

 	CustomerID 	Age 	Tenure 	Usage Frequency 	Support Calls 	Payment Delay 	Total Spend 	Last Interaction 	Subscription Type_Basic 	Subscription Type_Premium 	Subscription Type_Standard 	Contract Length_Annual 	Contract Length_Monthly 	Contract Length_Quarterly
207158 	212278.0 	0.452398 	1.260965 	0.836958 	-0.847330 	0.004189 	0.795828 	0.408995 	0.0 	0.0 	1.0 	0.0 	1.0 	0.0
383379 	390494.0 	-0.029305 	-0.766954 	1.652166 	-0.196005 	0.609835 	-0.260777 	1.805204 	1.0 	0.0 	0.0 	1.0 	0.0 	0.0
258610 	265724.0 	0.452398 	0.044213 	0.487584 	-0.847330 	-1.570493 	1.165128 	-1.219915 	1.0 	0.0 	0.0 	1.0 	0.0 	0.0
299098 	306212.0 	-0.029305 	-1.346360 	0.254667 	-0.847330 	0.125318 	0.866079 	-0.870863 	0.0 	0.0 	1.0 	1.0 	0.0 	0.0
340382 	347497.0 	-1.715267 	-0.940776 	1.535708 	-0.847330 	0.125318 	-0.300309 	-0.987214 	0.0 	1.0 	0.0 	1.0 	


X_train_transf.to_parquet("train.parquet")   ## Write out as parquet files
X_test_trans.to_parquet("test.parquet")



# Practice Lab: Feature Engineering for ML


## Modeling and Processing Unstructured Data for Machine Learning
# Modeling Image Data for ML Algorithms

Convolutional Neural Network (CNN)
  - made up of several layers
  - each layer tries to identify more image features  
    to help with the ML task
        - First layer: generic features
        - Later layer: Complex patterns and textures


- Start with pre-trained CNN algorithms
- fine tune these models for the specific task


Preparing Image Data for Training an ML Algorithm 
- resize image
- scale pixels

Data Augmentation: technique used to create new versions of existing images 
    - flipping, rotating, cropping, adjusting brightness
    - can increase the size and variety of training data


TensorFlow
  - has pre-processing functions that could be deployed to images

# Image Preprocessing Using TensorFlow

https://colab.research.google.com/drive/1NVBjtIfGuYoSomwBYkPg6hqyhj7HNhLS?usp=sharing#scrollTo=GB_biegiU5xU



# Preprocessing Textual Daa for Analysis and Text Classification

- Sentiment analysis of product reviews 
- Classification of news articles
- Chatbots and virtual assistants
- Spam detection
- Customer segmentation
- Product recommendations



Pre-processing Texts for ML 



[Textual Data might contain ] 
[contain typos, &           ]             [ Clean and   ]
[inconsistencies repititions]             [high-quality ]      ML
                                Data      [data         ]     Engineering           [Train a classical or ]
[May contain words or      ]   Engineer                       Team                  [ advanced ML model   ]
[characters not relevant   ]   ------->>>                      ------->>>
[to the NLP Task           ]
                                          [Removve and ]                            [  Other use cases    ]
[Training LLMs is expensive ]             [ irrelevant ]
[and time consuming         ]             [ words or   ]
                                          [ characters ]

Processing Text 

Cleaning: 
  - removing punctuations, extra spaces, characters that add no meaning 

Normalization:
  - convert test to a consistent format
      - transforming to lower-case
      - converting numbers or symbols to characters
      - expanding contractions / replacing accronyms, abbreviations

Tokenization:
  - splitting each review into individual tokens 
    ( works, subswords, short sentences)
  python str.split()
    ex. "This is a wonderful price for the amount you get" 
    becomes [this, is, a, wonderful, price, for, the, amount, you, get]


Removal of Stop Words:
  - removing frequently used words such as: "is", "are", "the", "for", "a"
  - define own list of stop words
  - use built-in set of NLP libraries: spaCy, NLTK, Gensim, TextBlob
    from [this, is, a, wonderful, price, for, the, amount, you, get]
    to   [this, wonderful, price, you, get]

Lemmatization: 
  - replacing each word with its base form or lemma
      i.e. getting / got  -->>  get
  - can use NLP libraries to get lemma of each word 

  ex. [i, bought, this, my, son, his, hair, thinning, i, do, not, know, yet, how,
       well, helping, he, said, smell, great]

to >> [i, buy, this, my, son, his, hair, thin, i, do, not, know, yet, how,
       well, help, he, say, smell, great ]



This file (functions_preprocess_texts.py) contains python functions that you can 
use to pre-process texts by: 

    - removing special characters and extra spaces
    - expanding contractions
    - converting characters to lower case
    - removing stop words
    - lemmatizing


# Text Vectorization and Embedding


Traditional Vectorization: assign a number to each word in a text based on its frequency of occurance
  - Bag of Words
      - each entry is number of occurances word appears
        - Only takes into account the word frequency in each document
        - Some frequently appearing words might carry little meaning
                ex.  "purchase" "buy"      -->> high frequency, little meaning
                ex.  "break" "exceptional" -->> low frequency, more significant


  - Term-Frequency Inverse-Dcoument-Frequency (TF-IDF)
      - Account for the weight and rarity of each word
          TF: the number of times the term occurred in a document divided by the total number of words in that document

         IDF: how common or rare that word is in the entire corpus
                closer to 0: more common
                closer to 1: more rare


                      Reviews

        +--> [this, wornderful, price, amount, you, get]     <<-- document
        |
corpus--+--> [great, product, big, amount]
        |
        +--> [I, buy, this, my, son, his, hair, thin, I, do, not, know, yet, how, well, help, he, say, smell, great]


the
vocabulary [this, wonderful, price, amount, you, get, great, product, big, I, buy, my, son, 
           his, hair, thin, do, not, know, yet, how, well, help, he, say, smell ]    # list of unique words in collection 

each document becomes a vector of vocabulary list:

this|wonderful|price|amount|you|get|great|product|big|I |buy|my |son|his|hair|thin|do |not|know|yet|how|well|help| he|say|smell 
 1     1        1      1     1   1   0      0      0   0  0   0    0  0   0     0   0   0    0   0   0   0    0     0  0   0
 0     0        0      1     0   0   1      1      1   0  0   0    0  0   0     0   0   0    0   0   0   0    0     0  0   0
 1     0        0      0     0   0   1      0      0   2  1   1    1  1   1     1   1   1    1   1   1   1    1     1  1   1
 
 
NOTE: scikit learn can vectorize text using bag of words method  and tf-idf

TF-IDF methodology
this|wonderful|price|amount|you|get|great|product|big|I |buy|my |son|his|hair|thin|do |not|know|yet|how|well|help| he|say|smell 
0.33  0.44      0.44  0.33 0.44 0.44 0      0      0   0  0   0    0  0   0     0   0   0    0   0   0   0    0     0  0   0
 0     0        0     0.43   0   0  0.43   0.56   0.56 0  0   0    0  0   0     0   0   0    0   0   0   0    0     0  0   0
0.16   0        0      0     0   0  0.16    0      0  043 0.22 0.22    0.22  0.22   0.22     0.22   0.22   0.22    0.22   0.22   0.22   0.22    0.22     0.22  0.22   0.22


Word Embedding: Vector that captures the semantics meaning of the word
  i.e. if two words have similar meaning they are "close" to each other

        "useful" embedding vector should be near "helpful" vs. "tree"

generate embedding vectors using algorithms"
  - word2vec --  Trained to learn the embeddings of words from their co-occurances in large collections of text
  - GLOVE    --


Sentence Embedding:  Vector that reflects the semantic meaning of the sentence
      - takes into account the position of a word in a sentence and the meaning of each word  
      - sentences with similar meanings, embedding vectors will be close to each other 
      - lower dimension than the vector generated by TF-IDF

Pre-trained NLP models
based on Large Language Models 

  Open-Source                    Closed-Source

Sentence Transformers               OpenAI
  (sbert.net)                       Anthropic
                                    Gemini


Text Reviews -->> Embeddings ------>> Features to train an ML Algo 
                                |
                                +-->> Clustering, Similarity Search  


Assignment: Modeling and Transforming Text Data for ML

Different encoding techniques for Feature Engineering

https://medium.com/anolytics/all-you-need-to-know-about-encoding-techniques-b3a0af68338b


Resources

Courses

https://www.deeplearning.ai/courses/machine-learning-specialization/

https://www.deeplearning.ai/courses/deep-learning-specialization/


Andrew explains the best practice of applying the same preprocessing steps and 
computed statistics from the training set to the testing set 
https://www.coursera.org/learn/deep-neural-network/lecture/lXv6U/normalizing-inputs

Convolutional Neural Networks
https://www.coursera.org/learn/convolutional-neural-networks?specialization=deep-learning

https://www.deeplearning.ai/courses/natural-language-processing-specialization/

https://www.deeplearning.ai/courses/machine-learning-in-production/

https://www.deeplearning.ai/courses/generative-ai-with-llms/

https://www.deeplearning.ai/short-courses/preprocessing-unstructured-data-for-llm-applications/

https://www.deeplearning.ai/short-courses/google-cloud-vertex-ai/

https://www.deeplearning.ai/short-courses/large-language-models-semantic-search/

https://www.deeplearning.ai/short-courses/serverless-llm-apps-amazon-bedrock/

https://www.deeplearning.ai/short-courses/pretraining-llms/

Reference

Pandas:
https://www.kaggle.com/learn/pandas
https://www.w3schools.com/python/pandas/default.asp


Sci-kit:
https://scikit-learn.org/stable/modules/preprocessing.html
https://scikit-learn.org/stable/modules/impute.html
https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction

https://www.kaggle.com/datasets/muhammadshahidazeem/customer-churn-dataset

https://medium.com/@lars.chr.wiik/best-embedding-model-openai-cohere-google-e5-bge-931bfa1962dc

https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2

https://www.sbert.net/index.html

https://seattledataguy.substack.com/p/data-engineering-vs-machine-learning

https://docs.cohere.com/docs/embeddings

https://docs.cohere.com/docs/how-to-convert-text-into-vectors

https://cohere.com/blog/sentence-word-embeddings

https://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/

https://p.migdal.pl/blog/2017/01/king-man-woman-queen-why

https://medium.com/deeper-learning/glossary-of-deep-learning-word-embedding-f90c3cec34ca

https://medium.datadriveninvestor.com/from-word-embeddings-to-sentence-embeddings-part-1-3-7ba9a715e917



## Data Modeling, Transformation and Serving
## Course 4
## Week 3
## Data Transformations & Technical Considerations 

# Batch Transformations


Transformation:  Manipulate and enhance data for downstream stakeholders


Transformations 

      Technical Considerations        |       Transformation Approaches
--------------------------------------------------------------------------------------
  Batch Transformations               |         - Single machine or use a distributed 
    - Size of the Data                |             processing tool (i.e. Spark)
    - Specification of the hardware   |         - Write transformation logic in SQL or Python 
    - Performance requirement         |
                                      |
  Streaming Transformation            |
    - Latency requirement             |
                                      |


# Batch Transformation Patterns and Use Cases



Transformations for Data Modeling

Target Model        Star            Data Vault


[data source]   -->>   [ Transformation ]  -->>   [Final Data Form]



ETL :  [data  ]  -->> [Extract ] -->> [Transform / ]  -->> [ Load ] -->> [   Target System  ]
       [source]                       [Staging Area]                     [Expected Data Form]



ELT :  [data  ]  -->> [Extract ] -->> [ Load  to     ]  -->> [ Transform in  ] -->> [ Expected Data Form ]
       [source]                       [Target System ]       [ Target System ] 


EtLT : [data  ]  -->> [Extract ] -->> [transform   ] -->> [ Load  to     ]  -->> [ Transform in  ] -->> [ Expected Data Form ]
       [source]                       [clean/dedupe]      [Target System ]       [ Target System ] 


examples:                             +----- based on Spark
                                      |
                                      v
ETL:            [AWS RDS] -->>  [AWS Glue ETL] -->>  [S3]


ELT:            [Target System                    ]
                [                                 ]  Note:  
                [                                 ]    - dbt is not an execution tool like Spark.
                [ source data          expected   ]
                [   |                  data form  ]    - SQL-tool that facilitates transformation 
                    |                      ^             within the database or data warehouse
                    v                      |             (utilizes resources of data platform itself)
                [------------- dbt  --------------]
                [                                 ]



Transformations for Data Cleaning

Data Wrangling:  take messy, malformed data and turn into clean data


[ Data  ]   ----------------->>  [ETL / ELT]  ----------------->>  [ Target ]  
[Source ]       source data                                        [ System ]
             
             - missing values                 clean and normalize
             - dupe entries                     - write your own code
             - outliers                         - Use wrangling tool
             - inconsistencies                      - AWS Glue DataBrew





Transformations for Data Updating


Truncate and Reload :  Delete all records in target system and reload the data from the source
                        - have small data set
                        - updated data once in a while


Change Data Capture :  Identify and capture changes in source system and apply to target
                        - check last updated columnt
                        - check db transaction log
                            I / U / D
        Capture Updates: 
            - Insert Only
                - insert new records without change / deleting old records 
                - add additional information  to new record to distinguish it from old record
            - Upsert Merge
                - use key of new data to search for existing record 
                - if key exists, update record with new
                - if key doesn't exists, insert new record

        Capture deletes: 
             - Hard delete: permanently delete record 
             - Soft delete: mark record as "deleted"
             - Insert only:  insert record with "deleted" flag and do not modify prior record



Single Row Inserts:  OK for row-oreiented database


Single Row Inserts:  BAD for OLAP Column-oriented databases:
                      - puts massive load on OLAP system
                      - Extremely inefficient for subsequent reads

                    GOOD for OLAP Column-oriented databases:
                        - micro-batch or batch fashion
                        - when inserted in bulk 
                            - organized data more efficiently into row groups
                                  and better compressed
                            - Leverage distributed parallel processing to speed up data loading





# Distributed Processing Framework: Hadoop MapReduce


2000s Big Data:   Commodity hardware became cheap and ubiquitous
                  Several innovations in large-scale distributed storage and computing:

2003  GFS:        Google File System  (GFS)

2004  MapReduce:  Parallel programming paradigm for processing data distributed over GFS

2006  Hadoop:     Yahoo Hadoop
                    - Hadoop Distributed File System 
                        - Key ingredient of current big data engines (EMR, Spark)

                    - Hadoop MapReduce
                        - Influences many of today's distributed systems

Hadoop Distributed File System

HDFS:   combines compute and storage on the same nodes

Object Storage: limited compute support for internal processing


HDFS: 
Large File -->> [1] [2] [3]  large file broken into blocks of few hundred megabytes in size

                                    [Data Node]           [Data Node ]
+------------------------+       +  [ [1] [2] ]           [[1][3][2] ]        Each block of data replicated to 3 data nodes
| NameNode               |       |                          +
| Maintains:             |       |                          |                                 
|    - Directories       |-------+--------------------------+                 Replication increases durability and 
|    - File metadata     |       |                          |                  availability of the data
|    - Detailed catalog  |       |                          +
|                        |       + [Data Node ]           [Data Node ]        If data node fails, Name Node will redistribute data
+------------------------+         [  [2][3]  ]           [ [1][3]   ]         to maintain 3x replication  


                                                                              Combining compute and storage allows in-place
                                                                              data processing (via MapReduce)




Hadoop MapReduce

last here @ 2:49


MAP LAST HERE 

https://www.coursera.org/learn/data-modeling-transformation-serving/lecture/OeBHP/batch-transformation-patterns-and-use-cases













.