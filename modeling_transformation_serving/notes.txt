## Data Modeling, Transformation and Serving
## Course 4
## Week 1

# Introduction to Data Modeling for Analytics

Overview

Data Modeling: A data model organizes and standardizes data in a precise structured representation 
              to enable and guide human and machine behavior, inform decision-making and facilitate actions 




Define the structure, relastionshiops and meaning of data.

Structure the data in a way that connects back to the organization
    - Data is understandable and valuable


Good Data Models 
    - Reflect the business goals and logic while incorporating business rules
    - Ensure compliance with operational standards and legal requirements
    - Outline the relationships between business processes
    - Serve as a powerful communication too, creating a "shared language"

ex. what is "an active users"



Poor Data Models 
    - Don't reflect how the busienss operates
    - Create more problems than they solve
    - Provide stakeholders with innaccurate information 
        and create confusion 


Targeted Data Modeling Approach
- Focus on specific business domains 

    Marketing:  Better understand customer behavior and campaign effectiveness
    Finance  :  Analyze spending patterns and identify cost-saving opportunities
    Machine Learning  :  Drive better decision-making and impactful AI models

# Conceptual, Logical and Physical Data Modeling


Conceptual:
    - describes business entities, relationships and attributes
    - business logic and rules
        - Entity - Relationshiop Diagram (ER)  (1 to 1, 1 to Many)

Logical 
    - details about the implementation of the conceptual model
        - column types, primary / foreign keys

Physical 
    - Details about the implementation of the logical model in a specific DBMS
    - Configuration details 
        - Data storage approach
        - Partitioning details
        - Replication details


# Normalization 


Normalization:  A data modeling practice typically applied to relational databases
                to remove the redundancy of data within a database and ensure referential 
                integrity between tables.

Edgar Codd's Objectives of Normalization 

    - To free the collections of relations from undesireable insertion, 
       update and deletion dependencies

    - To reduce the need for restructuring the collection of relations as 
       new types of data are intruduced



    Reduce Data Redundancy: Eliminating duplicate data saves storage space and ensures consistency across the database.
    Improve Data Integrity: Ensuring that each piece of data is stored in only one place reduces the likelihood of data anomalies and maintains the accuracy of the data.
    Enhance Update/Delete Query Performance




SalesOrders 
Denormalized Form 
 - contains redundant data 
 - contains nested data

OrderID         OrderItems              CustomerID      Customer Name   address     OrderDate

 101           [{                             5               Joe Reis    1st. St     1/08/2024
                "sku":1,
                "price":50,
                "quantity":1,
                "name": "Thigamajig",
               },{
                "sku":2,
                "price":50,
                "quantity":1,
                "name": "Whatchmacallit",
               }]

 102           [{                             5               Matt Housley    1st. St     1/08/2024
                "sku":3,
                "price":75,
                "quantity":1,
                "name": "Whoozeewhatzit",
               },{
                "sku":1,
                "price":50,
                "quantity":1,
                "name": "Thigamajig",
               }]
 

First Normal Form (1NF): 
    - Each column must be 
        - unique
        - have a single value
    - Unique Primary Key

  composite key
+---------------+
OrderID   Item Number  sku    price   quantity    name          CustomerID  CustomerName        address     OrderDate
100         1           1       50      1       Thingamajig         5           Joe Reis        1st St.     1/8/2024
100         2           2       25      2       Watchamcallit       5           Joe Reis        1st St.     1/8/2024
101         1           3       75      1       Whoozeewhatzit      7           Matt Housely    2nd Ave.    1/8/2024
101         2           2       25      3       Watchamcallit       7           Matt Housely    2nd Ave.    1/8/2024
---                                                                    +----------------------------------------------+
 |                                                                                           | 
 +------------------------------------------------------------------------------------------+
                                                                         Non-key columns that depend on OrderID 
                                                                    i.e. if you know an OrderID you would know the customer                        
Second Normal Form (2NF):
    - Requirements of 1NF must be met  
    - Partial dependencies should be removed

NOTE: 
Partial Dependency: A subset of non-key columns that depend on some columns in the composite key

Split to two tables: OrderItems and Orders

OrderItems                                                       Orders                                                       
+---------------------------------------------------------+     +----------------------------------------------------+
OrderID   Item Number  sku    price   quantity    name          OrderID CustomerID  CustomerName        address     OrderDate
100         1           1       50      1       Thingamajig       100       5           Joe Reis        1st St.     1/8/2024 
100         2           2       25      2       Watchamcallit     101       7           Matt Housely    2nd Ave.    1/8/2024  
101         1           3       75      1       Whoozeewhatzit      
101         2           2       25      3       Watchamcallit       


These two tables now have a Transitive Dependency 
Transitive Dependency:
  - A non-key column depends on another non-key column 

    OrderItems:
        price and name depend on sku

    Orders:
        CustomerName and address depend on CustomerID 


Third Normal Form (3NF):
    - Requirements of 2NF must be met  
    - Transitive dependencies should be removed


OrderItems                                  Orders                           Customers
+------------------------------------+     +----------------------------+   +------------------------------------+
OrderID   Item Number  sku    quantity      OrderID CustomerID  OrderDate    CustomerID  CustomerName     address
100         1           1       50            100       5        1/8/2024     5           Joe Reis        1st St.  
100         2           2       25            101       7        1/8/2024     7           Matt Housely    2nd Ave. 
101         1           3       75     
101         2           2       25    

                                                                                Items                                  
                                                                               +----------------------------+  
                                                                                sku   price    name
                                                                                1       50      Thingamajig 
                                                                                2       25      Watchamcallit   
                                                                                3       75      Whoozeewhatzit      




# Dimensin Modeling: Star Schema
                                                                         
                                                                [Dim]  [Dim]  [Dim]
    Fact Table     :  business measures                            \     |     /
    Dimension Table:  Contextual Informtion                         \    |    /
                                                                    \   |   / 
                                                            [Dim]----- [Fact] -----[Dim] 
                                                                         |
                                                                         |
                                                                       [Dim]

Fact Table: contains quantitative business measurements that result from a business event or process
       - Each row contins the facts of a particular business event 
       - Data in Fact table is immutable (append-only)
       - Typically narrow and long
            - not alot of columns but alot of rows


Business Event          Facts                          Grain (detail of Fact)                       Dimensions
order a ride share      Trip duration, trip price      - all rides by all customers in one day,     - customers
                        tip paid, trip delays, etc.    - all rides by one customer on one day       - drivers
                                                            - one ride by one customer              - trip locations

Atomic Grain: most detailed level of a business proces


Dimension Table: Provide the reference data, attributes and relational context for the events in the fact table
        - Describe the events' what, who, where and when  
        - Typically wide and short
            - many columns fewer rows 



Conformed Dimension:  Dimension table used with multiple fact tables

                [Dim]  [Dim]  [Dim]                [Dim]  [Dim]  [Dim]
                  \     |     /                      \     |     /
                   \    |    /                        \    |    /
                    \   |   /                          \   |   /
           [Dim]----- [Fact] ----------[Dim]------------ [Fact] -----[Dim] 
                        |                                   |
                        |                                   |
                      [Dim]                               [Dim]

Each DIM   has a PKey
Fact table has a PKey
Fact connected to Dim via FKey

Best practice:  create a surrogate key 
                    - used to combine data from differnt source systems with 
                      natural primary keys that are in different formats

                    - used to decouple the primary key of the star schema from source systems


Analytical Queries:
    - Apply aggregate queries to find the sum, average, maximum of fact
    - Use dim tables to filter or group facts


         [ dim_customers ]                   [ fact_orders ]               [ dim_products ]
         customerNumber (pk)+--+            OrderLineNumber (pk)    +---- productCode (pk)
         customerName          |            OrderNumber     (pk)    |     productName
         contactLastName       +-----------+customerNumber  (fk)    |     productLine
         contactFirstName                   postalCode      (fk)-+  |     productScale
         phone                              productCode     (fk)-|- +     productVendor
         addressLine1                       orderDate            |        productDescription
         addressLine2                       quantityOrdered      |        productLineDescription
                                            priceEac             |
                                            buyPrice             |  
                                            orderAmount          | 
                                            MSRP                 | 
                                                                 | 
                                            [dim_location]       | 
                                            postalCode (PK) +----+
                                            city 
                                            state 
                                            country 


ex. Find the total sales amount for each product line within the usa
SELECT  
  dim_product.productLine, 
  SUM(fact_orders.orderAmount) AS total_sales
FROM fact_orders 
JOIN dim_product ON 
  fact_orders.productCode =       
  dim_products.productCode       
JOIN dim_locations ON 
  fact_orders.postalCode =       
  dim_locations.postalCode
WHERE dim_locations.country = 'USA' 
GROUP by dim_products.productLine         


Star Schema vs 3NF

 - Star Schema organizes data so it's easier for business users to understand, navigate and use
 - Star Schema results in simpler queries with fewer joins



## Data Modeling Techniques 

# Inmon vs kimball Data Modeling Approaches for Data Warehouses


Inmon Data Modeling Approach

  datawarehouse: A subject-oriented, integrated, nonvolatile and time-variant collection of data 
                 in support of management's decisions.

The data warehouse contains granular corporate data.  Data in the data warehouse is able to be used
for many different purposes, including sitting and waiting for future requirements which are unknown today.


             +-------------------- Data Warehouse ----------------+    +----> [Sales: Data Mart]
 [Source1]-+   Major Subject Areas              Subject Details        |       (star schema)            +---------------+
 [Source2]-+>  [Products]  [Orders]           - business keys          |                                |   Reports     | 
 [Source3]-+   [Customers] [Shipments]        - relationships       ---+----> [Marketing Data Mart]     |     &         |
                                              - attributes             |                                |   Analysis    |
                      (Highly normalized (3NF))                        |                                |               |
                                                                       |                                |               |
                                                                       +----> [Purchasing Data Mart]    +---------------+


Kimball Data Modeling Approach:
  Kimball's approach effectively allows you to server data that's structured as star schemas 
  (or similar variants) directly from the data warehouse
      - faster modeling and iteration 
      - more data redundancy and duplications

                [Dim]  [Dim]  [Dim]                [Dim]  [Dim]  [Dim]                [Dim]  [Dim]  [Dim]      +--------------+
                  \     |     /                      \     |     /                      \     |     /          |              |
[Source1]+         \    |    /                        \    |    /                        \    |    /      -->> |  Reports     |
         |          \   |   /                          \   |   /                          \   |   /            |     &        |
[Source1]+> [Dim]----- [Fact] ----------[Dim]------------ [Fact] ----------[Dim]----------- [Fact] -----[Dim]  |  Analyssis   |
         |              |                                   |                                 |                |              |
[Source1]+              |                                   |                                 |           -->> |              |
                      [Dim]                               [Dim]                             [Dim]              +--------------+


Kimball Data Modeling Approach:
  - Quick insights are your highest priority
  - Rapid implementation and iteration

Inmon Data Modeling Approach
  -  Data quality is your highest priority
  -  The analysis requiremetns are not defined


# From Normalized Model to Star Shema






Normalized Data

OrderItems                     Items                                  
+-------------------+         +---------+  
order_id         (pk)           sku    (pk)
item_line_number (pk)           price    
itme_sku         (fk)           name
item_quantity                   brand

Orders                         Stores                                  
+-------------------+         +----------------+   
order_id         (pk)          store_id     (pk)
customer_id      (fk)          store_name
store_id         (fk)          store_city
order_date                     store_zipcode

Customers
+-------------------+
customer_id      (pk)
customer_name     
customer_zipcode     

                                                                               
                                                                               
Understand the needs of the business
1. select the business process   -->> 2. Declare the grain   -->> 3. Identify the dims  -->> 4. Identify the facts


User Needs:
- analyze sales data 
  - which products are sellin in which stores on a given day
  - differences in the sales between the stores
  - which product brands are most popular


Business process:  Company's sales transactions

Atomic Grain:  Individual product item in a sales transaction


Dims:   analysts wants stores, dates, brands 
    [ dim_stores ]    [ dim_items ]   [ dim_date ]

[dim_stores]
store_key (PK)           ## NOTE MD5 is a has function to create a surrogate key
store_id                 SELECT MD5(store_id) as store_key,    
store_name                      store_id, store_name,   store_city,  store_zipcode
store_city               FROM stores;
store_zipcode 



[dim_items]               SELECT MD5(sku) as item_key, sku, price,   grand
item_key(pk)              FROM dim_items;        
sku                       
price 
brand

Date Table 
Date            Year    Quarter     Month   Day-of-wek 
2022-03-01      2022       1          3       Tuesday
2022-03-02      2022       1          3       Wednesday

[dim_date]          ## NOTE: using pgSQL generate_series()  
date_key (pk)       select date_key, 
day_of_week           EXTRACT(DAY     FROM date_key) as day_of_week,
month                 EXTRACT(MONTH   FROM date_key) as month,
quarter               EXTRACT(Quarter FROM date_key) as quarter,
Year                  EXTRACT(year    FROM date_key) as YEAR
                      FROM generate_series ('2020-01-01'::date, '2025-01-01'::date, '1 day'::interval)  as date_key


[fact_order_items]          
fact_order_key (pk)
order_id                SELECT MD5(CONCAT(OrderItems.order_id, OrderItems.item_line_number)) as fact_order_key,
item_line_number              OrderItems.order_id,
store_key                     OrderItems.item_line_number,
item_key                      MD5(OrderItems.store_id) as store_key,
date_key                      MD5(OrderItems.item_sku) as item_key,
item_quantity                 Orders.order_date as date_key, 
item_price                    OrderItems.item_quantity,
                              Items.price as item_price
                        FROM OrderItems
                              JOIN Orders on Orders.order_id = OrderItems.order_id
                              JOIN Items  on Items.sku       = OrderItems.item_sku


Model data with dbt
  - connects to data warehouse
  - transforms and validates your data with the data warehouse
  - generates the sql code behind the scense to transform your data
  - CAN'T join together data from different sources or move transformed data to another target system

AWS Glue 
  - can connect to differnt sources, apply transformations and store processed data somewhere else




# Conversation about dbt with Drew Banin


dbt: applies business logic rules to data to become information (?)

    - setup rules with SQL and python
    - instruct data warehouse (db) to transform (t) in place 


pre-dbt:  sql scripts,  wildwest etc.



post-dbt:  version control code
           community of practice:  analytics engineer, between engineers and business

dbt: "ruby on rails for data"
  there is a "dbt way"

Things to standardize around dbt: 
 - use a sql style code 
 - column names
 - how logic is transformed to: 
    - staging tables 
    - intermediate transformations
    - marts / dimensional models
 - software engineering best practices 
    - modular code
    - test as you go / unit testing
    - code review 
    - CI/CD etc.


# Data Vault


3 layers to Data Vault

                      [Enterprise]                  [ Information ]
[ Staging ]           [Data      ]                  [ Delivery    ]
                      [Warehouse ]

                     Data Vault Model                 
insert only         - hubs, links, satellites         Data delivered to Data Marts

                   separate business objects and 
                   their relationshipsfrom their 
                   descriptive attributes


- Only change the structure in which the data is stored: 
    - Allows you to trace the data back to its source
    - Helps you avoid restructuring the data when business requirements change 


Data Vault Model: Three types of tables 

Hub : Stores a unique list of business keys to represent a core business concept: 
        Customers, products, employees, vendors


Link: Connects two or more hubs. Represents relationship, transaction, event between 
      two or more business concepts 


Satellite: Contains attributes that provide context for hubs and links  
        eg. User will query a hub which will link to a satellite table containing 
        the query's relevant attributes.


                  Data Vault Model 

+-----------------------------------------------------------------------+
  [ Hub ]-------[ Link ]-------[ Hub ]-------[ Link ]-------[ Hub ]
     |             |              |              |             | 
  [Satellite]   [Satellite]    [Satellite]   [Satellite]    [Satellite]


+-----------------------------------------------------------------------+


Data Vault Steps 

OrderItems                     Items                                  
+-------------------+         +---------+  
order_id         (pk)           sku    (pk)
item_line_number (pk)           price    
itme_sku         (fk)           name
item_quantity                   brand

Orders                         Stores                                  
+-------------------+         +----------------+   
order_id         (pk)          store_id     (pk)
customer_id      (fk)          store_name
store_id         (fk)          store_city
order_date                     store_zipcode

Customers
+-------------------+
customer_id      (pk)
customer_name     
customer_zipcode     

                                                                               
Step 1:  Model the Hubs, which contain business keys 

        Identify business keys: 
          - what is the identifiable business element ? 
          - How do users commonly look for data ?
          - business key: 
              - colmn(s) used by the business to identify and locate the data
              - not be a key generated in or tied to particular source system 

      
    [  Customer   ]             [  Order   ]
     customer_hash_key (PK)     order_hash_key (PK)
     customer_id                order_id
     load_date                  load_date
     resource_source            resource_source 

    [  item   ]                 [  Store   ]
     item_hash_key (PK)         store_hash_key (PK)
     sku                        store_id
     load_date                  load_date
     resource_source            resource_source 


  Hubs:  should contain 
    - The business key 
    - The hash key
        - Calculated as a hash of the business key 
        - Used as the Hub primary key
    - The load date    : date on which the business key was first loaded
    - The record source: the source of the businexx key 


Step 2:  Model the links, link table to connect 2 or more hubs



                    (link table)                                   (link table)
[  item   ]---------[Item_Order ]------------[  Order   ]---------[Order_Customer] -------------[  Customer   ]             
item_hash_key (PK)  item_order_hash_key(pk)  order_hash_key (PK)   customer_order_hash_key(pk)  customer_hash_key (PK)     
sku                 item_hash_key            order_id              order_hash_key               customer_id                
load_date           order_hash_key           load_date             customer_hash_key            load_date                  
resource_source     sku                      resource_source       order_id                     resource_source            
  |                 order_id                      |                customer_id
  |                 load_date                (link table)          load_date
  |                 record_source            [Order_Store]         record_source
  |                       |                  order_store_hash_key
 (satellite)        (satellite)              order_hash_key
[item]              [item_order]             store_hash_key 
item_hash_key(pk)                            order_id
load_date(pk)                                store_id
price                                        load_date
name                                         record_source
brand                                             |                     (satelite)
record_source                                [  Store   ]-------------[  Store   ]
                                             store_hash_key (PK)      store_hash_key (PK)
                                             store_id                 load_date(PK)
                                             load_date                store_name
                                             resource_source          store_zipcode
                                                                      record_source

Each table must contain must containt the primary and business keys from its parent hubs,
the load date of a row, and the source for the record.

For each table, the primary key consists of a hash calculated based on the business keys of the parent hubs.

With link tables, you can easily add new relationships or update a current relationship
without having to re engineer the Data Vault


Step 3:  Satellites
satelite table has additional information with hash_key and load date and record source


# One Big Table (OBT)


All data into a single wide table 
  - thousands of columns
  - column can be single or nested  
  - highly denormalized and flexible

example of a wide table 

OrderID         OrderItems              CustomerID      Customer Name   address     OrderDate

 101           [{                             5               Joe Reis    1st. St     1/08/2024
                "sku":1,
                "price":50,
                "quantity":1,
                "name": "Thigamajig",
               },{
                "sku":2,
                "price":50,
                "quantity":1,
                "name": "Whatchmacallit",
               }]

 102           [{                             5               Matt Housley    1st. St     1/08/2024
                "sku":3,
                "price":75,
                "quantity":1,
                "name": "Whoozeewhatzit",
               },{
                "sku":1,
                "price":50,
                "quantity":1,
                "name": "Thigamajig",
               }]
 

- Can have hundreds or more columns
- combines varous data types
- no need for complex joins 
- supports fast analytical queries
- wide table contains ALL the data 

Why OBT becoming popular: 
  - Low cost of cloud storage
  - Nested data allows for flexible schemas

Columnar storage helps optimiase the storage and processing of OBTs
- wide tables are sparse (expensive in an RDBMS) 
- columnar database reads only columns selected in a query and 
   reading nulls is esentially free

Cons:
  - You might lose the business logic in your analytics
  - Your need complex data structures to store nested data

  - Can have poor update and aggregation performance


# Demo: Transforming Data with dbt  pt 1
# Demo: Transforming Data with dbt  pt 2



# Lab: Data Modeling with DBT


source lab-venv/bin/activate

dbt --version



# Initiate the `classicmodels_modeling` project with the `init` command

dbt init classicmodels_modeling




(lab-venv) voclabs:~/environment $ ls -ltR classicmodels_modeling/
-rw-r--r--. 1 ec2-user ec2-user 1280 Oct 11 12:47 dbt_project.yml
-rw-r--r--. 1 ec2-user ec2-user  571 Oct 11 12:41 README.md
drwxr-xr-x. 2 ec2-user ec2-user   22 Oct 11 12:41 analyses
drwxr-xr-x. 2 ec2-user ec2-user   22 Oct 11 12:41 macros
drwxr-xr-x. 3 ec2-user ec2-user   21 Oct 11 12:41 models
drwxr-xr-x. 2 ec2-user ec2-user   22 Oct 11 12:41 seeds
drwxr-xr-x. 2 ec2-user ec2-user   22 Oct 11 12:41 snapshots
drwxr-xr-x. 2 ec2-user ec2-user   22 Oct 11 12:41 tests
classicmodels_modeling/analyses/
classicmodels_modeling/macros/
classicmodels_modeling/models
classicmodels_modeling/models/example/
-rw-r--r--. 1 ec2-user ec2-user 475 Oct 11 12:41 my_first_dbt_model.sql
-rw-r--r--. 1 ec2-user ec2-user 115 Oct 11 12:41 my_second_dbt_model.sql
-rw-r--r--. 1 ec2-user ec2-user 447 Oct 11 12:41 schema.yml
classicmodels_modeling/seeds/
classicmodels_modeling/snapshots/
classicmodels_modeling/tests/




cp ./scripts/packages.yml ./classicmodels_modeling/
cd classicmodels_modeling


## fetch latest versions of tools / libraries specified in packages.yml
dbt deps

scripts/profile.yaml contains connection informatino 

cp ../scripts/profiles.yml ~/.dbt/profiles.yml 

# attempt to connect to profile source
dbt debug


Resources: 
https://www.amazon.com/Building-Data-Warehouse-W-Inmon/dp/0764599445

https://www.amazon.com/Data-Warehouse-Toolkit-Definitive-Dimensional/dp/1118530802

https://www.oreilly.com/library/view/building-a-scalable/9780128026489/

https://vertabelo.com/blog/data-vault-series-data-vault-2-0-modeling-basics/



## Data Modeling, Transformation and Serving
## Course 4
## Week 2
## Data Modeling and Transformation for Machine Learning

# Modeling and Processing Tabular Data for Machine Learning 
# Overview 

Machine Learning Project Lifecycle Framework 


[--- Scoping --]  [------------ Data ------]  [---Modeling/Algo Dev ----]  [------- Deployment ------]
[Define Project]  [define data/] [label and]  [select and ] [Perform err]  [Deploy in ] [Monitor &   ]     
                  [est baseline] [org data ]  [train model] [analysis   ]  [production] [maintain sys]    


# Machine Learning Overview



ML System 1 : Predict customer churn                               Learn            
                                                        ----------->>                 categorical label
[cust        ] [browsing ] [hist sales  ] [customer  ]  [Supervised]   [ML    ] -->> [ Predict churn]
[demographics] [ history ] [product data] [churn data]  [Learning  ]   [System]      [ / not churn  ]
+---------------------------------------+ +----------+
                  features                    label     (Classification)

Supervised Learning:  Labels subervised what needs to be learned from the features
Classification:  value given is a classification of customer who will or will not churn


ML System 2 : Predict sales for the next new year holiday    Learn
                                                        ----------->>                 numerical label
[cust        ] [browsing ] [hist sales  ] [historical]  [Supervised]   [ML    ] -->> [ Predict churn]
[demographics] [ history ] [product data] [sales data]  [Learning  ]   [System]      [ / not churn  ]
+---------------------------------------+ +----------+   
                  features                    label     (Regression)
                  
Regression:  value given is a number




ML System 3 : segment customers into groups based on similar purchasing behaviors
                                                            Learn
                                                        ------------->> 
[cust        ] [browsing ] [hist sales  ] [  NO   ]  [Unsupervised]   [ML    ] -->> [ Predict churn]
[demographics] [ history ] [product data] [LABELS ]  [  Learning  ]   [System]      [ / not churn  ]
+---------------------------------------+ 
                  features only                   
                  
Unsupervised Learning:   No labels, algo has to figure out grouping based on similarities





[--- Scoping --]  [------------ Data ------]  [---Modeling/Algo Dev ----]  [------- Deployment ------]
[Define Project]  [define data/] [label and]  [select and ] [Perform err]  [Deploy in ] [Monitor &   ]     
                  [est baseline] [org data ]  [train model] [analysis   ]  [production] [maintain sys]    

Scoping:
  - ML Engineer / Data Scientist defines project and decide which business problems to apply ML to 

Data: 
  - ML Eng will work with Data Eng to determine which features and labels are needed to train ML algo

Algo Development:
  - Select and train model and perform err analaysis
  - ML Eng takes data provided and uses it to train ML algo
    - split data into training and test set
    - use training set to train several ML algos with different configs

    - Classical ML algorithms:
      - Linear regression
      - Logistic regression
      - Decision trees
      - Random  foreset / boosted trees 

    - Classical ML algos expect tabluar data
      - as data increases, Classical ML algos will reach a "plateau"

    - Complex ML algorithms:
      - Deep neural networks
        - can work with tabular data
      - Convolutional neural networks
        - can work with image data
      - Recurrent neural networks
        - can work with timeseries data
      - Large language models
        - can work with text data

  - select the best model through cross-validation 
    - Evaluate the model performance using the test set

  - May go back to data engineer to:
    - fix something in collected data
    - add more features
    - collect more data
    - go back to Data step

Deployment:
  - ML Eng / Data Scientists check to make sure the system's performance is good an reliable
  - Write software to put system in to production
  - Monitor the system, track data and maintain system 
  - Data Engineer may:
    - prepare and serve the data that is needed for the deployed model
    - serve an updated set of data to re-train and update the model


# Modeling Data for Traditional ML Algorithms

Traditional ML Algos expect tabular Numerical data

ex.  customer
# of Items   Date of          Customer    Minutes on    Account     
purchased    last purchase     income      Platform     Type       Churned
   14         7/5/2024         $50,000       15         Family      No
    9         3/4/2024         $40,000       13         Platinum    Yes
 Null         8/12/2024         null         null       Null        Yes
    2         8/24/2024         null         35         Basic       No

                                               derived field -----+
customer as numerical                                             |
# of Items   Days since        Customer    Minutes on  Account  Purchase   
purchased    last purchase     income      Platform     Type    per min      Churned
   0.93         0.90           0.5          0.24         1       0.93           0
   0.57         0.29           0.4          0.20         2       0.69           1   Churn
   0.07         0.35           0.35         0.64         0       0.06           0   Not Churn
+---------------------------------------------------------------------+    +----------+   
                                    features                                  labels

- No missing valaues or duplicat rows 
- Each column consists of numerical values that are within a similar range



Feature Engineering:
  - any change or processing done to a raw column and any creation of new features
        - handling missing values
        - feature scaling 
        - converting categorical columns into numerical ones
        - creating new columns by combining or modifying existing ones

Handling missing values:
  - delete the entire column or row (if there is no risk of losing valuable data)
  - impute the missing values with summary statistics 
      - replace missing values with the column mean or median
      - replace missing values with values from a similar record

Scaling Numerical Features:
  - scale features so that the values of each feature end up within a similar range 
  - Training an ML algo is based on solving an optimization problem:
      - if values vary drastically, it could take longer for the optimizaiton algo to converge
  - Certain ML algo are based on distinct metrics
      - accuracies can be affected by different ranges of values

  Standardiztion scaling 
    value - column mean / column standard deviation 
        - reulting value has mean of 0 and variance of 1

  Min-Max scaling 
    value - column min / (column max - molumn min)
        - reulting value is between 0 and 1 


How to handle non-numerical values (Family, Basic, Platinum, etc.)

One Hot Encoding:  break out values into columns and place a one in the column that it is

Acct Type     Basic   Family  Platinum
---------     -----   ------- ----------
Family          0       1        0 
Platinum        0       0        1
Basic           1       0        0

Ordinal Encoding:  assign a value to each "level" of acct type

Acct Type     Acct Type
---------     ----------
Family           2            middle  
Platinum         3            most expensive
Basic            1            least


NOTE:  Preprocessing of data == Feature Engineering 





# Conversation with Wes McKinney (creator of Pandas)

Apache Arrow 
ibis-project.org  <<-- look at this 
polars            <<-- look at this 



# Processing Tabular Data for Classical Machine Learning Algorithms  
# Using Scikit-Learn pt 1.


utilize sci-kit learn for pre-processing 



Two processing methods:
  - Standardiztion for the numerical columns
  - One-hot encoding for the categorical columns



0. download data set
https://www.kaggle.com/datasets/muhammadshahidazeem/customer-churn-dataset


1. start jupyter
jupyter notebook 

import pandas as pd 
data = pd.read_csv("customer_churn_dataset-training-master.csv")
data.shape
data.head(5)
data.describe()
data.isnull().sum()
data[data['CustomerID'].isnull()]   ## this retrieves the row that the CustomerID is null
data = data.dropna()   ## drop null row
data.isnull().sum()    ## verify it is gone

for col in ['Subscription Type', 'Contract Length']:   ## returns each column value and % or rows for each value
    print(col)
    print(data[col].value_counts(normalize=True))
    print('\n')


features = data.iloc[:,0:-1     ## gets features columns, all columns but last one on right]
labels = data.iloc[:,-1]        ## gets labels column (Churn), the last column on right

Preparing Data for Training a Machine Learning Model 

1. Split the data into training and test sets

2. Process the training data
  a. Numerical columns   -->> standardize
  b. Categorical columns -->> one hot encoding
  c. Combine processed columns with the Customer ID into a Pandas data frame
  d. Convert Pandas data frame into a parquet file

3. Process the test data
  a. Numerical columns   -->> standardize
  b. Categorical columns -->> one hot encoding
  c. Combine processed columns with the Customer ID into a Pandas data frame
  d. Convert Pandas data frame into a parquet file


Use the same computed statistics (i.e. standardize, one hot encoding) on test set that was 
done on the training set 



# Processing Tabular Data for Classical Machine Learning Algorithms  
# Using Scikit-Learn pt 2.


Preparing Data for Training a Machine Learning Model 


use scikit learn tran_test_split 

import sklearn.model_selection import train_test_split


# x == features, y == labels for each train and test 
# 
X_train, X_test, y_train, y_test = train_test_split(features, 
                                                    labels, 
                                                    test_size = 0.2,
                                                    random_state=32)

from sklearn.preprocessing import StandardScaler

numerical_columns = ['Age', 'Tenure', 'Usage Frequency', 'Support Calls', 
                     'Payment Delay', 'Total Spend', 'Last Interaction']

X_train_numerical = X_train[numerical_columns]  ## Just pull out numerical cols from X_train 

scaler = StandardScaler()


## NOTE:  
  scaler.fit()        computes  the mean and the std dev of each column
  scaler.transform()  uses the mean and std deviation to scale the values of each column

scaler.fit(X_train_numerical)                         # first part of transforming values
X_train_scaled = scaler.transform(X_train_numerical)  # scale numerical values accordingly

## below creates a data frame with X_train_scale as data and pass in index and columns
## 
X_train_scaled_df = pd.DataFrame(data=X_train_scaled, index=X_train.index, columns=numerical_columns)

X_train_scaled_df.head(5) shows all features have been scaled to lay within a similar range 

 	        Age 	    Tenure 	    Usage Frequency 	Support Calls 	Payment Delay 	Total Spend 	Last Interaction
21498 	-0.751860 	-0.882836 	-1.375749 	      0.129657 	      1.578870 	      -1.703974 	  -1.336266
302478 	0.291831 	  -0.477252 	0.254667 	        -1.172992 	    -1.207105 	    0.661850 	    0.525345
253281 	-0.912428 	1.376846 	  -0.909916 	      -1.172992 	    0.973223 	      1.232594 	    1.339801
177014 	-0.189873 	0.913322 	  -1.259290 	      1.106644 	      0.488706 	      0.320443 	    -0.754513
308214 	0.291831 	  1.492727 	  -0.327624 	      -0.521667 	    -0.964846 	    1.315442 	    0.059943


from sklearn.preprocessing import OneHotEncoder     ## handles One Hot Encoding

categorical_columns = ['Subscription Type', 'Contract Length']  # set column names for categories / One Hot

X_train_categorical = X_train[categorical_columns]  ## now get the training set just for these columns

## NOTE:  
  encoder.fit()   - check the unique values within each categorical column 
                  - prepares the labels of the output columns

X_train_encoded = encoder.transform(X_train_categorical)  ## returns encoded columns as compressed matrix

print(type(X_train_encoded))
<class 'scipy.sparse._csr.csr_matrix'>       #csr = compresses sparse row


print(X_train_encoded)          # returns only locations where 1's are stored

<Compressed Sparse Row sparse matrix of dtype 'float64'
	with 705330 stored elements and shape (352665, 6)>
  Coords	Values
  (0, 2)	1.0
  (0, 3)	1.0
  (1, 0)	1.0
.....
  (352663, 0)	1.0
  (352663, 3)	1.0
  (352664, 1)	1.0
  (352664, 4)	1.0


X_train_encoded.todense()       ## convert to regular matrix
matrix([[0., 0., 1., 1., 0., 0.],
        [1., 0., 0., 0., 0., 1.],
        [1., 0., 0., 0., 0., 1.],
        ...,
        [1., 0., 0., 1., 0., 0.],
        [1., 0., 0., 1., 0., 0.],
        [0., 1., 0., 0., 1., 0.]])

encoder.get_feature_names_out()   ## column names of One Hot Columns
array(['Subscription Type_Basic', 'Subscription Type_Premium',
       'Subscription Type_Standard', 'Contract Length_Annual',
       'Contract Length_Monthly', 'Contract Length_Quarterly'],
      dtype=object)



## now create data frame with encoded categorical columns

X_train_encoded_df = pd.DataFrame(X_train_encoded.todense(),
                                  index=X_train.index,
                                  columns = encoder.get_feature_names_out())

 	    Subscription Type_Basic 	Subscription Type_Premium 	Subscription Type_Standard 	Contract Length_Annual 	Contract Length_Monthly 	Contract Length_Quarterly
21498 	0.0 	0.0 	1.0 	1.0 	0.0 	0.0
302478 	1.0 	0.0 	0.0 	0.0 	0.0 	1.0
253281 	1.0 	0.0 	0.0 	0.0 	0.0 	1.0
177014 	0.0 	1.0 	0.0 	0.0 	0.0 	1.0
308214 	0.0 	1.0 	0.0 	1.0 	0.0 	0.0


## Now concatenate the CustomerID, the scaled df and the encoded df, axis=1 means concat horizontally
## 

X_train_transf = pd.concat([X_train['CustomerID'], 
                            X_train_scaled_df,
                            X_train_encoded_df], axis=1)

X_train_transf.head(5)


 	CustomerID 	Age 	Tenure 	Usage Frequency 	Support Calls 	Payment Delay 	Total Spend 	Last Interaction 	Subscription Type_Basic 	Subscription Type_Premium 	Subscription Type_Standard 	Contract Length_Annual 	Contract Length_Monthly 	Contract Length_Quarterly
21498 	21507.0 	-0.751860 	-0.882836 	-1.375749 	0.129657 	1.578870 	-1.703974 	-1.336266 	0.0 	0.0 	1.0 	1.0 	0.0 	0.0
302478 	309592.0 	0.291831 	-0.477252 	0.254667 	-1.172992 	-1.207105 	0.661850 	0.525345 	1.0 	0.0 	0.0 	0.0 	0.0 	1.0
253281 	260395.0 	-0.912428 	1.376846 	-0.909916 	-1.172992 	0.973223 	1.232594 	1.339801 	1.0 	0.0 	0.0 	0.0 	0.0 	1.0
177014 	181825.0 	-0.189873 	0.913322 	-1.259290 	1.106644 	0.488706 	0.320443 	-0.754513 	0.0 	1.0 	0.0 	0.0 	0.0 	1.0
308214 	315328.0 	0.291831 	1.492727 	-0.327624 	-0.521667 	-0.964846 	1.315442 	0.059943 	0.0 	1.0 	0.0 	1.0 	0.0 	0.0


## scale the numerical columns in test set
## 
X_test_scaled = scaler.transform(X_test[numerical_columns])
X_test_scaled_df = pd.DataFrame(data=X_test_scaled, 
                                index=X_test.index, 
                                columns = numerical_columns)

## encoding the categorical columns in testing set
## 
X_test_encoded = encoder.transform(X_test[categorical_columns])
X_test_encoded_df = pd.DataFrame(data=X_test_encoded.todense(), 
                                 index=X_test.index,
                                 columns = encoder.get_feature_names_out())


## concatenate the two dataframes 
## 
X_test_trans = pd.concat([X_test['CustomerID'],
                          X_test_scaled_df, 
                          X_test_encoded_df], axis=1)

X_test_trans.head(5)

 	CustomerID 	Age 	Tenure 	Usage Frequency 	Support Calls 	Payment Delay 	Total Spend 	Last Interaction 	Subscription Type_Basic 	Subscription Type_Premium 	Subscription Type_Standard 	Contract Length_Annual 	Contract Length_Monthly 	Contract Length_Quarterly
207158 	212278.0 	0.452398 	1.260965 	0.836958 	-0.847330 	0.004189 	0.795828 	0.408995 	0.0 	0.0 	1.0 	0.0 	1.0 	0.0
383379 	390494.0 	-0.029305 	-0.766954 	1.652166 	-0.196005 	0.609835 	-0.260777 	1.805204 	1.0 	0.0 	0.0 	1.0 	0.0 	0.0
258610 	265724.0 	0.452398 	0.044213 	0.487584 	-0.847330 	-1.570493 	1.165128 	-1.219915 	1.0 	0.0 	0.0 	1.0 	0.0 	0.0
299098 	306212.0 	-0.029305 	-1.346360 	0.254667 	-0.847330 	0.125318 	0.866079 	-0.870863 	0.0 	0.0 	1.0 	1.0 	0.0 	0.0
340382 	347497.0 	-1.715267 	-0.940776 	1.535708 	-0.847330 	0.125318 	-0.300309 	-0.987214 	0.0 	1.0 	0.0 	1.0 	


X_train_transf.to_parquet("train.parquet")   ## Write out as parquet files
X_test_trans.to_parquet("test.parquet")



# Practice Lab: Feature Engineering for ML


## Modeling and Processing Unstructured Data for Machine Learning
# Modeling Image Data for ML Algorithms

Convolutional Neural Network (CNN)
  - made up of several layers
  - each layer tries to identify more image features  
    to help with the ML task
        - First layer: generic features
        - Later layer: Complex patterns and textures


- Start with pre-trained CNN algorithms
- fine tune these models for the specific task


Preparing Image Data for Training an ML Algorithm 
- resize image
- scale pixels

Data Augmentation: technique used to create new versions of existing images 
    - flipping, rotating, cropping, adjusting brightness
    - can increase the size and variety of training data


TensorFlow
  - has pre-processing functions that could be deployed to images

# Image Preprocessing Using TensorFlow

https://colab.research.google.com/drive/1NVBjtIfGuYoSomwBYkPg6hqyhj7HNhLS?usp=sharing#scrollTo=GB_biegiU5xU



# Preprocessing Textual Daa for Analysis and Text Classification

- Sentiment analysis of product reviews 
- Classification of news articles
- Chatbots and virtual assistants
- Spam detection
- Customer segmentation
- Product recommendations



Pre-processing Texts for ML 



[Textual Data might contain ] 
[contain typos, &           ]             [ Clean and   ]
[inconsistencies repititions]             [high-quality ]      ML
                                Data      [data         ]     Engineering           [Train a classical or ]
[May contain words or      ]   Engineer                       Team                  [ advanced ML model   ]
[characters not relevant   ]   ------->>>                      ------->>>
[to the NLP Task           ]
                                          [Removve and ]                            [  Other use cases    ]
[Training LLMs is expensive ]             [ irrelevant ]
[and time consuming         ]             [ words or   ]
                                          [ characters ]

Processing Text 

Cleaning: 
  - removing punctuations, extra spaces, characters that add no meaning 

Normalization:
  - convert test to a consistent format
      - transforming to lower-case
      - converting numbers or symbols to characters
      - expanding contractions / replacing accronyms, abbreviations

Tokenization:
  - splitting each review into individual tokens 
    ( works, subswords, short sentences)
  python str.split()
    ex. "This is a wonderful price for the amount you get" 
    becomes [this, is, a, wonderful, price, for, the, amount, you, get]


Removal of Stop Words:
  - removing frequently used words such as: "is", "are", "the", "for", "a"
  - define own list of stop words
  - use built-in set of NLP libraries: spaCy, NLTK, Gensim, TextBlob
    from [this, is, a, wonderful, price, for, the, amount, you, get]
    to   [this, wonderful, price, you, get]

Lemmatization: 
  - replacing each word with its base form or lemma
      i.e. getting / got  -->>  get
  - can use NLP libraries to get lemma of each word 

  ex. [i, bought, this, my, son, his, hair, thinning, i, do, not, know, yet, how,
       well, helping, he, said, smell, great]

to >> [i, buy, this, my, son, his, hair, thin, i, do, not, know, yet, how,
       well, help, he, say, smell, great ]



This file (functions_preprocess_texts.py) contains python functions that you can 
use to pre-process texts by: 

    - removing special characters and extra spaces
    - expanding contractions
    - converting characters to lower case
    - removing stop words
    - lemmatizing


# Text Vectorization and Embedding


Traditional Vectorization: assign a number to each word in a text based on its frequency of occurance
  - Bag of Words
      - each entry is number of occurances word appears
        - Only takes into account the word frequency in each document
        - Some frequently appearing words might carry little meaning
                ex.  "purchase" "buy"      -->> high frequency, little meaning
                ex.  "break" "exceptional" -->> low frequency, more significant


  - Term-Frequency Inverse-Dcoument-Frequency (TF-IDF)
      - Account for the weight and rarity of each word
          TF: the number of times the term occurred in a document divided by the total number of words in that document

         IDF: how common or rare that word is in the entire corpus
                closer to 0: more common
                closer to 1: more rare


                      Reviews

        +--> [this, wornderful, price, amount, you, get]     <<-- document
        |
corpus--+--> [great, product, big, amount]
        |
        +--> [I, buy, this, my, son, his, hair, thin, I, do, not, know, yet, how, well, help, he, say, smell, great]


the
vocabulary [this, wonderful, price, amount, you, get, great, product, big, I, buy, my, son, 
           his, hair, thin, do, not, know, yet, how, well, help, he, say, smell ]    # list of unique words in collection 

each document becomes a vector of vocabulary list:

this|wonderful|price|amount|you|get|great|product|big|I |buy|my |son|his|hair|thin|do |not|know|yet|how|well|help| he|say|smell 
 1     1        1      1     1   1   0      0      0   0  0   0    0  0   0     0   0   0    0   0   0   0    0     0  0   0
 0     0        0      1     0   0   1      1      1   0  0   0    0  0   0     0   0   0    0   0   0   0    0     0  0   0
 1     0        0      0     0   0   1      0      0   2  1   1    1  1   1     1   1   1    1   1   1   1    1     1  1   1
 
 
NOTE: scikit learn can vectorize text using bag of words method  and tf-idf

TF-IDF methodology
this|wonderful|price|amount|you|get|great|product|big|I |buy|my |son|his|hair|thin|do |not|know|yet|how|well|help| he|say|smell 
0.33  0.44      0.44  0.33 0.44 0.44 0      0      0   0  0   0    0  0   0     0   0   0    0   0   0   0    0     0  0   0
 0     0        0     0.43   0   0  0.43   0.56   0.56 0  0   0    0  0   0     0   0   0    0   0   0   0    0     0  0   0
0.16   0        0      0     0   0  0.16    0      0  043 0.22 0.22    0.22  0.22   0.22     0.22   0.22   0.22    0.22   0.22   0.22   0.22    0.22     0.22  0.22   0.22


Word Embedding: Vector that captures the semantics meaning of the word
  i.e. if two words have similar meaning they are "close" to each other

        "useful" embedding vector should be near "helpful" vs. "tree"

generate embedding vectors using algorithms"
  - word2vec --  Trained to learn the embeddings of words from their co-occurances in large collections of text
  - GLOVE    --


Sentence Embedding:  Vector that reflects the semantic meaning of the sentence
      - takes into account the position of a word in a sentence and the meaning of each word  
      - sentences with similar meanings, embedding vectors will be close to each other 
      - lower dimension than the vector generated by TF-IDF

Pre-trained NLP models
based on Large Language Models 

  Open-Source                    Closed-Source

Sentence Transformers               OpenAI
  (sbert.net)                       Anthropic
                                    Gemini


Text Reviews -->> Embeddings ------>> Features to train an ML Algo 
                                |
                                +-->> Clustering, Similarity Search  


Assignment: Modeling and Transforming Text Data for ML

Different encoding techniques for Feature Engineering

https://medium.com/anolytics/all-you-need-to-know-about-encoding-techniques-b3a0af68338b


Resources

Courses

https://www.deeplearning.ai/courses/machine-learning-specialization/

https://www.deeplearning.ai/courses/deep-learning-specialization/


Andrew explains the best practice of applying the same preprocessing steps and 
computed statistics from the training set to the testing set 
https://www.coursera.org/learn/deep-neural-network/lecture/lXv6U/normalizing-inputs

Convolutional Neural Networks
https://www.coursera.org/learn/convolutional-neural-networks?specialization=deep-learning

https://www.deeplearning.ai/courses/natural-language-processing-specialization/

https://www.deeplearning.ai/courses/machine-learning-in-production/

https://www.deeplearning.ai/courses/generative-ai-with-llms/

https://www.deeplearning.ai/short-courses/preprocessing-unstructured-data-for-llm-applications/

https://www.deeplearning.ai/short-courses/google-cloud-vertex-ai/

https://www.deeplearning.ai/short-courses/large-language-models-semantic-search/

https://www.deeplearning.ai/short-courses/serverless-llm-apps-amazon-bedrock/

https://www.deeplearning.ai/short-courses/pretraining-llms/

Reference

Pandas:
https://www.kaggle.com/learn/pandas
https://www.w3schools.com/python/pandas/default.asp


Sci-kit:
https://scikit-learn.org/stable/modules/preprocessing.html
https://scikit-learn.org/stable/modules/impute.html
https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction

https://www.kaggle.com/datasets/muhammadshahidazeem/customer-churn-dataset

https://medium.com/@lars.chr.wiik/best-embedding-model-openai-cohere-google-e5-bge-931bfa1962dc

https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2

https://www.sbert.net/index.html

https://seattledataguy.substack.com/p/data-engineering-vs-machine-learning

https://docs.cohere.com/docs/embeddings

https://docs.cohere.com/docs/how-to-convert-text-into-vectors

https://cohere.com/blog/sentence-word-embeddings

https://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/

https://p.migdal.pl/blog/2017/01/king-man-woman-queen-why

https://medium.com/deeper-learning/glossary-of-deep-learning-word-embedding-f90c3cec34ca

https://medium.datadriveninvestor.com/from-word-embeddings-to-sentence-embeddings-part-1-3-7ba9a715e917



## Data Modeling, Transformation and Serving
## Course 4
## Week 3
## Data Transformations & Technical Considerations 

# Batch Transformations


Transformation:  Manipulate and enhance data for downstream stakeholders


Transformations 

      Technical Considerations        |       Transformation Approaches
--------------------------------------------------------------------------------------
  Batch Transformations               |         - Single machine or use a distributed 
    - Size of the Data                |             processing tool (i.e. Spark)
    - Specification of the hardware   |         - Write transformation logic in SQL or Python 
    - Performance requirement         |
                                      |
  Streaming Transformation            |
    - Latency requirement             |
                                      |


# Batch Transformation Patterns and Use Cases



Transformations for Data Modeling

Target Model        Star            Data Vault


[data source]   -->>   [ Transformation ]  -->>   [Final Data Form]



ETL :  [data  ]  -->> [Extract ] -->> [Transform / ]  -->> [ Load ] -->> [   Target System  ]
       [source]                       [Staging Area]                     [Expected Data Form]



ELT :  [data  ]  -->> [Extract ] -->> [ Load  to     ]  -->> [ Transform in  ] -->> [ Expected Data Form ]
       [source]                       [Target System ]       [ Target System ] 


EtLT : [data  ]  -->> [Extract ] -->> [transform   ] -->> [ Load  to     ]  -->> [ Transform in  ] -->> [ Expected Data Form ]
       [source]                       [clean/dedupe]      [Target System ]       [ Target System ] 


examples:                             +----- based on Spark
                                      |
                                      v
ETL:            [AWS RDS] -->>  [AWS Glue ETL] -->>  [S3]


ELT:            [Target System                    ]
                [                                 ]  Note:  
                [                                 ]    - dbt is not an execution tool like Spark.
                [ source data          expected   ]
                [   |                  data form  ]    - SQL-tool that facilitates transformation 
                    |                      ^             within the database or data warehouse
                    v                      |             (utilizes resources of data platform itself)
                [------------- dbt  --------------]
                [                                 ]



Transformations for Data Cleaning

Data Wrangling:  take messy, malformed data and turn into clean data


[ Data  ]   ----------------->>  [ETL / ELT]  ----------------->>  [ Target ]  
[Source ]       source data                                        [ System ]
             
             - missing values                 clean and normalize
             - dupe entries                     - write your own code
             - outliers                         - Use wrangling tool
             - inconsistencies                      - AWS Glue DataBrew





Transformations for Data Updating


Truncate and Reload :  Delete all records in target system and reload the data from the source
                        - have small data set
                        - updated data once in a while


Change Data Capture :  Identify and capture changes in source system and apply to target
                        - check last updated columnt
                        - check db transaction log
                            I / U / D
        Capture Updates: 
            - Insert Only
                - insert new records without change / deleting old records 
                - add additional information  to new record to distinguish it from old record
            - Upsert Merge
                - use key of new data to search for existing record 
                - if key exists, update record with new
                - if key doesn't exists, insert new record

        Capture deletes: 
             - Hard delete: permanently delete record 
             - Soft delete: mark record as "deleted"
             - Insert only:  insert record with "deleted" flag and do not modify prior record



Single Row Inserts:  OK for row-oreiented database


Single Row Inserts:  BAD for OLAP Column-oriented databases:
                      - puts massive load on OLAP system
                      - Extremely inefficient for subsequent reads

                    GOOD for OLAP Column-oriented databases:
                        - micro-batch or batch fashion
                        - when inserted in bulk 
                            - organized data more efficiently into row groups
                                  and better compressed
                            - Leverage distributed parallel processing to speed up data loading





# Distributed Processing Framework: Hadoop MapReduce


2000s Big Data:   Commodity hardware became cheap and ubiquitous
                  Several innovations in large-scale distributed storage and computing:

2003  GFS:        Google File System  (GFS)

2004  MapReduce:  Parallel programming paradigm for processing data distributed over GFS

2006  Hadoop:     Yahoo Hadoop
                    - Hadoop Distributed File System 
                        - Key ingredient of current big data engines (EMR, Spark)

                    - Hadoop MapReduce
                        - Influences many of today's distributed systems

Hadoop Distributed File System

HDFS:   combines compute and storage on the same nodes

Object Storage: limited compute support for internal processing


HDFS: 
Large File -->> [1] [2] [3]  large file broken into blocks of few hundred megabytes in size

                                    [Data Node]           [Data Node ]
+------------------------+       +  [ [1] [2] ]           [[1][3][2] ]        Each block of data replicated to 3 data nodes
| NameNode               |       |                          +
| Maintains:             |       |                          |                                 
|    - Directories       |-------+--------------------------+                 Replication increases durability and 
|    - File metadata     |       |                          |                  availability of the data
|    - Detailed catalog  |       |                          +
|                        |       + [Data Node ]           [Data Node ]        If data node fails, Name Node will redistribute data
+------------------------+         [  [2][3]  ]           [ [1][3]   ]         to maintain 3x replication  


                                                                              Combining compute and storage allows in-place
                                                                              data processing (via MapReduce)




Hadoop MapReduce

MapReduce:  send computation code to the nodes that contain the data

                 (hashing the key)
      [  Map  ]  ----------------->>  [  Shuffle  ]     ----------------->>  [ Reduce ]  
Map task reads data               Shuffle redistributes the                 Aggregates data 
bloocks and produce a           Map results across cluster                   on each node
set of key-value pairs          so values with same key are 
                                at the same nodes

+-----------------------+        +-----------------------+                +------------------------+            
| Data Node             |        | Data Node             |                | Data Node              |                      
|  [ 1 ]  [ 2 ]  [ 3 ]  |        |                       |                |                        |-------+ 
|    |      |      |    |        |  k'-v   k'-v    k'-v  |                |  k' -->> [ Agg Value ] |       |
|    |      |      |    |        +-----------------------+                +------------------------+       |    
|    v      v      v    |                                                                                  +---- collect 
|   k'-v   k-v    k'-v  |        +-----------------------+                +------------------------+       |     results
|   k-v    k'-v   k-v   |        | Data Node             |                | Data Node              |       |                          
+-----------------------+        |                       |                |                        |-------+                
                                 |   k-v    k-v    k-v   |                |  k  -->> [ Agg Value ] |                    
                                 +-----------------------+                +------------------------+            



Hadoop MapReduce Shortcomings

  [disk] --read--> [mapreduce] --write--> [disk] --read--> [mapreduce] --write--> [disk]
                   [   task  ]                             [   task  ]


- utilizes many short lived MapReduce tasks that read data from disk 
  and write intermediate computations to disk

- no results saved to memory, all data transferred between tasks stored 
  to disk and pushed over network

              Pros                                      Cons
- simplifies state and workflow mgmt                - drives high-disk bandwidth utilization              
- minimizes memory consumption                      - increases processing time



Spark:   [RAM] --read--> [mapreduce] --write--> [RAM] --read--> [mapreduce] --write--> [RAM]
                         [   task  ]                            [   task  ]

- In-memory processing speeds up data processing tasks
- Disk is treated as a second-class data storage layer
I


# Distributed Processing Framework: Spark 


Spark:  Computing engine designed for processing large datasets 
          - Retains intermediate results in memory 
          - Limits disk I/O interactions, enabling faster computation


Spark:   [RAM] --read--> [mapreduce] --write--> [RAM] --read--> [mapreduce] --write--> [RAM]
                         [   task  ]                            [   task  ]


    Run it                              Separate persistent storage systems
      - Locally                             - RDBMS
      - Data centers                        - Object Store
      - Cloud                               - HDFS


Unified Platform:     Can run different types of analytical workloads on one engine
                        - Perform SQL queries
                        - Train and test ML algorithms
                        - Apply streaming transformations

          Spark Libs:   [ Sparl ]     [ MLlib ]   [  Spark  ]   [ GraphX  ]
                        [  SQL  ]                 [Streaming]

          Spark APIs    [ Python ]    [ Java ]    [ Scala ]     [    R    ]   <--> External 3rd party libs


Spark Applications


                    +---------------+              +---------------+
                    | Driver Node   |              | Worker Node   |
                    |               |              |               |
Central controller  | [  Spark  ]   |--------------|   Executor    |
of a Spark          | [ Session ]   |              | [CPU]  [CPU]  |
Applications -------|    |   ^      |              | [CPU]  [CPU]  |              +---------------+
                    |    v   |      |              +---------------+              | Worker Node   |
                    | [User Code ]  |                  |      ^                   |               |
                    |               |------------------|------|-------------------|   Executor    |
                    |               |                  |      |                   | [CPU]  [CPU]  |
                    +---------------+                  |      |                   | [CPU]  [CPU]  |
                        |      ^                       |      |                   +---------------+
                        |      |                       |      |                       |      ^
                        v      |                       v      |                       v      |
                    +-------------------------------------------------------------------------------------------------+ 
Allocates and       |                                                                                                 |
manages resources   |                                          Cluster Manager                                        |
          +---------|                                                                                                 |
                    +-------------------------------------------------------------------------------------------------+





Driver Node     :  Central cotnroller of Spark Application 

Cluster Manager :  Communicates with Driver Node.  Allocates computational and memory resources 
                    across the cluster and manages resources

Worker Node     :  Contains Spark executor that executes tasks assigned to them by Driver Node.


Spark applies partitioning scheme to break up data into partitions when loading from disk and  
allocates to each executor the partitins that are closest to it in the network.
    - each executors cpu core gets a partition of data to work on

Spark Application:    start with instantiating a Spark session object: 
                        - represents a single unified entry point to all Spark's functionality
                        - via sesion object: 
                            - define data frames
                            - read data sources 
                            - perform SQL queries

                      Driver node translates instructions (via python, scala, etc.)  into Spark Jobs
                        - will be executed one by one based on job's priority.  
                        - Driver transforms each job into sequence of stages  
                            - represented by a Directed Acyclical Graph (DAG)
                            - DAG like execution plan for each job
                            - Each stage broken down into tasks (written in smart code) 
                              that can run in parallel. 
                            - Executor executes the tasks and returns results to Driver Node
                        - Driver aggregates computations across all tasks and returns results back 

       +---------------------------------------------+
       |    [ Job ]   [ Job ]   [ Job ]   [ Job ]    |
       +---------------------------------------------+
                         |
                         v
       +---------------------------------------------------------+
       | DAG                                                     |
       |                    * Run in parallel    +->  [Task]     |
       |                          +-[ Stage 2]---+->  [Task]     |
       |                          |              +->  [Task]     |
       |                          |              +->  [Task]     |
       |                          |                              |
       |                          |                              |
       |    [ Stage 1 ]-----------+                              |
       |      | | | |             |                              |
       |      v | | |             |                              |
       |   Task | | v             |               +->  [Task]    |
       |        v | Task          |               +->  [Task]    |
       |     Task |               +-[ Stage 3]----+->  [Task]    |
       |          v         ** Run in parallel    +->  [Task]    |
       |         Task                                            |
       |                                                         |
       |                                                         |
       +---------------------------------------------------------+




Spark DataFrames


+--------------------------------------------------+
| High-level Structured API                        |  <-- Use simpler / more expressive operations:
|                                                  |      - filtering, selecting, counting, aggregating, grouping
|      Spark DataFrames                            |
|                                                  |
|                                                  |
|                                                  |
|                                                  |
|                                                  |  ----->>  Two types of operations
+--------------------------------------------------+              Transformation : filtering, selecting, joining, grouping
                        |                                                create new data frames without modifying original data
      Immutable data structures (fault tolerant)                 
                        |                                         Actions:  counting, showing, saving
+--------------------------------------------------+                     triggers the execution of the transformation*
| Low-level Structured API                         |              
|                                                  |
|      Resilient Distributed DataFrames (RDD)      |
|      (actual paritioned collection of records)   |            (orig Immutable)     Lazy Evluation*
|                                                  |               [Data Frame]     -----------------> [Data Frame]  ---------->   [Data Frame]             
|                                                  | 
|                                                  | 
|                                                  |    *NOTE: 
|                                                  |          - Transformations are recorded as a lieneage, only executed when                 
+--------------------------------------------------+              an action is invoked

                                                              - Lazy evaluation allows Spark to optimize exeuction plan by rearranging
                                                                  transformation operations for efficiency  

                                                              - Lineage and immutability ensure fault tolerance
                                                                    - allows you to reporduce original state in case of failure
                                                    
                                                    



Working with Spark DataFrames using Python

# Create a Spark session first
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName('example').getOrCreate()


## show Spark session:
spark

NOTE:  See all of the below in spark/demo_walk_thru.ipynb

SparkSession - in-memory

SparkContext

Spark UI

Version   v3.5.3
Master    local[*]
AppName   example
    

## create data frame, takes two arguements: data and schema
orders_df = spark.createDataFrame(
    data = [(100, 1, 1, 50.1, 1, "Thingamjig", 5, "Joe Reis"),
            (100, 2, 2, 25.08, 2, "Whatchamacallit", 5, "Joe Reis"),
            (101, 1, 3, 75.23, 1, "Whoozeewhatzit", 7, "Matt Housley")],
    schema = """ OrderID long, ItemNumber integer, SKU integer, Price double, Quantity integer, Name string, CustomerID long, CustomerName string""")

## show dataframe
orders_df.show()

+-------+----------+---+-----+--------+---------------+----------+------------+
|OrderID|ItemNumber|SKU|Price|Quantity|           Name|CustomerID|CustomerName|
+-------+----------+---+-----+--------+---------------+----------+------------+
|    100|         1|  1| 50.1|       1|     Thingamjig|         5|    Joe Reis|
|    100|         2|  2|25.08|       2|Whatchamacallit|         5|    Joe Reis|
|    101|         1|  3|75.23|       1| Whoozeewhatzit|         7|Matt Housley|
+-------+----------+---+-----+--------+---------------+----------+------------+


## Load online transaction data: https://doi.org/10.24432/C5CG6D
## or https://archive.ics.uci.edu/static/public/502/online+retail+ii.zip 
## 
## NOTE: download was an xlsx file, had to do a save as to .csv

# now lets import a csv file into a dataframe,  still using the original spark object to do this:
transactions_df = spark.read.csv('online_retail_map.csv', header=True)

# show first 6 records:
transactions_df.show(n=6)
+-------+---------+--------------------+--------+--------------+-----+-----------+--------------+
|Invoice|StockCode|         Description|Quantity|   InvoiceDate|Price|Customer ID|       Country|
+-------+---------+--------------------+--------+--------------+-----+-----------+--------------+
| 489434|    85048|15CM CHRISTMAS GL...|      12|12/1/2009 7:45| 6.95|      13085|United Kingdom|
| 489434|   79323P|  PINK CHERRY LIGHTS|      12|12/1/2009 7:45| 6.75|      13085|United Kingdom|
| 489434|   79323W| WHITE CHERRY LIGHTS|      12|12/1/2009 7:45| 6.75|      13085|United Kingdom|
| 489434|    22041|"RECORD FRAME 7""...|      48|12/1/2009 7:45|  2.1|      13085|United Kingdom|
| 489434|    21232|STRAWBERRY CERAMI...|      24|12/1/2009 7:45| 1.25|      13085|United Kingdom|
| 489434|    22064|PINK DOUGHNUT TRI...|      24|12/1/2009 7:45| 1.65|      13085|United Kingdom|
+-------+---------+--------------------+--------+--------------+-----+-----------+--------------+
only showing top 6 rows


# show the columns
transactions_df.columns

# select the columns, and bring back first 6 rows
transactions_df.select('Price', 'Quantity', 'Country').show(n=6)

+-----+--------+--------------+
|Price|Quantity|       Country|
+-----+--------+--------------+
| 6.95|      12|United Kingdom|
| 6.75|      12|United Kingdom|
| 6.75|      12|United Kingdom|
|  2.1|      48|United Kingdom|
| 1.25|      24|United Kingdom|
| 1.65|      24|United Kingdom|
+-----+--------+--------------+
only showing top 6 rows

# get summaries of columns
transactions_df.select('Price', 'Quantity', 'Country').describe().show()

[Stage 6:==============>                                            (2 + 6) / 8]

+-------+------------------+------------------+-----------+
|summary|             Price|          Quantity|    Country|
+-------+------------------+------------------+-----------+
|  count|           1067371|           1067371|    1067371|
|   mean| 4.649387727415791|   9.9388984711033|       NULL|
| stddev|123.55305872146302|172.70579407675285|       NULL|
|    min|         -11062.06|                -1|  Australia|
|    max|             99.96|               992|West Indies|
+-------+------------------+------------------+-----------+

# Create a new column that is a calculation of two other columns
transactions_df= transactions_df.withColumn(colName='Amount', col= transactions_df.Price * transactions_df.Quantity)
transactions_df.show(n=6)
+-------+---------+--------------------+--------+--------------+-----+-----------+--------------+------------------+
|Invoice|StockCode|         Description|Quantity|   InvoiceDate|Price|Customer ID|       Country|            Amount|
+-------+---------+--------------------+--------+--------------+-----+-----------+--------------+------------------+
| 489434|    85048|15CM CHRISTMAS GL...|      12|12/1/2009 7:45| 6.95|      13085|United Kingdom|              83.4|
| 489434|   79323P|  PINK CHERRY LIGHTS|      12|12/1/2009 7:45| 6.75|      13085|United Kingdom|              81.0|
| 489434|   79323W| WHITE CHERRY LIGHTS|      12|12/1/2009 7:45| 6.75|      13085|United Kingdom|              81.0|
| 489434|    22041|"RECORD FRAME 7""...|      48|12/1/2009 7:45|  2.1|      13085|United Kingdom|100.80000000000001|
| 489434|    21232|STRAWBERRY CERAMI...|      24|12/1/2009 7:45| 1.25|      13085|United Kingdom|              30.0|
| 489434|    22064|PINK DOUGHNUT TRI...|      24|12/1/2009 7:45| 1.65|      13085|United Kingdom|39.599999999999994|
+-------+---------+--------------------+--------+--------------+-----+-----------+--------------+------------------+
only showing top 6 rows

# lets rename an existing column name
transactions_df = transactions_df.withColumnRenamed(existing='Invoice', new='ID')

# lets drop a column
transactions_df = transactions_df.drop('Description')

# delete rows with null values
transactions_df = transactions_df.dropna()

# lets filter rows 
transactions_df = transactions_df.filter(transactions_df.Quantity>0)

+------+---------+--------+--------------+-----+-----------+--------------+------------------+
|    ID|StockCode|Quantity|   InvoiceDate|Price|Customer ID|       Country|            Amount|
+------+---------+--------+--------------+-----+-----------+--------------+------------------+
|489434|    85048|      12|12/1/2009 7:45| 6.95|      13085|United Kingdom|              83.4|
|489434|   79323P|      12|12/1/2009 7:45| 6.75|      13085|United Kingdom|              81.0|
|489434|   79323W|      12|12/1/2009 7:45| 6.75|      13085|United Kingdom|              81.0|
|489434|    22041|      48|12/1/2009 7:45|  2.1|      13085|United Kingdom|100.80000000000001|
|489434|    21232|      24|12/1/2009 7:45| 1.25|      13085|United Kingdom|              30.0|
|489434|    22064|      24|12/1/2009 7:45| 1.65|      13085|United Kingdom|39.599999999999994|
+------+---------+--------+--------------+-----+-----------+--------------+------------------+
only showing top 6 rows

# calculate total amount spent on each order
transactions_df.groupby("ID").sum("Amount").show()

+------+------------------+
|    ID|       sum(Amount)|
+------+------------------+
|489677|             192.0|
|491045|             303.2|
|491658|155.05999999999997|
|493542|            118.75|
|493977|            275.95|
|494244|            6711.0|
|494277|           1335.92|
|495185|           2507.06|
|495783|             48.96|
|496171|199.29999999999998|
|496233|188.82999999999998|
|496427|291.14000000000004|
|497229| 312.5899999999999|
|498070|207.15000000000006|
|498125|190.95000000000005|
|498328|            275.04|
|500148|431.59000000000015|
|500903|           2655.96|
|500979|              11.9|
|501046|             612.0|
+------+------------------+
only showing top 20 rows



# total number of rows for each country in descending order
transactions_df.groupby("Country").count().orderBy('count', ascending=False).show()


[Stage 16:==================================================>       (7 + 1) / 8]

+---------------+------+
|        Country| count|
+---------------+------+
| United Kingdom|725296|
|        Germany| 16703|
|           EIRE| 15745|
|         France| 13813|
|    Netherlands|  5093|
|          Spain|  3720|
|        Belgium|  3069|
|    Switzerland|  3012|
|       Portugal|  2446|
|      Australia|  1815|
|Channel Islands|  1569|
|          Italy|  1468|
|         Norway|  1437|
|         Sweden|  1319|
|         Cyprus|  1155|
|        Finland|  1032|
|        Austria|   922|
|        Denmark|   798|
|         Greece|   657|
|    Unspecified|   521|
+---------------+------+
only showing top 20 rows



Spark supports User-Defined Functions (UDF)
create function and then wrap in a UDF object

def toUpper(word:str):
    return word.upper()


from pyspark.sql.functions import udf

## wrap local function in UDF object
## 
udf_to_upper = udf(toUpper, returnType='string')
transactions_df.select('ID', udf_to_upper('Country')).show(n=6)

+------+----------------+
|    ID|toUpper(Country)|
+------+----------------+
|489434|  UNITED KINGDOM|
|489434|  UNITED KINGDOM|
|489434|  UNITED KINGDOM|
|489434|  UNITED KINGDOM|
|489434|  UNITED KINGDOM|
|489434|  UNITED KINGDOM|
+------+----------------+
only showing top 6 rows

## NOTE:  Here is the same logic with a decorator


@udf('string')
def toUpper(word:str):
    return word.upper()

## udf_to_upper = udf(toUpper, returnType='string')
transactions_df.select('ID', toUpper('Country')).show(n=6)

+------+----------------+
|    ID|toUpper(Country)|
+------+----------------+
|489434|  UNITED KINGDOM|
|489434|  UNITED KINGDOM|
|489434|  UNITED KINGDOM|
|489434|  UNITED KINGDOM|
|489434|  UNITED KINGDOM|
|489434|  UNITED KINGDOM|
+------+----------------+
only showing top 6 rows



# Let's replace an existing column in place with new values
transactions_df = transactions_df.withColumn(colName='Country', 
                                             col=toUpper('Country'))
transactions_df.show(n=6)

+------+---------+--------+--------------+-----+-----------+--------------+------------------+
|    ID|StockCode|Quantity|   InvoiceDate|Price|Customer ID|       Country|            Amount|
+------+---------+--------+--------------+-----+-----------+--------------+------------------+
|489434|    85048|      12|12/1/2009 7:45| 6.95|      13085|UNITED KINGDOM|              83.4|
|489434|   79323P|      12|12/1/2009 7:45| 6.75|      13085|UNITED KINGDOM|              81.0|
|489434|   79323W|      12|12/1/2009 7:45| 6.75|      13085|UNITED KINGDOM|              81.0|
|489434|    22041|      48|12/1/2009 7:45|  2.1|      13085|UNITED KINGDOM|100.80000000000001|
|489434|    21232|      24|12/1/2009 7:45| 1.25|      13085|UNITED KINGDOM|              30.0|
|489434|    22064|      24|12/1/2009 7:45| 1.65|      13085|UNITED KINGDOM|39.599999999999994|
+------+---------+--------+--------------+-----+-----------+--------------+------------------+
only showing top 6 rows



Python UDFs  / Spark is native in Scala

+--------------------------------------------------------------------------------+ 
|  Worker Node                                                                   |
|                                                                                |
|       +-----------------Data Serialization -------+                            |
|       |                                           v                            |
|   [  JVM  ]                                   [Python ]  Execute the function  |
|   [Process]                                   [Process]  (UDF) row by row      |
|       ^                                           |                            |
|       +-----------------DeSerialization    -------+                            |
|                                                                                |
|                                                                                |
+--------------------------------------------------------------------------------+
NOTE: 
      - The JVM and Python processes will compete for memory resources
      - Serialization and DeSerialization are expensive
      - Consider writing UDFs in Scala or Java; they will run directly within JVM



Demo: Working with Spark SQL


- Need to create a temporary view to interact with SQL 
- persists as long as the spark session is running
- provides an interface for you to work with SQL


transactions_df.createOrReplaceTempView('orders')

use spark session object and use sql method, returns dataframe with results of the query


total_df = spark.sql("""
SELECT ID, SUM(amount) as total
from orders
GROUP BY ID
ORDER BY total DESC
""")


total_df.show(n=6)


[Stage 20:=============================>                            (4 + 4) / 8]

+------+------------------+
|    ID|             total|
+------+------------------+
|581483|          168469.6|
|541431|           77183.6|
|493819|44051.600000000006|
|556444|           38970.0|
|524181|33167.799999999996|
|537659|31770.979999999996|
+------+------------------+
only showing top 6 rows



Can create function and use it within query
  - need to register function for sql queries


# create function
def toLower(word:str):
    return word.lower()

# register function
_ = spark.udf.register("udf_to_lower", toLower)

# use registered function in an sql query
spark.sql("""
SELECT DISTINCT udf_to_lower(country)
FROM orders
""").show()

[Stage 23:==================================================>       (7 + 1) / 8]

+---------------------+
|udf_to_lower(country)|
+---------------------+
|              finland|
|            australia|
|               greece|
|             portugal|
|              nigeria|
|               poland|
|              austria|
|                malta|
|                japan|
|          switzerland|
|               sweden|
|          netherlands|
| united arab emirates|
|                 eire|
|               france|
|          unspecified|
|       united kingdom|
|              germany|
|                  rsa|
|                italy|
+---------------------+
only showing top 20 rows




Can create multiple tables / views and join together

# create a data frame with static data
# 
product_category_df = spark.createDataFrame(
    data = [(22423, 'category_a'), (21212, 'category_b'), 
            (21232, 'category_c'), (84879, 'category_a')],
    schema = 'itemID string, category string')




# create temp table / view called items off of above dataframe
product_category_df.createTempView('items')


spark.sql("""
SELECT category, AVG(amount)
from items
LEFT JOIN orders ON items.itemID = orders.stockcode
GROUP by category
""").show()

[Stage 27:=======>                                                  (1 + 7) / 8]

+----------+------------------+
|  category|       avg(amount)|
+----------+------------------+
|category_c|20.994959999999978|
|category_a| 66.62807636539425|
|category_b|16.604122079879243|
+----------+------------------+



# Amazon Elastic MapReduce (EMR)


EMR can run Spark Jobs

EMR 
  - Massively Parallel Processin (MPP)
  - elastic cluster
  - Job is run in parallel
  - results stored in several places
  - supported frameworks: 
      - hadoop
      - hive
      - presto
      - spark
      - flink
      - hbase

  - Managed Environment
    - simplifies setup / scaling
    - natively integrates with AWS
    - focus on data workflows
    - decouple compute and storage
    - analyze large amount of data

  - Can integrate with AWS services:
    - DynamoDB, RDS, Redshift


ex. 
        S3 -> EMR -> Hadoop

Overview:
  - when start cluster, EMR streams data from S3 to each instance 
    in cluster and begins processing it. 

  - can use multiple EMR clusters to process same S3 data in different ways.


EMR Studio:
  - browswer based IDE for Jupyter Notebooks that run on EMR clusters


NOTE: need EMR Studio to manage EMR Serverless apps
AWS console >  EMR  > EMR Serverless > Get Started > Create and launch EMR Studio

name: example
type: Spark
release ver:  <default>
application setup options:
    choose Use default for interactive workloads
[create] -> [start application]


EMR studio > Dashboard > Create workspace
  Brings Up Error: Need to enable studio for interactive workspaces
    > [Edit Studio]

update 
Studio service role:  choose existing IAM role (emr)
    role allows interaction with other AWS resources

Workspace storage:   [Browse S3]
  - choose S3 bucket for backup location for workspaces

[Save Changes]
[View Workspaces]   (top right)

EMR > Workspaces > [Create Workspace]

workspace name: example
[Create Workspace]

select created workspace [Launch Workspace]
  - this brings up the notebook


Need to connect notebook to compute 
 - compute tab on left (under folder icon)
 - EMR Serverless applicaton (dropdown)
    - select dropdown and choose application created above 
      - exmple 
 - Interactive runtime role  (dropdown)  ## Role that will be assumed to call other AWS services
    - select dropdown and choose interactive runtime role
      - emr-serverless-role 

[Attach]


Files in notebook 
  - select empty notebook
  - select Kernel: PySpark

Run a PySpark script to do data analysis on a file uploaded to S3
 - paste script 
 - shift + enter to run

NOTE:  This is the code that was displayed in the overview 

################################################################################################33
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, avg

spark = SparkSession.builder.appName("AverageFareAmount").getOrCreate()

#Define S3 bucket path to the csv file
s3_bucket_path = s3a://sample-taxi-data-2024/fake_taxi_data.csv"

#Read the dataset from S3 into a Spark Dataframe
df = spark.read.csv(s3_bucket_path, header=True, inferSchema=True)

#define the time period for filtering
start_date = "2020-06-01"
end_date   = "2020-06-30"

# filter the DataFrame for the specified time period
filtered_df = df.filter((col(pickup_datetime") >= start_date) & (col("pickup_datetime") <= end_date))

#calculate the average fare amount
average_fare = filtered_df.select(avg(col("fare_amount")).alias("average_fare_amount))

average_fare.show()
spark.stop() 

################################################################################################33

notebook was run as a job on serverless EMR cluster that it was attached to



# AWS Glue Overview


AWS Glue utilizes Spark
  - is a data integration service
  - can read from sources
  - transform
  - deliver to targets



Ways to Create Glue Jobs:

1. AWS Glue Data Brew (No code / low code)
  like a spreadsheet that utilizes Spark
    - UI based, don't need code to get functionality


2. Glue Studio (SQL Code)
  - drag and drop 
    - sources
    - transformations
    - targets
    - Popular ??


3. Jupyter Notebook (Spark code from scratch)
      Can utilize Amazon Q Developer or Q chatbot to start


Perform ETL operations 
  - Glue ETL


Orchestrate ETL operatins 
  - Glue triggers, blueprints, workflows 


AWS Glue Data Catalog 
  - utilized for Data Lakes and Lakehouses

AWS Glue Crawler
  - crawl over data from data sources and extracts 
     metadata and stores to Glue Data Catalog
        - definition 
        - structure 
        - data types 
        - partitioning information of data


- AWS Glue is serverless 
- Easy to scale the ETL jobs
    - Start with a few Data Procesing Units (DPUs)
    - Scale up as needed


AWS Glue Data Catalog can integrate with other AWS Services:
  - Athena    :  run SQL queries against your data
  - QuickSight:  Analyitics and BI dashboards  
  - SageMaker :  Build, train and deploy ML Model

In prior course, Terraform was used to perform:
  - extract, transform and load of ETL pipeline.
    utilizing Glue_jobs.py


# Glue Visual ETL Demo

  script: 
      - took normalized data from RDS
      - transformed it to star scheam
      - delivered to S3 bucket

console > Glue > Visual ETL > [Visual ETL]  

(blank canvas)

"+" button (add nodes)

tabbed menu:  sources, transforms, targets, popular

- choose Data Source - MySQL 
  name: customers_source

connection name: choose dropdown of rds connection

choose JDBC connection details 
enter table name: customers
choose IAM role for glue job to run (Cloud9-de-c1w2-glue_role)

shows data preview 




"+" node > Transforms > Transform - SQL Query

Enter SQL Query: with dim_customers as ....

Name: dim_customers

SQL Aliases: input:       customers_source 
             SQL aliases: customers

Create Datasource - MySQL for each table being converted to fact or dim


when creating dim_products you can create previous sql sources that were created as input sources


for fact_orders table 
  - select all of the dim tables (Node Parents) that were created previously



choose targets S3
  - setup separate target node for each fact / dim


i.e. dim_customers_target
        - Node Parents 
            dim_customers

   format          : Parquet
   Compression type: Snappy
  [ Browse S3 and choose bucket ]


[Save]  glue_job_demo


On canvas tab, choose "Script" 
  - this displays the python script for the visual ETL  


On canvas tab, choose "Job Details" 
Name: 
IAM Role: 
Type: Spark 
Glue Version: 4.0
Lang: Python 3
Worker type:  G 1X (4vcpu and 16GB RAM)
Requested number of workers:  2
Number of retries: 0 
Job timeout (minutes): 3
[Save]
[RUN]

On canvas tab, choose "Runs" 


# Technical Considerations


Transformation Complexity:            
  Simple trsnformation                    
       Python and SQL approaches have comparable performance:            
          - translated to the same execution plan 
          - executed by the same engine

  Complex trsnformation
      Cannot be implemented in SQL/not in a straightforward way 
          - transpose operation (pivot)
              - DataFrame: df.T
              - SQL: Not supported

          - normalizing and cleaning data
              - Python may requires more code than SQL

Code Reusability / Testability:
  DataFrames: more testable, maintainable, modular/ reusable code.
  SQL : no good notion of reusability for more complex query component 


Team skills / background:
  SQL : writing SQL queries might be simpler and easier than working with DataFrames



When to use Spark DataFrames 

Spark: Distributed Framework: load the entire data into a cluster of nodes
Use Spark: if data doesn't fit entirely into memory or you want to leverage distributed computations

Pandas:  Non-distributed Framework: load the entire data into memory
Use Pands: if data cat fit into memory


Best Practices: 
  - Extract only the data you need from the source
  - Apply transformation inside the source database to reduce the size of the ingested data




# Stream Processing

Streaming Transformations: Prepares data for downstream consumption by converting a stream of 
                           events into another stream 


Enrich a Stream:

IoT events 
[.] [.] [.] [.] [.] --+
                      |    Enriched stream
                      +--> [+] [+] [+] [+]
                      |
[Additional Data ]  --+
[Device Metadata ]



Join two Streams
                              Join
clickstream                 Processor
[.] [.] [.] [.] [.] --+    [----------]
                      |    [  Buffer  ]            Unified user 
                      +--> [          ] -------->  [+] [+] [+] [+]
                      |    [  Buffer  ]            activity view
[.] [.] [.] [.] [.] --+    [----------]
IoT data
 
Windowed Queries 
---------time---------->
[.][.][.]                      [+] [+] [+] 
      [.][.][.]                ^
            [.][.][.]          |
         +------+              |
            |                  |
            +------------------+
                (roll up stats)


            Streaming Platform            Stream Processor     - Distributed stream processing tools 
            Kinesis Data Streams            Apache Spar        - Open-sourcek
                 Kafka                      Apache Flink       - Python or SQL queries




Stream Processing Tool Considerations 
  - use case 
  - latency requirements
  - performance capabilities of the stream processing framework

Spark Streaming: 
  - Microbatch stream processing 
  - batches events up to 2 minutes
  - process each batch in parallel
        i.e. sales metrics

Apack Flink:
  - Each node continuously listens to messages and updates its dependent nodes
  - Delivers processed events at very low latency
  - comes with significant overhead
        i.e. security metrics



LAB: Capture Data Change with Flink and Debezium

MAP LAST HERE 

https://www.coursera.org/learn/data-modeling-transformation-serving/lecture/EKi4c/streaming-processing




.