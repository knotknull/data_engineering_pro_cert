## Data Modeling, Transformation and Serving
## Course 4
## Week 1

# Introduction to Data Modeling for Analytics

Overview

Data Modeling: A data model organizes and standardizes data in a precise structured representation 
              to enable and guide human and machine behavior, inform decision-making and facilitate actions 




Define the structure, relastionshiops and meaning of data.

Structure the data in a way that connects back to the organization
    - Data is understandable and valuable


Good Data Models 
    - Reflect the business goals and logic while incorporating business rules
    - Ensure compliance with operational standards and legal requirements
    - Outline the relationships between business processes
    - Serve as a powerful communication too, creating a "shared language"

ex. what is "an active users"



Poor Data Models 
    - Don't reflect how the busienss operates
    - Create more problems than they solve
    - Provide stakeholders with innaccurate information 
        and create confusion 


Targeted Data Modeling Approach
- Focus on specific business domains 

    Marketing:  Better understand customer behavior and campaign effectiveness
    Finance  :  Analyze spending patterns and identify cost-saving opportunities
    Machine Learning  :  Drive better decision-making and impactful AI models

# Conceptual, Logical and Physical Data Modeling


Conceptual:
    - describes business entities, relationships and attributes
    - business logic and rules
        - Entity - Relationshiop Diagram (ER)  (1 to 1, 1 to Many)

Logical 
    - details about the implementation of the conceptual model
        - column types, primary / foreign keys

Physical 
    - Details about the implementation of the logical model in a specific DBMS
    - Configuration details 
        - Data storage approach
        - Partitioning details
        - Replication details


# Normalization 


Normalization:  A data modeling practice typically applied to relational databases
                to remove the redundancy of data within a database and ensure referential 
                integrity between tables.

Edgar Codd's Objectives of Normalization 

    - To free the collections of relations from undesireable insertion, 
       update and deletion dependencies

    - To reduce the need for restructuring the collection of relations as 
       new types of data are intruduced



    Reduce Data Redundancy: Eliminating duplicate data saves storage space and ensures consistency across the database.
    Improve Data Integrity: Ensuring that each piece of data is stored in only one place reduces the likelihood of data anomalies and maintains the accuracy of the data.
    Enhance Update/Delete Query Performance




SalesOrders 
Denormalized Form 
 - contains redundant data 
 - contains nested data

OrderID         OrderItems              CustomerID      Customer Name   address     OrderDate

 101           [{                             5               Joe Reis    1st. St     1/08/2024
                "sku":1,
                "price":50,
                "quantity":1,
                "name": "Thigamajig",
               },{
                "sku":2,
                "price":50,
                "quantity":1,
                "name": "Whatchmacallit",
               }]

 102           [{                             5               Matt Housley    1st. St     1/08/2024
                "sku":3,
                "price":75,
                "quantity":1,
                "name": "Whoozeewhatzit",
               },{
                "sku":1,
                "price":50,
                "quantity":1,
                "name": "Thigamajig",
               }]
 

First Normal Form (1NF): 
    - Each column must be 
        - unique
        - have a single value
    - Unique Primary Key

  composite key
+---------------+
OrderID   Item Number  sku    price   quantity    name          CustomerID  CustomerName        address     OrderDate
100         1           1       50      1       Thingamajig         5           Joe Reis        1st St.     1/8/2024
100         2           2       25      2       Watchamcallit       5           Joe Reis        1st St.     1/8/2024
101         1           3       75      1       Whoozeewhatzit      7           Matt Housely    2nd Ave.    1/8/2024
101         2           2       25      3       Watchamcallit       7           Matt Housely    2nd Ave.    1/8/2024
---                                                                    +----------------------------------------------+
 |                                                                                           | 
 +------------------------------------------------------------------------------------------+
                                                                         Non-key columns that depend on OrderID 
                                                                    i.e. if you know an OrderID you would know the customer                        
Second Normal Form (2NF):
    - Requirements of 1NF must be met  
    - Partial dependencies should be removed

NOTE: 
Partial Dependency: A subset of non-key columns that depend on some columns in the composite key

Split to two tables: OrderItems and Orders

OrderItems                                                       Orders                                                       
+---------------------------------------------------------+     +----------------------------------------------------+
OrderID   Item Number  sku    price   quantity    name          OrderID CustomerID  CustomerName        address     OrderDate
100         1           1       50      1       Thingamajig       100       5           Joe Reis        1st St.     1/8/2024 
100         2           2       25      2       Watchamcallit     101       7           Matt Housely    2nd Ave.    1/8/2024  
101         1           3       75      1       Whoozeewhatzit      
101         2           2       25      3       Watchamcallit       


These two tables now have a Transitive Dependency 
Transitive Dependency:
  - A non-key column depends on another non-key column 

    OrderItems:
        price and name depend on sku

    Orders:
        CustomerName and address depend on CustomerID 


Third Normal Form (3NF):
    - Requirements of 2NF must be met  
    - Transitive dependencies should be removed


OrderItems                                  Orders                           Customers
+------------------------------------+     +----------------------------+   +------------------------------------+
OrderID   Item Number  sku    quantity      OrderID CustomerID  OrderDate    CustomerID  CustomerName     address
100         1           1       50            100       5        1/8/2024     5           Joe Reis        1st St.  
100         2           2       25            101       7        1/8/2024     7           Matt Housely    2nd Ave. 
101         1           3       75     
101         2           2       25    

                                                                                Items                                  
                                                                               +----------------------------+  
                                                                                sku   price    name
                                                                                1       50      Thingamajig 
                                                                                2       25      Watchamcallit   
                                                                                3       75      Whoozeewhatzit      




# Dimensin Modeling: Star Schema
                                                                         
                                                                [Dim]  [Dim]  [Dim]
    Fact Table     :  business measures                            \     |     /
    Dimension Table:  Contextual Informtion                         \    |    /
                                                                    \   |   / 
                                                            [Dim]----- [Fact] -----[Dim] 
                                                                         |
                                                                         |
                                                                       [Dim]

Fact Table: contains quantitative business measurements that result from a business event or process
       - Each row contins the facts of a particular business event 
       - Data in Fact table is immutable (append-only)
       - Typically narrow and long
            - not alot of columns but alot of rows


Business Event          Facts                          Grain (detail of Fact)                       Dimensions
order a ride share      Trip duration, trip price      - all rides by all customers in one day,     - customers
                        tip paid, trip delays, etc.    - all rides by one customer on one day       - drivers
                                                            - one ride by one customer              - trip locations

Atomic Grain: most detailed level of a business proces


Dimension Table: Provide the reference data, attributes and relational context for the events in the fact table
        - Describe the events' what, who, where and when  
        - Typically wide and short
            - many columns fewer rows 



Conformed Dimension:  Dimension table used with multiple fact tables

                [Dim]  [Dim]  [Dim]                [Dim]  [Dim]  [Dim]
                  \     |     /                      \     |     /
                   \    |    /                        \    |    /
                    \   |   /                          \   |   /
           [Dim]----- [Fact] ----------[Dim]------------ [Fact] -----[Dim] 
                        |                                   |
                        |                                   |
                      [Dim]                               [Dim]

Each DIM   has a PKey
Fact table has a PKey
Fact connected to Dim via FKey

Best practice:  create a surrogate key 
                    - used to combine data from differnt source systems with 
                      natural primary keys that are in different formats

                    - used to decouple the primary key of the star schema from source systems


Analytical Queries:
    - Apply aggregate queries to find the sum, average, maximum of fact
    - Use dim tables to filter or group facts


         [ dim_customers ]                   [ fact_orders ]               [ dim_products ]
         customerNumber (pk)+--+            OrderLineNumber (pk)    +---- productCode (pk)
         customerName          |            OrderNumber     (pk)    |     productName
         contactLastName       +-----------+customerNumber  (fk)    |     productLine
         contactFirstName                   postalCode      (fk)-+  |     productScale
         phone                              productCode     (fk)-|- +     productVendor
         addressLine1                       orderDate            |        productDescription
         addressLine2                       quantityOrdered      |        productLineDescription
                                            priceEac             |
                                            buyPrice             |  
                                            orderAmount          | 
                                            MSRP                 | 
                                                                 | 
                                            [dim_location]       | 
                                            postalCode (PK) +----+
                                            city 
                                            state 
                                            country 


ex. Find the total sales amount for each product line within the usa
SELECT  
  dim_product.productLine, 
  SUM(fact_orders.orderAmount) AS total_sales
FROM fact_orders 
JOIN dim_product ON 
  fact_orders.productCode =       
  dim_products.productCode       
JOIN dim_locations ON 
  fact_orders.postalCode =       
  dim_locations.postalCode
WHERE dim_locations.country = 'USA' 
GROUP by dim_products.productLine         


Star Schema vs 3NF

 - Star Schema organizes data so it's easier for business users to understand, navigate and use
 - Star Schema results in simpler queries with fewer joins



## Data Modeling Techniques 

# Inmon vs kimball Data Modeling Approaches for Data Warehouses


Inmon Data Modeling Approach

  datawarehouse: A subject-oriented, integrated, nonvolatile and time-variant collection of data 
                 in support of management's decisions.

The data warehouse contains granular corporate data.  Data in the data warehouse is able to be used
for many different purposes, including sitting and waiting for future requirements which are unknown today.


             +-------------------- Data Warehouse ----------------+    +----> [Sales: Data Mart]
 [Source1]-+   Major Subject Areas              Subject Details        |       (star schema)            +---------------+
 [Source2]-+>  [Products]  [Orders]           - business keys          |                                |   Reports     | 
 [Source3]-+   [Customers] [Shipments]        - relationships       ---+----> [Marketing Data Mart]     |     &         |
                                              - attributes             |                                |   Analysis    |
                      (Highly normalized (3NF))                        |                                |               |
                                                                       |                                |               |
                                                                       +----> [Purchasing Data Mart]    +---------------+


Kimball Data Modeling Approach:
  Kimball's approach effectively allows you to server data that's structured as star schemas 
  (or similar variants) directly from the data warehouse
      - faster modeling and iteration 
      - more data redundancy and duplications

                [Dim]  [Dim]  [Dim]                [Dim]  [Dim]  [Dim]                [Dim]  [Dim]  [Dim]      +--------------+
                  \     |     /                      \     |     /                      \     |     /          |              |
[Source1]+         \    |    /                        \    |    /                        \    |    /      -->> |  Reports     |
         |          \   |   /                          \   |   /                          \   |   /            |     &        |
[Source1]+> [Dim]----- [Fact] ----------[Dim]------------ [Fact] ----------[Dim]----------- [Fact] -----[Dim]  |  Analyssis   |
         |              |                                   |                                 |                |              |
[Source1]+              |                                   |                                 |           -->> |              |
                      [Dim]                               [Dim]                             [Dim]              +--------------+


Kimball Data Modeling Approach:
  - Quick insights are your highest priority
  - Rapid implementation and iteration

Inmon Data Modeling Approach
  -  Data quality is your highest priority
  -  The analysis requiremetns are not defined


# From Normalized Model to Star Shema






Normalized Data

OrderItems                     Items                                  
+-------------------+         +---------+  
order_id         (pk)           sku    (pk)
item_line_number (pk)           price    
itme_sku         (fk)           name
item_quantity                   brand

Orders                         Stores                                  
+-------------------+         +----------------+   
order_id         (pk)          store_id     (pk)
customer_id      (fk)          store_name
store_id         (fk)          store_city
order_date                     store_zipcode

Customers
+-------------------+
customer_id      (pk)
customer_name     
customer_zipcode     

                                                                               
                                                                               
Understand the needs of the business
1. select the business process   -->> 2. Declare the grain   -->> 3. Identify the dims  -->> 4. Identify the facts


User Needs:
- analyze sales data 
  - which products are sellin in which stores on a given day
  - differences in the sales between the stores
  - which product brands are most popular


Business process:  Company's sales transactions

Atomic Grain:  Individual product item in a sales transaction


Dims:   analysts wants stores, dates, brands 
    [ dim_stores ]    [ dim_items ]   [ dim_date ]

[dim_stores]
store_key (PK)           ## NOTE MD5 is a has function to create a surrogate key
store_id                 SELECT MD5(store_id) as store_key,    
store_name                      store_id, store_name,   store_city,  store_zipcode
store_city               FROM stores;
store_zipcode 



[dim_items]               SELECT MD5(sku) as item_key, sku, price,   grand
item_key(pk)              FROM dim_items;        
sku                       
price 
brand

Date Table 
Date            Year    Quarter     Month   Day-of-wek 
2022-03-01      2022       1          3       Tuesday
2022-03-02      2022       1          3       Wednesday

[dim_date]          ## NOTE: using pgSQL generate_series()  
date_key (pk)       select date_key, 
day_of_week           EXTRACT(DAY     FROM date_key) as day_of_week,
month                 EXTRACT(MONTH   FROM date_key) as month,
quarter               EXTRACT(Quarter FROM date_key) as quarter,
Year                  EXTRACT(year    FROM date_key) as YEAR
                      FROM generate_series ('2020-01-01'::date, '2025-01-01'::date, '1 day'::interval)  as date_key


[fact_order_items]          
fact_order_key (pk)
order_id                SELECT MD5(CONCAT(OrderItems.order_id, OrderItems.item_line_number)) as fact_order_key,
item_line_number              OrderItems.order_id,
store_key                     OrderItems.item_line_number,
item_key                      MD5(OrderItems.store_id) as store_key,
date_key                      MD5(OrderItems.item_sku) as item_key,
item_quantity                 Orders.order_date as date_key, 
item_price                    OrderItems.item_quantity,
                              Items.price as item_price
                        FROM OrderItems
                              JOIN Orders on Orders.order_id = OrderItems.order_id
                              JOIN Items  on Items.sku       = OrderItems.item_sku


Model data with dbt
  - connects to data warehouse
  - transforms and validates your data with the data warehouse
  - generates the sql code behind the scense to transform your data
  - CAN'T join together data from different sources or move transformed data to another target system

AWS Glue 
  - can connect to differnt sources, apply transformations and store processed data somewhere else




# Conversation about dbt with Drew Banin


dbt: applies business logic rules to data to become information (?)

    - setup rules with SQL and python
    - instruct data warehouse (db) to transform (t) in place 


pre-dbt:  sql scripts,  wildwest etc.



post-dbt:  version control code
           community of practice:  analytics engineer, between engineers and business

dbt: "ruby on rails for data"
  there is a "dbt way"

Things to standardize around dbt: 
 - use a sql style code 
 - column names
 - how logic is transformed to: 
    - staging tables 
    - intermediate transformations
    - marts / dimensional models
 - software engineering best practices 
    - modular code
    - test as you go / unit testing
    - code review 
    - CI/CD etc.


# Data Vault


3 layers to Data Vault

                      [Enterprise]                  [ Information ]
[ Staging ]           [Data      ]                  [ Delivery    ]
                      [Warehouse ]

                     Data Vault Model                 
insert only         - hubs, links, satellites         Data delivered to Data Marts

                   separate business objects and 
                   their relationshipsfrom their 
                   descriptive attributes


- Only change the structure in which the data is stored: 
    - Allows you to trace the data back to its source
    - Helps you avoid restructuring the data when business requirements change 


Data Vault Model: Three types of tables 

Hub : Stores a unique list of business keys to represent a core business concept: 
        Customers, products, employees, vendors


Link: Connects two or more hubs. Represents relationship, transaction, event between 
      two or more business concepts 


Satellite: Contains attributes that provide context for hubs and links  
        eg. User will query a hub which will link to a satellite table containing 
        the query's relevant attributes.


                  Data Vault Model 

+-----------------------------------------------------------------------+
  [ Hub ]-------[ Link ]-------[ Hub ]-------[ Link ]-------[ Hub ]
     |             |              |              |             | 
  [Satellite]   [Satellite]    [Satellite]   [Satellite]    [Satellite]


+-----------------------------------------------------------------------+


Data Vault Steps 

OrderItems                     Items                                  
+-------------------+         +---------+  
order_id         (pk)           sku    (pk)
item_line_number (pk)           price    
itme_sku         (fk)           name
item_quantity                   brand

Orders                         Stores                                  
+-------------------+         +----------------+   
order_id         (pk)          store_id     (pk)
customer_id      (fk)          store_name
store_id         (fk)          store_city
order_date                     store_zipcode

Customers
+-------------------+
customer_id      (pk)
customer_name     
customer_zipcode     

                                                                               
Step 1:  Model the Hubs, which contain business keys 

        Identify business keys: 
          - what is the identifiable business element ? 
          - How do users commonly look for data ?
          - business key: 
              - colmn(s) used by the business to identify and locate the data
              - not be a key generated in or tied to particular source system 

      
    [  Customer   ]             [  Order   ]
     customer_hash_key (PK)     order_hash_key (PK)
     customer_id                order_id
     load_date                  load_date
     resource_source            resource_source 

    [  item   ]                 [  Store   ]
     item_hash_key (PK)         store_hash_key (PK)
     sku                        store_id
     load_date                  load_date
     resource_source            resource_source 


  Hubs:  should contain 
    - The business key 
    - The hash key
        - Calculated as a hash of the business key 
        - Used as the Hub primary key
    - The load date    : date on which the business key was first loaded
    - The record source: the source of the businexx key 


Step 2:  Model the links, link table to connect 2 or more hubs



                    (link table)                                   (link table)
[  item   ]---------[Item_Order ]------------[  Order   ]---------[Order_Customer] -------------[  Customer   ]             
item_hash_key (PK)  item_order_hash_key(pk)  order_hash_key (PK)   customer_order_hash_key(pk)  customer_hash_key (PK)     
sku                 item_hash_key            order_id              order_hash_key               customer_id                
load_date           order_hash_key           load_date             customer_hash_key            load_date                  
resource_source     sku                      resource_source       order_id                     resource_source            
  |                 order_id                      |                customer_id
  |                 load_date                (link table)          load_date
  |                 record_source            [Order_Store]         record_source
  |                       |                  order_store_hash_key
 (satellite)        (satellite)              order_hash_key
[item]              [item_order]             store_hash_key 
item_hash_key(pk)                            order_id
load_date(pk)                                store_id
price                                        load_date
name                                         record_source
brand                                             |                     (satelite)
record_source                                [  Store   ]-------------[  Store   ]
                                             store_hash_key (PK)      store_hash_key (PK)
                                             store_id                 load_date(PK)
                                             load_date                store_name
                                             resource_source          store_zipcode
                                                                      record_source

Each table must contain must containt the primary and business keys from its parent hubs,
the load date of a row, and the source for the record.

For each table, the primary key consists of a hash calculated based on the business keys of the parent hubs.

With link tables, you can easily add new relationships or update a current relationship
without having to re engineer the Data Vault


Step 3:  Satellites
satelite table has additional information with hash_key and load date and record source


# One Big Table (OBT)


All data into a single wide table 
  - thousands of columns
  - column can be single or nested  
  - highly denormalized and flexible

example of a wide table 

OrderID         OrderItems              CustomerID      Customer Name   address     OrderDate

 101           [{                             5               Joe Reis    1st. St     1/08/2024
                "sku":1,
                "price":50,
                "quantity":1,
                "name": "Thigamajig",
               },{
                "sku":2,
                "price":50,
                "quantity":1,
                "name": "Whatchmacallit",
               }]

 102           [{                             5               Matt Housley    1st. St     1/08/2024
                "sku":3,
                "price":75,
                "quantity":1,
                "name": "Whoozeewhatzit",
               },{
                "sku":1,
                "price":50,
                "quantity":1,
                "name": "Thigamajig",
               }]
 

- Can have hundreds or more columns
- combines varous data types
- no need for complex joins 
- supports fast analytical queries
- wide table contains ALL the data 

Why OBT becoming popular: 
  - Low cost of cloud storage
  - Nested data allows for flexible schemas

Columnar storage helps optimiase the storage and processing of OBTs
- wide tables are sparse (expensive in an RDBMS) 
- columnar database reads only columns selected in a query and 
   reading nulls is esentially free

Cons:
  - You might lose the business logic in your analytics
  - Your need complex data structures to store nested data

  - Can have poor update and aggregation performance


# Demo: Transforming Data with dbt  pt 1
# Demo: Transforming Data with dbt  pt 2



# Lab: Data Modeling with DBT


source lab-venv/bin/activate

dbt --version



# Initiate the `classicmodels_modeling` project with the `init` command

dbt init classicmodels_modeling




(lab-venv) voclabs:~/environment $ ls -ltR classicmodels_modeling/
-rw-r--r--. 1 ec2-user ec2-user 1280 Oct 11 12:47 dbt_project.yml
-rw-r--r--. 1 ec2-user ec2-user  571 Oct 11 12:41 README.md
drwxr-xr-x. 2 ec2-user ec2-user   22 Oct 11 12:41 analyses
drwxr-xr-x. 2 ec2-user ec2-user   22 Oct 11 12:41 macros
drwxr-xr-x. 3 ec2-user ec2-user   21 Oct 11 12:41 models
drwxr-xr-x. 2 ec2-user ec2-user   22 Oct 11 12:41 seeds
drwxr-xr-x. 2 ec2-user ec2-user   22 Oct 11 12:41 snapshots
drwxr-xr-x. 2 ec2-user ec2-user   22 Oct 11 12:41 tests
classicmodels_modeling/analyses/
classicmodels_modeling/macros/
classicmodels_modeling/models
classicmodels_modeling/models/example/
-rw-r--r--. 1 ec2-user ec2-user 475 Oct 11 12:41 my_first_dbt_model.sql
-rw-r--r--. 1 ec2-user ec2-user 115 Oct 11 12:41 my_second_dbt_model.sql
-rw-r--r--. 1 ec2-user ec2-user 447 Oct 11 12:41 schema.yml
classicmodels_modeling/seeds/
classicmodels_modeling/snapshots/
classicmodels_modeling/tests/




cp ./scripts/packages.yml ./classicmodels_modeling/
cd classicmodels_modeling


## fetch latest versions of tools / libraries specified in packages.yml
dbt deps

scripts/profile.yaml contains connection informatino 

cp ../scripts/profiles.yml ~/.dbt/profiles.yml 

# attempt to connect to profile source
dbt debug


Resources: 
https://www.amazon.com/Building-Data-Warehouse-W-Inmon/dp/0764599445

https://www.amazon.com/Data-Warehouse-Toolkit-Definitive-Dimensional/dp/1118530802

https://www.oreilly.com/library/view/building-a-scalable/9780128026489/

https://vertabelo.com/blog/data-vault-series-data-vault-2-0-modeling-basics/



## Data Modeling, Transformation and Serving
## Course 4
## Week 2
## Data Modeling and Transformation for Machine Learning

# Modeling and Processing Tabular Data for Machine Learning 
# Overview 

Machine Learning Project Lifecycle Framework 


[--- Scoping --]  [------------ Data ------]  [---Modeling/Algo Dev ----]  [------- Deployment ------]
[Define Project]  [define data/] [label and]  [select and ] [Perform err]  [Deploy in ] [Monitor &   ]     
                  [est baseline] [org data ]  [train model] [analysis   ]  [production] [maintain sys]    


# Machine Learning Overview



ML System 1 : Predict customer churn                               Learn            
                                                        ----------->>                 categorical label
[cust        ] [browsing ] [hist sales  ] [customer  ]  [Supervised]   [ML    ] -->> [ Predict churn]
[demographics] [ history ] [product data] [churn data]  [Learning  ]   [System]      [ / not churn  ]
+---------------------------------------+ +----------+
                  features                    label     (Classification)

Supervised Learning:  Labels subervised what needs to be learned from the features
Classification:  value given is a classification of customer who will or will not churn


ML System 2 : Predict sales for the next new year holiday    Learn
                                                        ----------->>                 numerical label
[cust        ] [browsing ] [hist sales  ] [historical]  [Supervised]   [ML    ] -->> [ Predict churn]
[demographics] [ history ] [product data] [sales data]  [Learning  ]   [System]      [ / not churn  ]
+---------------------------------------+ +----------+   
                  features                    label     (Regression)
                  
Regression:  value given is a number




ML System 3 : segment customers into groups based on similar purchasing behaviors
                                                            Learn
                                                        ------------->> 
[cust        ] [browsing ] [hist sales  ] [  NO   ]  [Unsupervised]   [ML    ] -->> [ Predict churn]
[demographics] [ history ] [product data] [LABELS ]  [  Learning  ]   [System]      [ / not churn  ]
+---------------------------------------+ 
                  features only                   
                  
Unsupervised Learning:   No lables, algo has to figure out grouping based on similarities





[--- Scoping --]  [------------ Data ------]  [---Modeling/Algo Dev ----]  [------- Deployment ------]
[Define Project]  [define data/] [label and]  [select and ] [Perform err]  [Deploy in ] [Monitor &   ]     
                  [est baseline] [org data ]  [train model] [analysis   ]  [production] [maintain sys]    

Scoping:
  - ML Engineer / Data Scientist defines project and decide which business problems to apply ML to 

Data: 
  - ML Eng will work with Data Eng to determine which features and labels are needed to train ML algo

Algo Development:
  - Select and train model and perform err analaysis
  - ML Eng takes data provided and uses it to train ML algo
    - split data into training and test set
    - use training set to train several ML algos with different configs

    - Classical ML algorithms:
      - Linear regression
      - Logistic regression
      - Decision trees
      - Random  foreset / boosted trees 

    - Classical ML algos expect tabluar data
      - as data increases, Classical ML algos will reach a "plateau"

    - Complex ML algorithms:
      - Deep neural networks
        - can work with tabular data
      - Convolutional neural networks
        - can work with image data
      - Recurrent neural networks
        - can work with timeseries data
      - Large language models
        - can work with text data

  - select the best model through cross-validation 
    - Evaluate the model performance using the test set

  - May go back to data engineer to:
    - fix something in collected data
    - add more features
    - collect more data
    - go back to Data step

Deployment:
  - ML Eng / Data Scientists check to make sure the system's performance is good an reliable
  - Write software to put system in to production
  - Monitor the system, track data and maintain system 
  - Data Engineer may:
    - prepare and serve the data that is needed for the deployed model
    - serve an updated set of data to re-train and update the model







.