## Storage Queries
## Course 3
## Week 1






## Data Storage Deep Dive

# Overview



Storage Solution Considerations
    - Data type         - Data size
    - Data format       - Access and update pattern



Storge Hierarchy 
---------------------

[Storage Abstraction  ]
[Cloud Datawarehouse  ]
[Data Lake/Lakehouse  ]
        ^
        |

[Storage Systems     ]
[RDBMS, Graph, Vector]
[Object Storage      ]
        ^
        |
[Raw Ingrediants      ]
[Magnetic, SSD, RAM   ]
[Network, compression ]
[CPU, Caching         ]

Management System:
Organizes data in the raw components and allows 
you to interact with stored data


OLTP: Online Transactional Processing Systems
  Focus on performing read / write queries with low latency


OLAP: Online Analytical Processing Systems
  Focus on applying analytical activities on data
  (aggregation, summarization)


Trade-off between storage costs and performance


# Storage Raw Ingredients - Physical Components of Data Storage

Raw Storage Ingredients

Magnetic Disks / HDDs / Magnetic platters with disk heads
    track / sector give address
    write: encode binary data by change magnetic field
    read:  converts magnetic field into binary data


Solid State Drives / SSD / Flash memory
    electrical charges in flash memory cells
      - charged   cell = 1 bit
      - uncharged cell = 1 bit
    read / writes faster than HDD


Performance Comparison
                       |   Magnetic Disk   |       SSD            |      RAM         |   CPU Cache      |
-----------------------|-------------------|----------------------|------------------|------------------|
Latency                | 4ms               |    0.1ms             | 0.1 microseconds |    1 nanosecond  |
IOPS                   | hundreds          |    10s of thousounds |  Millions        |                  |
Data Transfer speed ** | Up to 300 MB/s    |    4 GB/s            |  100 GB/s        |    1 TB/s        |
Cost                   |  $0.03 = 0.06/GB  |   $0.08 - 0.10/GB    |  > $3/GB         |                  |

** # MBs read/ write form disk to memory 

Improving Performance
Distributed Storage 
    - distribute daa across many HDD
    - read simulatneously 
    - transfer speed limiited by network


Partitioning 
    - slice SSDs into partitions
    - multiple storage controllers running in parallel




Latency (data access time)= Seek time + Rotational latency
Commercial magnetic disk drive: 7200 RPM (rev / min)


# Storage Raw Ingredients - Processes Required for Data Storage


Networking / CPU  
Enhance:
  - read and write performance
  - data durability
  - data availability

Serialization

                Serilize
[ In-memory ] -----------------> [Disk ]
[  Format   ]      Transform     [Format]
              <----------------
                 DeSerialization
Data structures
optimized for 
CPU Usage



Row-based Serialization
[first record][ second record][ third record]
[object 1 ] [ object  2] ....

Column-based Serialization
[ bytes represent 1st col ] [ bytes represent 2nd col ] [ bytes represent 3rd col ] 
[ bytes represent 1st key ] [ bytes represent 2nd key ] [ bytes represent 3rd key ] 





Serialization Formats

Human-readable Textual Formats
    - csv:  row-based format
            prone to error, no defined schema
            adding new rows or columns requires manual handling

    - xml   viewed as legacy format
            slow to serialize / deserialize

    - json  used for plain-text object serialiazation
            viewed as new standard for data echanage of APIs

Binaryi Formats

    - parquet  Column-based format
               for efficient storage and big data proces

    - avro     row-based format
               Uses a schema to define its data structre 
               Supports schema evolution



Compression: a way to reduce the number of bits needed to reqpresent the data
    - remove redundancy and repetition of data to achieve 
       more efficeint encoding 

    - encode characters based on their frequency
    - requces disk space
    - improves query performance
    - Reduces the I/O time needed to load data


Compression Algorithms: 

lossless compression: decompressing file returns exact copy
lossy compression   : decompressions recovers something that sounds/looks like original. 
                      Not an exact copy.  (i.e. audio / video files) 


New compression algos prioritize speed and CPU efficiency over compression ratio:
    - Snappy
    - Zstandard
    - LZFSE
    - LZ4

General purpose compression to compress row and column stores:
    -LZO, LZ4, Snappy, Brotli, Oracle OZIP, Zstd
    

Some algos are specific to column stores: consecutive values from same column are stored together on disk. 
Compression improves database performance:
    - database processes queries faster 
        - less data to read from disk to memory and memory to CPU

Column-based compresion algos:
Run-lengh Encodding: compresses a run of the same values in a column 
      - each run is replaced with a tuple that has 3 elements 
        (value, start position, runLength)
      - each element is represented with a fixed number of bits

eg.     
        34 34 34 63 32 32 32 67 67 67

With RLE, you'll get this result: 

        (34, 1, 3), (63, 4, 1,), (32, 5, 3), (67, 8, 3)

Bit-Vector Encoding (or bitmap encoding): each distinct value is associated with a sequence of bits 
          where the length of the sequence is the same as the number of records / rows in the column  
           a '1' in the i-th position means that the ditinct value appears in the i-th row

34 34 34 63 32 32 32 67 67 67

With this algorithm, this data would be represented by four sequences of ten bits (i.e. the number of rows):

    bit-string for value 34: 1110000000

    bit-string for value 63: 0001000000

    bit-string for value 32: 0000111000

    bit-string for value 67: 0000000111

Bit-vecotr encoding is the most useful when columns have a limited number of unique values 
( i.e. US Staes, store id, product id)


Compression Resources: 
https://stratos.seas.harvard.edu/files/stratos/files/columnstoresfntdbs.pdf
https://docs.aws.amazon.com/redshift/latest/dg/c_Compression_encodings.html
https://martin.kleppmann.com/2012/12/05/schema-evolution-in-avro-protocol-buffers-thrift.html
https://airbyte.com/data-engineering-resources/parquet-vs-avro



Parquet is column based
Avro    is row based


Parquet Benefits:
    columnar storage
    various compression schemes: dictionary, run-length encoding 
    predicate pushdown:  query engines can skip reading irrelevant data blocks
    schema evolution  :  file format enables flexibl changes to data schemas
                            add or modify columns without breaking compatibility
    compatability     :  accomodates big data file formats 

Parquet Use Cases:
    - Big Data analytics
    - Data Warehousing
    - ETL Pipelines
        - Parquet can be used as an intermediate storage format in ETL pipelines
            can transform and process more efficiently before loading into data warehouse
    - Log Analytics: Parquet is well-suited for analysing log files and event data
            faster log analysis
    - Data Archiving: Good choice for long-term data archiving.



Avro Benefits:
    binary row-oriented format
    efficient serialize / deserialize
    Suitable for data interchange
    Schema-based Serialization
        - schema defines data structure 
        - schema and serialized data are stored in Avro data file
    Schema Evolution
        forward / backwared compatability
    Dynamic Typing 
        data can be searialized and deserialized without generating / sharing specific code
    Interoperability 
        data format supports several programming languages

Avro Use Cases:
    Data Interchange   : between apps, services, languages
    Streaming Analytics: used in streaming data pipelines
        efficient Serialization and compatability with streaming platform   
    Messaging Systems  :  message queues, etc.
    Data Replication   :  replicate data from one system to another
    Big Data Processing:  used with big data frameworks, Kafka.  
        data ingested, processed and analyzed efficiently across different stages of data pipeline


Clod Storage Options: Block, Object and File Storage

File Storage: organizes files into a directory tree
    Each directory contains metadata about its files / folders:
        - Name
        - Owner
        - Last Modified
        - Permissions
        - pointer to the actual entity

    Cloud FIle Storage Service 
        - Amazon Elastic File System (EFS)
        - provides access to shared files over a network 
        - Networking, scaling and configuration handled by clour vendor

    File storage sits on top of Block Storage


File Storage: divides files into small, fixed-size blocks of data and stores on disk
    - each block has a unique identifier
    - efficiently retrieve and modify data in individual blocks
    - can distribute blocks across multiple storage disks
        - higher scalability
        - stronger data durability

    i.e. think direct db writes or VM storage


Block Storage Lookup Table 
    File Piece     Block Identifier
  1st Piece             1232  
  2nd Piece             1234
  3rd Piece             1236
  4th Piece             1238


when ask for a file, all blocks are gathered and merged together
    can update a specific block by identifer and don't have to write entire file


Block Storage Use Cases:
    - Ideal for freuent access and modification
    - Enables OLTP systems to perform small and frequent read and 
      write operations with low latency
    - Provides persistent storage for virtual machines

Default storage for EC2:
    Amazon Elastic Block Storage (EBS)
    1. SSD for latency-sensitive workloads
    2. Magnetic disks to store infrequently-accessed data

NOTE: Block storage caps out at around several TB because it is attached to compute


Object Storage decouples data storage layer from the compute layer
    - can scale to Petabytes



Object Storage
    - Stores immutable files as data objects in a flat structure
    - organized into top-level logical containers (S3 Bucker)
    - each object assigned unique identifier (key)

s3://oreilly-data-engineering-book/data-example.json
|                                | |               |
+--------------------------------+ +---------------+
                |                         |
           
(must be unique across AWS)

    - once data is writtent object becomes immutable
    - have to re-write the entire object if file is updated
    - can scale horizontally and support performant parallel operations
        - each node holds shards of objects


Object Storage Use Cases:

        Ideal for                                       Not Ideal for
- Storage layer of cloud                            - Not good for supporting transactional workloads 
  data warehouses and data lakes

- Storing data needed in OLAP systems 
        (read heavy analytics)

- Machine learning pipelines
    - raw text
    - images
    - Videos
    - Audio


Cloud Storage Options 

    File Storage                    Block Storage               Object Storage
-------------------------       ------------------------    -------------------------   
-supports data sharing          - supports transaction      - Supports analytical queries on 
                                   workloads                    massive datasets

-Easy to manage with low        - Allows frequent read      - Offers high scalability and
 with low performance and         and write operations        parallel data processing
 scalabilitya requirements        with low latency


Storage Tiers: Hot, Warm, Cold Data

                         Hot Storage             Warm Storage               Cold Storage
                        -------------           --------------             -------------- 
Access frequency        Very frequent           Less Frequent               Infrequently

Example                 Product                 Reqular reports             Archive
                        Recommendation           & Analsysis

Storage Medium          SSD & Memory            Magnetic Disk or            Low Cost magnetic disks 
                                                hybrid storage systems

Storage Cost            High                        Medium                      Low

Retrieval  Cost         Low                         Medium                      How



AWS Storage Tiers
                                Amazon S3
                              Access Frequency

        Hot Storage              Warm Storage          Cold Storage
        -------------           --------------         -------------- 
    S3 Express One zone         S3 Standard IA         S3 Glacier Flexible Retrieval
    S3 Standard                 S3 One Zone-IA         S3 Glacier Deep Archive
                                                       S3 Glacier Instant Retrieval 
** IA: Infrequently Accessed



# Distributed Storage Systems

How Distributed Storage System Works

    - distribute data across multiple servers called nodes
        connected by network

    - groups of nodes make up a cluster

    - each node contains storage mediums
        - HDD / SSD

    - each node has processing abilities 
        - data management
        - replication / access control
Total Capacity is capacity of all nodes


 [------] [------] [------]
 [......] [......] [......] 
 [------] [------] [------]
    |        |        |
    +--------+--------+
             |
        Nodes / Groups of Nodes = cluster

Can scale horizontally by adding additional nodes to cluster

Single Machine Storage Architecture 
    - can only scale vertical
    - can only upgrade storage capacity of a single server

Multiple Nodes / Clusters: 
    -  Higher fault tolerance and data durability
    -  High Availability
    -  Processing many reads and write 
        operations in parallel
    -  Fast data access 
        - serve request from nearest replica


Distributed Storage Architecture utilized by 
    - Object Storage
    - Cloud Data Warehouse
    - Hadoop HDFS
    - Spark


Methods for Distributing Data

Replication:   Same copy across many nodes 
                High availability and performance                


Partitioning:   splits big dataset into smaller subsets 
(sharding)          i.e. partitions / shards

Takes time for updates to complete:
    - wait for update to complete
    - access "sort of" recent data


Distributed Storage Considerations: CAP Therom

CAP Therom: Any distributed  system can only guarantee 2 of 3 properties
    - Consistency
    - Availability
    - Partition Tolerance

Strong Consistency : every read reflects the latest write operation

Availability       :  Every request will recieve a response

Partition Tolerance:  System continues to function when network experiences 
                       distruptions or failures isolate some nodes from otheres
                        s(split brain)


No Distributed System is immune from network failures or disruptions  
    Network Paritioning has to be tolerated


Hence, have to choose between Availability or Consistence if Partition Tolerance is mandatory
    i.e. only 2 of 3 guaranteed 


    ACID (RDBMS) vs. BASE

BASE:  Baseically Available:  consistent data is available most of the time
       Soft-state          :  uncertain if transaction is committed or uncommitted   
       Eventual Consistency:  at some point, reading data will return consistent values   


# Database Partitioning / Sharding Methods

Common Sharding Methods:

Range-based sharding:
    - method splits rows based on a range of values
i.e. split by the first letter of  customer name

Name            Shard Key
A to I              A
J to S              B
T to Z              C
 

NOTE: can lead to unbalanced shards


Hashed sharding:
    - method uses a mathematical fomula called a hash function to determine how to partition data.
        - get a hash of some data attributes and use that as shard key

Geo sharding:
    - method partitions data by geographic location
        store cutomer informatin in a node that is physically in that location 


Partitioning / sharding resources: 
https://aws.amazon.com/what-is/database-sharding/
https://www.amazon.com/Designing-Data-Intensive-Applications-Reliable-Maintainable/dp/1449373321/ref=sr_1_1?adgrpid=1344703291324157&dib=eyJ2IjoiMSJ9.i1bUGZK7N-KyWM2sQR7-B8KYS_yn_vgEDIPgCZKZEqrD3_kYv1WLMRNg2a_cyMTZkenScKZLD1xQT6PoxGtZjpfYLwagMBcOcvqwyg12Ux6vvPPHgXX1vMZoOg1vTM_pc7M5GoJOYAWtL-UQU8rix049vlX-qOUnpYLTJ2MrssfiHjzXSj62mtpldPZ9F8sSVwb2QyjkabDuQFUBKt8wljiPffjJIMY5B8rR7JfDvO8.HDmJ7Lu7-7fnydSo4DSG8hxechXwbUNz0baNI01HWH0&dib_tag=se&hvadid=84044027549737&hvbmt=be&hvdev=c&hvlocphy=44152&hvnetw=o&hvqmt=e&hvtargid=kwd-84044312865379%3Aloc-190&hydadcr=16438_10463512&keywords=designing+data+intensive+applications&qid=1713574421&sr=8-1





# Comparing Cloud Storage Options

Object Storage
File   Storage
Block  Storage
Memory

Lab:

Object storage contains immutable objects of various sizes and types. 
Unlike files on a local disk, objects cannot be modified in place.

Between other features of object storage systems you can find:

    Durability and Redundancy: Object storage systems typically employ data redundancy and 
      replication techniques to ensure data durability and availability. Objects are often 
      replicated across multiple storage nodes or data centers to protect against hardware failures, 
      data corruption, and other types of data loss.

    Access Methods: Objects are accessed using standardized application programming interfaces (APIs) 
      such as Amazon S3 (Simple Storage Service) API. These APIs provide a set of operations for storing, 
      retrieving, updating, and deleting objects, as well as managing object metadata and access control.

    Data Access Patterns: Object storage systems are well-suited for storing unstructured data and large 
      volumes of data, such as multimedia files, backups, archives, log files, and data lakes. They are 
      commonly used in cloud storage services, content delivery networks (CDNs), and big data analytics platforms.



Lab:

following is an example of the timed_lru_cache decorator from cache-pandas package 
It allows you to store in memory the resulting data from from a function.  Compare the 
time of the first read and then after it is stored in cache:


@timed_lru_cache(seconds=100, maxsize=None)
def read_csv_to_memory(path: str) -> pd.DataFrame:
    """Read CSV function with a cache decorator."""
    return pd.read_csv(path)

start = timeit.default_timer()
df = read_csv_to_memory('data/employees.csv')
end = timeit.default_timer()
print(f'Elapsed time: {end - start} seconds')



# first run
Elapsed time: 2.7644586910000726 seconds

# second run
Elapsed time: 5.668999983754475e-05 seconds


# thrid run, after waiting the 100 seconds on the cache
Elapsed time: 2.157280639000419 seconds


htop:  fancy top 
   shows CPU, Mem, Swp, processes


# Storage in Databases 

# How Databases Store Data

Database Management System
[    Transport System    ]
[    Query Processor     ]
[    Execution Engine    ]
[    Storage Engine      ]


Storage Engine:
    - serialization
    - arrangement of data on disk
    - indexing

    Modern Storage Engines
    - Support the performance characteristics of SSDs
    - Handle modern data types and structures
    - Offer robust column storage support

Index: data structure that helps you efficiently locate data

    Index table
    Country  | Row Address
    Brazil   |     ###
    Canada   |     ###
    Mexico   |     ###
    Usa      |     ###



Scanning all rows O(n) vs. Binary search on rows O(log n)


In-Memory Storage Systems 

    - Excellent transfer speed and low latency
    - Volatile
    - Used to present data for ultra-fast retrieval
        - Caching Applications
        - Real-time bidding
        - Gaming leaderboards

Memcached
    - key-value store to cache database query results or API calls
    - used when it's acceptable for data to be lost

Redis
    - key-value store that supports more cmplex data types
    - persistent functionality: snap-shotting, journaling
    - supports high-performance applications that can tolerate minor data loss


# Row vs Column Storage


Row Oriented Storage

Order id        Price       SKU         Quantity    Customer Id
--------       --------   -----------   ----------  ------------
  1             40          45865           10          67t
  2             23          90234           14          56t
  3             45          12558           12          87q
  4             50          45682           13          98q

Stores data row by row in physical storage

  1  | 40 | 45865 | 10 | 67t |   2  | 23 | 90234 | 14 | 56t |   3  | 45 | 12558 | 12 | 87q |   4  | 50 | 45862 | 13 | 98q | 
+----------------------------+                                                             +------------------------------+
             |                                                                                              |
        first record                                                                                    Nth Record

Row storage is ideal for OLTP. 
    - Performs read and write operations with low latency


What if you wanted to do analytics on price: 

select sum(price) from my_table

1 million rows
30 columns  
100 bytes per entry == 3 GB  of data to transfer to RAM
Data Transfer speed: 200 MB /s
Total transfer time: 3000 MB / 200 MB/s = 15 sec

if 1 billion rows == 3000 GB 

Total transfer time: 3000000 MB / 200 MB/s =  > 4 hrs


Column Oriented Storage  (Column Store)

Order id        Price       SKU         Quantity    Customer Id
--------       --------   -----------   ----------  ------------
  1             40          45865           10          67t
  2             23          90234           14          56t
  3             45          12558           12          87q
  4             50          45682           13          98q



Stores data column by column 
Pysical Storage


  1  | 2 | 3 | 4 | 40 |  23  | 45 | 50 | 45865 | 90234 | 12558  | 45682 | 10 | 14 | 12 | 14  | 67t | 56t | 87q | 98q | 
+----------------+                                                                           +-----------------------+
         |                                                                                            |
 all data in                                                                                      all data in 
 first column                                                                                      Nth column


select sum(price) from my_table

1 billion rows
1 columns  
100 bytes per entry == 100 GB  of data to transfer to RAM
Data Transfer speed: 200 MB /s
Total transfer time: 100000 MB / 200 MB/s = 8.33 minutes



Column storage is ideal for OLAP. 
BUT, column storage is terrable for transaction workloads


# The Parquet Format 

Parquet and ORC (optimized row column) combine row and column approaches
    - paritioning rows into groups where each row group is stored in a column-wise format


Product Sku    Price       Quantity    Customer Id
-----------   --------   -----------   ------------
  45865         40          10          67t
  90234         23          14          56t
  12558         45          12          87q
  45682         50          13          98q

Hybrid approach


 045865 | 902348 | 40 |  23  | 1 | 4 | 67t | 56t |  
 125893 | 456829 | 45 |  50  | 2 | 3 | 87q | 98q |  


Parquet file: data is horizontally partitioned into row groups, each row group has 
              a default size of 125 MB.  A row group consists of a column chunk for  
              each column in the data set.  Each column chunk is divided into pages, 
              where each page contains the encoded values for that column chunk, 
              metadata like min, max and count of values, along with other data 
              (repetition and definition levels) used to reconsturct the nested 
              structure of the data. 

NOTE: Parquet can be used to store tabular dta as well as nested data (i.e. JSON)
      An ADVANTAGE is that Parquet is portable and can interoperate with external tools easily.


Resources: 
https://www.youtube.com/watch?v=1j8SdS7s_NY&t=643s
https://www.upsolver.com/blog/the-file-format-fundamentals-of-big-data
https://parquet.apache.org/docs/file-format/

# Wide-Column Databases

Key / Value
[ Key ] --> [ Value ]

Wide Column structures the value part of a key-value pair into more key-value pairs
    - Like two-dimensional key-value maps

                    +---------------------------------------------------+
                    |                                                   |
[ Row Key 1] --->>  | [ Column Key 1]  [ Column Key 2]  [ Column Key 3] |
                    |       |                 |               |         |
                    |       V                 V               V         |
                    | [   Value 1   ]  [   Value 2   ]  [   Value 3   ] |  
                    |                                                   |
                    +---------------------------------------------------+

                    +---------------------------------------------------+
                    |                                                   |
[ Row Key 2] --->>  | [ Column Key 1]  [ Column Key 2]  [ Column Key 3] |
                    |       |                 |               |         |
                    |       V                 V               V         |
                    | [   Value 1   ]  [   Value 2   ]  [   Value 3   ] |  
                    |                                                   |
                    +---------------------------------------------------+


Each row describes a single entity that is identified by its row key.
Within a row, data that's related is modeled into column families that 
contain column names.  Actual data is stored in cells which are uniquely 
identified by the combination of the row key, column family and column name.



                    +---------------------------------------------------+ +------------------------------------+
                    |     [ Column Family: Customer Information ]       | | [ Column Family: Purchase Info ]   | 
[ Row Key 1] --->>  | [ Column Key 1]  [ Column Key 2]  [ Column Key 3] | | [ Column Key 1]  [ Column Key 2]   | 
   12345            |       |                 |               |         | |       |                 |          | 
                    |       V                 V               V         | |       V                 V          | 
                    | [   Value 1   ]  [   Value 2   ]  [   Value 3   ] | | [   Value 1   ]  [   Value 2   ]   |   
                    |                                                   | |                                    | 
                    +---------------------------------------------------+ +------------------------------------+

hence customer_id 12345 points  to two column families: customer information ( with three columns) 
and purchase information (with two columns)

NOTE: each cell is versioned and uniquely identified by its version number, typically a timestamp.

Wide column database doesn't envorce strict table schema, adding columns becomes very flexible. 
Column is only written if there's data for it.

column families are usually stored separately on disk i.e. customer separate from purchase info.
Data in a column family is stored in a row-oriented fashion.  


Wide column databases: 

https://databass.dev/links/118
https://databass.dev/links/117
https://cassandra.apache.org/_/index.html
https://aws.amazon.com/keyspaces/


https://scaleyourapp.com/wide-column-and-column-oriented-databases/
https://www.scylladb.com/glossary/wide-column-database/
https://www.amazon.com/Cassandra-Definitive-Guide-Distributed-Scale/dp/1491933666
http://0b4af6cdc2f0c5998459-c0245c5c937c5dedcca3f1764ecc9b2f.r43.cf2.rackcdn.com/9353-login1210_khurana.pdf
https://aws.amazon.com/compare/the-difference-between-cassandra-and-hbase/



Graph Databases

    Store data with nodes and edges 
    Node: represents data items
            entities i.e. people, products, locations 

    Edge: represents connection / relatinoship between the data items




    [node] ---edge---> [node] ---edge---> [node]
                        /\
                      /    \
                    /       \
                [node]     [node] 


 

Use Cases 
    - recommending products 
    - modeling social networks
    - representing network and IT operations
    - simulating supply chains logis
    - tracing data lineage

Use Case: Fraud Detection 
    - can flag users using same credit cars from different ip addresses


Use Case: Knowledge Graph
    - connect diverse data from disparate sources
    - chatbot utilizes RAG (Retrieval Augmented Generation) against purchase history (user, product, shipment)
    - gets fresh data to LLM to improve chatbot answers    


Graph Databases:
    neo4j
    ArangoDB
    Amazon Neptune    

Examples of Graph Query Languge
    Cypher
    Gremlin
    SparQL


# Vector Databases

    efficiently query data based on "semantic simularities" 
        - i.e. similarity search

Vector data:  numerical values arranged in an array

Vecotr embeddings:  
    capture semantic meaning on an item, like a text document or image

    [ Original  ] ---> [  ML  ] ---> [  Vector  ] ---> [  Vector  ]
    [  Data     ]      [Model ]      [embedding ]      [ database ]

    - can convert an entire database of docs or text into embeddings 
    - Embeddings help you more efficiently find and retireve similar items

    ex. finding similar text
            - compute embeddings for the query item
            - database returns similar vectors (based on closeness)


Vectors that are similar to one antother SEMANTICALLY will be closer to one another 
  in high dimensional vector space the are represented in.

        i.e. if vectors are close than items are similar

Distance Metric
    Vector database uses a distance metric to find similar vectors
        - euclidean distance (straigh line distance between points)
        - cosine distance  (angle)
        - Manhattan distance along their axis

Similarity Search:  Popular Algorithm 
    Want to find the K most similar items to a given item

    K-nearest neighbors (KNN)
        - will do exhaustive search over all the vector embeddings of all items to 
          compute the distance between those items and the given one 

        - becomes inefficient when the data size increases
        - suffers from the curse of dimensionality

    Approximate Nearest Neighbors (ANN)
        - find a good guess for the nearest neighbors
        - more efficient than KNN
            - with less accurate result
        - Vector databases are built to support ANN algorithms

        - When storing vector embeddings/ Vector database applies ANN algorithm to represent data for search
        - When querying Vector datbase to do a similarity search based on an item 
                - the dabase uses specific ANN algorith to traverse data structure to 
                  return items that are approximately close


# ANN Algorithm:  Hierarchical Navigable Small World (HNSW)

        - HNSW is a popular ANN algorithm that underpins many vector databases
            - hierarchical graph representation of embeddings   
            - each layer consists of a graph representation of data 
                - top layer contains more of the longest links
                - botom layer contains more of the shortest links
                - as you move up from lowers layers to highest layer, number of nodes decreases

        - for a query, start at bottom and go to top layer, 
            starts entry node at top layer and navigates through the graph of that layer 
                each time choosing the neighboring node that is closest to the entry point. 
                stops at node the doesn't have any neighboring nodes closer to the query point. 
                    algorithm shifts to the node in the next lower layer and starts search again.
                    repeat until bottom layer. 

Resources:
https://www.pinecone.io/learn/what-is-similarity-search/

https://hyper-space.hashnode.dev/what-is-similarity-search-definition-and-use-cases
https://www.pinecone.io/learn/vector-database/
https://www.pinecone.io/learn/series/faiss/hnsw/
https://arxiv.org/pdf/1603.09320.pdf
https://docs.aws.amazon.com/neptune-analytics/latest/userguide/algorithms.html

https://www.pinecone.io/
https://www.deeplearning.ai/short-courses/building-applications-vector-databases/
https://weaviate.io/
https://www.deeplearning.ai/short-courses/vector-databases-embeddings-applications/



# Neo4j Graph DB and Cypher Language part 1


Property Graph Model 

                        [ Supplier ]
                            |
                            |
                (supplies)  |
                            v           PART_OF
                        [ Product  ] -----------> [ Category ]
                            ^
                (orders)    |
                            |
                            |
  [ Customer ]  ------->[  Order  ]
                (purchased)




Node Label: type of node  (i.e. Category)

edges known as relationships and relationship has a type
    - earch relationship type has a source node and a target node

    supplies relationship has Supplier as source node and Product as target node

    can associate more than one property to each node

i.e. Customer Properties        Product Properties      Supplier Properties     Order Properties    Category Properties 
        - address               - productID             - address               - freight           - categoryName
        - city                  - productName           - city                  - orderDate
        - companyName           - unitPrice             - contactName           - orderID
        - contactName           - unitsInStock          - fax                   - requiredDate
        - contactTitle          - unitesOnOrder         - region                - shipAddress
        - country               - productID             - supplierID
        - customerID- productID                         - postalCode


Can specify properites for a relationship

    orders properties: 
        - discount 
        - quantity 
        - unitPrice 


Creating a Graph Database:

    instructions describing graph model with data (csv files)
        - utlize Cypher to create database and query it


# Neo4j Graph DB and Cypher Language part 2

MATCH statement to retrieve information from a graph (neo4j)
i.e. select statement 

    match pattern return result

ex. retrieve all nodes

use parenthesis to denote a node 

# this returns all nodes 
match(n) return n

# NOTE: n is a variable which represents node

# get total number of nodes:
# 
match (n) return count(n)

# explore node labels in the graph
# only bring back unique labels
# 
match(n) return distinct labels(n)

i.e.  labels(n)
    ["Supplier"]
    ["Category"]
    ["Product"]
    ["Order"]
    ["Customer"]

# Remember a node label is a type of node


# count specific type of nodes
# 
match(n:Order) return count(n)
> 99

# explore properties of each order node
#   - bring back just first node
match (n:Order) return Properties(n) limit 1

{
   "shipCity": "Reims", 
   "orderID": "10248",
   ..... 
   "requiredDate": "00:00.0",
   "shipPostalCode": "51100",
}

NOTE: use [] to denote a relationship, reference variable inside []
      relationships happen between two nodes 
      
      path (source_node)-[r]->(target_node)

# count all directed paths in graph

Match()-[r]->() return count(r)

NOTE: looking for any relationship r, that goes 
      from any source node  to any target node.
      Notice we are returning r, for relationship, 
      and note n for node.

count(r)
518

# get the distinct type of relationships
Match()-[r]->() return distinct type(r)

type(r)
"SUPPLIES"
"PART_OF"
"PURCHASED"
"ORDERS"

# Specify the type of relationship

Match()-[r:ORDERS]->() return AVG(r.quantity * r.unitPrice) as average_price

NOTE: take the quanity * unitPrice and averaging across all ORDERS relationships

average_price
502.464


# Get average price for all orders grouped by product category

Match()-[r:ORDERS]->()-[part:PART_OF]->(c:Category) return c.categoryName, AVG(r.quantity * r.unitPrice) as average_price

NOTE: All Products that are PART_OF Category nodes
      essential this is a relational join

c.categoryName          average_price
"Beverages"             439.2
"Condiments"            553.40
....
"Produce"               645.65
"Seafood"               553434.82 


# Filter results with WHERE statement:
# Retrieve product name and product unit price of all products that belong to categoy "Meat/Poultry"

match(p:Product)-[:PART_OF](c:Category)
where c.categoryName="Meat/Poultry"
return p.productName, p.unitPrice

NOTE: specify path where product node is PART_OF category node
       use where to filter

NOTE: Don't have specify node property in where statment 
    You specify property inside the node parenthesis
     i.e. use curly brackes to specifty category name you want to filter by

match(p:Product)-[:PART_OF]->(c:Category{categoryName:"Meat/Poultry"})
return p.productName, p.unitPrice

IS EQUIVALENT TO:  

match(p:Product)-[:PART_OF](c:Category)
where c.categoryName="Meat/Poultry"
return p.productName, p.unitPrice


NOTE: in the above you are actually applying the where close to the property of the Category node 
      that you are identifying 





# Retrieve the product name of all products orderd by the customer QUEDE
# 


my thoughts: 
        going to need (c:Customer{customerID:"QUEDE})-[:PURCHASED]->()-[:ORDERS]->(p:Product) return distinct p.productName 

solution:
        Match (c1:Customer{customerID:"QUEDE})-[:PURCHASED]->()-[:ORDERS]->(p:Product) return p.productName 


# NOTE: chainging from customer to Product via order but order isn't specified only the ORDERS 
        relationship is specifed:
                 ..-[:PURCHASED]-()-[:ORDERS]->(p:Product)

                                 ^-- is empty parenthesis is the Order node but we don't need to specify
                                     because the :PURCHASED and the ORDERS relastionships get us there

NOTE: again the relastion ships are similar to relational joins



# Get the ID of other customers who orderd the same products as QUEDE
# start with original query then chain in reverse <- from p:Product to Order to Customer
# 

Match (c1:Customer{customerID:"QUEDE})-[:PURCHASED]->()-[:ORDERS]->(p:Product) <-[:ORDERS]<-()-[:PURCHASED]<-(c2:Customer)
return c2.customerID

NOTE: From Products found for QUEDE go back to :ORDERS relationship to Order node and then 
from Order node (which is anonymous), chain to relationship [:PURCHASED]<- to Customer node


# Retrieve the orders that contain at most two products
# 

match(o:Order)-[:ORDERS]->(p:Product) return o.orderID as ID, count(p) as countProduct
NOTE: this is equivalent to SQL GROUP BY statement 

now filter for at least 2 products 

match(o:Order)-[:ORDERS]->(p:Product) 
with o.orderID as ID, count(p) as countProd
where countProd <= 2 
return ID, countProd


NOTE: change original return to with to access order / product items and then utilize the where clause

HENCE: 
() : node
[] : relationship
{} : propertiy

# Links to Data and Cyper Instructions

https://github.com/neo4j-documentation/developer-resources/tree/gh-pages/data/northwind
https://neo4j.com/docs/getting-started/appendix/tutorials/guide-import-relational-and-etl/


LAB


When using Cypher to query your graph database, you need to understand the difference between nodes, relationships, and paths. Let's take a closer look at these components.
Nodes

A node is used to capture a data item, usually an entity, like a customer, an order, a product, etc.

    (): This represents a node. You did not specify a specific type of node or any properties of that node. It's not relevant to the query.
    (n): This represents a node referred to by the variable n. You can refer to this variable in other parts of your query.
    (n:Airport): Nodes can have different types (i.e. they can belong to different classes/categories). You can add a label to your node to specify its type. Here you are assigning the variable n the nodes with the type Airport.
    (n:Airport {code: 'BOS', desc: 'Boston Logan'}): A node can have properties, which you can specify with {}. Here you are assigning the variable n the nodes of type Airport that have specific values for the code and desc properties.
    n.code: You can access a specific property using this syntax, in this case, the code from the node denoted by n.

Relationships/Edges

A relationship or edge is used to describe a connection between two nodes.

    [r]: This represents a relationship referred to by the variable r. You can refer to this variable in other parts of your query.
    [r:Route]: Relationships can have different types (i.e. they can belong to different classes/categories). You can add a label to your relationship to specify its type. Here you are assigning the variable r the relationships with the type Route.
    [:Route]: A relationship with the label Route not referred to by any variable.
    [r:Route {dist:809}]: Relationships can have properties, which you can specify with {}. Here you are assigning the variable r the relationships of type Route that have specific values for the dist property.
    [r:Route*..4]: This syntax is used to match a pattern where the relationship r with the label route can be repeated between 1 to 4 times. In other words, it matches paths where the route relationship occurs consecutively at least once and at most four times.

Paths

A path is used to capture the graph structure.

    (a:Airport)-[:Route]-(b:Airport): This represents a path that describes that node a and node b are connected by a Route relationship.
    (a:Airport)-[:Route]->(b:Airport): A path can be directed. In this case, this represents a path that describes a directed relationship from node a to node b, but not the other way around.
    (a:Airport)<-[:Route]-(b:Airport): A path that describes a directed relationship from node b to node a, but not the other way around.
    (a:Airport)-[:Route]-(b:Airport)-[:Route]-(c:Airport): A path can chain multiple relationships and any of them can be directional.

You will see more about nodes, relationships and paths in the next exercises while exploring the syntax of the language.

The variables will appear by naming parts of the patterns or a query to reference them. You will see the examples below.

Pattern Matching Syntax

In the following table you can find the characters that represent each component in the Cypher language:
Cypher Pattern 	Description
( ) 	A node
[ ] 	An edge
--> 	Follow outgoing edges from a node
<-- 	Follow incoming edges to a node
-- 	Follow edges in either direction
-[]-> 	Include the outgoing edges in the query (for example, to check a label or property)
<-[]- 	Include the incoming edges in the query (for example, to check a label or property)
-[]- 	Include edges in either direction in the query
-[]->( ) 	The node on the other end of an outgoing edge
<-[]-() 	The node on the other end of an incoming edge






# Conversatoin with Juan Sequeda

graph models: 
    - RDF 
    - Property Graph 

RDF: Resource Description Framework
    - from web, the web is a graph of documents
    RDF is standard for data vs. HTML is the standard for text 
    RDF is based on triples (subject, predicate, object) vs node, edge, node
        i.e. this document, was written on, 1/1/2022


Property Graph
    node, edge, node 
        key-value pairs that could be associated with a node or with an edge


SparQL:  graph query language for RDF
Cypher: neo4j  query language
OpenCypher is open sourced Cypeher
ISO created standard for graphs called GQL


graphs are east to modify, add a node and edges

any model can be turned into a graph: Tables, JSON, etc

knowledge of a knowledge graph is the data on the graph

knowledge graph impact on LLMs

LLMs are 3x more accurate if trained on knowledge graph. 

the graph attributes have knowledge and are helpful in training LLMs
where in relational databases this knowledge is dependent on foreign keys
i.e. more context in a knowledge graph to train an LLM 

"limits of my language are the limits of my world"


Resources: 

https://www.amazon.com/Database-Internals-Deep-Distributed-Systems/dp/1492040347
https://stratos.seas.harvard.edu/files/stratos/files/columnstoresfntdbs.pdf
https://www.deeplearning.ai/short-courses/knowledge-graphs-rag/
https://www.deeplearning.ai/short-courses/building-applications-vector-databases/


## Storage Abstractions
## Course 3
## Week 2

## Data Warehouses & Data lakes

# BIll Inmon

You can use data beyond transaction processing 
    - IBM against data warehousing
    - Marketers were the driver for data warehouses



# Data Warehouse - Key Architectural ideas


data warehouse: subject-oriented, integrated, nonvolatile and time-variant collection of data 
                in support of management's decisions 




Subject Oriented: organizes and stores data around key business domains 
                  (models data to suuport decistion making )

Integrated:       combines data from different sources into a consistent format

Nonvolatile:      data is read-only and cannot be deleted or updated
                    needed for historical analysis

Time-variant:     stores current and historical data

[ Customers ]  [ Products ]
[   Sales   ]  [ Finance  ]




Data Warehouse-Centric Architecture                                           
                                                                                        Data Marts
    Extract                 Transform           Load                          
                                                                              +-----  [ Sales ]   ----+
from source system      - clean and         - Load to Data Warehouse          |                       |    Analytics 
to staging area         standardize data    - utilize comprehensive schema  --+-----  [Marketing] ----+--> and Reports 
                        - model the data                                      |                       |
                                                                              +-----  [finance]   ----+
(change data capture)
                                                                                - simple denormalized schema


History 
Early Data Warehouses
    - Big monolithic server


Data Warehouses with Massively Parallel Processing (MPP)
    - scans large amounts of data in parallel
    - comples configurations and requires effort to maintain


Modern Cloud Data Warhouses 
    - Amazon Redshift
    - Google Big Query
    - Snowflake
        - separates compute from storage
        - expands capability of MPP systems







.