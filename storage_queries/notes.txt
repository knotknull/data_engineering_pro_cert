## Storage Queries
## Course 3
## Week 1






## Data Storage Deep Dive

# Overview



Storage Solution Considerations
    - Data type         - Data size
    - Data format       - Access and update pattern



Storge Hierarchy 
---------------------

[Storage Abstraction  ]
[Cloud Datawarehouse  ]
[Data Lake/Lakehouse  ]
        ^
        |

[Storage Systems     ]
[RDBMS, Graph, Vector]
[Object Storage      ]
        ^
        |
[Raw Ingrediants      ]
[Magnetic, SSD, RAM   ]
[Network, compression ]
[CPU, Caching         ]

Management System:
Organizes data in the raw components and allows 
you to interact with stored data


OLTP: Online Transactional Processing Systems
  Focus on performing read / write queries with low latency


OLAP: Online Analytical Processing Systems
  Focus on applying analytical activities on data
  (aggregation, summarization)


Trade-off between storage costs and performance


# Storage Raw Ingredients - Physical Components of Data Storage

Raw Storage Ingredients

Magnetic Disks / HDDs / Magnetic platters with disk heads
    track / sector give address
    write: encode binary data by change magnetic field
    read:  converts magnetic field into binary data


Solid State Drives / SSD / Flash memory
    electrical charges in flash memory cells
      - charged   cell = 1 bit
      - uncharged cell = 1 bit
    read / writes faster than HDD


Performance Comparison
                       |   Magnetic Disk   |       SSD            |      RAM         |   CPU Cache      |
-----------------------|-------------------|----------------------|------------------|------------------|
Latency                | 4ms               |    0.1ms             | 0.1 microseconds |    1 nanosecond  |
IOPS                   | hundreds          |    10s of thousounds |  Millions        |                  |
Data Transfer speed ** | Up to 300 MB/s    |    4 GB/s            |  100 GB/s        |    1 TB/s        |
Cost                   |  $0.03 = 0.06/GB  |   $0.08 - 0.10/GB    |  > $3/GB         |                  |

** # MBs read/ write form disk to memory 

Improving Performance
Distributed Storage 
    - distribute daa across many HDD
    - read simulatneously 
    - transfer speed limiited by network


Partitioning 
    - slice SSDs into partitions
    - multiple storage controllers running in parallel




Latency (data access time)= Seek time + Rotational latency
Commercial magnetic disk drive: 7200 RPM (rev / min)


# Storage Raw Ingredients - Processes Required for Data Storage


Networking / CPU  
Enhance:
  - read and write performance
  - data durability
  - data availability

Serialization

                Serilize
[ In-memory ] -----------------> [Disk ]
[  Format   ]      Transform     [Format]
              <----------------
                 DeSerialization
Data structures
optimized for 
CPU Usage



Row-based Serialization
[first record][ second record][ third record]
[object 1 ] [ object  2] ....

Column-based Serialization
[ bytes represent 1st col ] [ bytes represent 2nd col ] [ bytes represent 3rd col ] 
[ bytes represent 1st key ] [ bytes represent 2nd key ] [ bytes represent 3rd key ] 





Serialization Formats

Human-readable Textual Formats
    - csv:  row-based format
            prone to error, no defined schema
            adding new rows or columns requires manual handling

    - xml   viewed as legacy format
            slow to serialize / deserialize

    - json  used for plain-text object serialiazation
            viewed as new standard for data echanage of APIs

Binaryi Formats

    - parquet  Column-based format
               for efficient storage and big data proces

    - avro     row-based format
               Uses a schema to define its data structre 
               Supports schema evolution



Compression: a way to reduce the number of bits needed to reqpresent the data
    - remove redundancy and repetition of data to achieve 
       more efficeint encoding 

    - encode characters based on their frequency
    - requces disk space
    - improves query performance
    - Reduces the I/O time needed to load data


Compression Algorithms: 

lossless compression: decompressing file returns exact copy
lossy compression   : decompressions recovers something that sounds/looks like original. 
                      Not an exact copy.  (i.e. audio / video files) 


New compression algos prioritize speed and CPU efficiency over compression ratio:
    - Snappy
    - Zstandard
    - LZFSE
    - LZ4

General purpose compression to compress row and column stores:
    -LZO, LZ4, Snappy, Brotli, Oracle OZIP, Zstd
    

Some algos are specific to column stores: consecutive values from same column are stored together on disk. 
Compression improves database performance:
    - database processes queries faster 
        - less data to read from disk to memory and memory to CPU

Column-based compresion algos:
Run-lengh Encodding: compresses a run of the same values in a column 
      - each run is replaced with a tuple that has 3 elements 
        (value, start position, runLength)
      - each element is represented with a fixed number of bits

eg.     
        34 34 34 63 32 32 32 67 67 67

With RLE, you'll get this result: 

        (34, 1, 3), (63, 4, 1,), (32, 5, 3), (67, 8, 3)

Bit-Vector Encoding (or bitmap encoding): each distinct value is associated with a sequence of bits 
          where the length of the sequence is the same as the number of records / rows in the column  
           a '1' in the i-th position means that the ditinct value appears in the i-th row

34 34 34 63 32 32 32 67 67 67

With this algorithm, this data would be represented by four sequences of ten bits (i.e. the number of rows):

    bit-string for value 34: 1110000000

    bit-string for value 63: 0001000000

    bit-string for value 32: 0000111000

    bit-string for value 67: 0000000111

Bit-vecotr encoding is the most useful when columns have a limited number of unique values 
( i.e. US Staes, store id, product id)


Compression Resources: 
https://stratos.seas.harvard.edu/files/stratos/files/columnstoresfntdbs.pdf
https://docs.aws.amazon.com/redshift/latest/dg/c_Compression_encodings.html
https://martin.kleppmann.com/2012/12/05/schema-evolution-in-avro-protocol-buffers-thrift.html
https://airbyte.com/data-engineering-resources/parquet-vs-avro



Parquet is column based
Avro    is row based


Parquet Benefits:
    columnar storage
    various compression schemes: dictionary, run-length encoding 
    predicate pushdown:  query engines can skip reading irrelevant data blocks
    schema evolution  :  file format enables flexibl changes to data schemas
                            add or modify columns without breaking compatibility
    compatability     :  accomodates big data file formats 

Parquet Use Cases:
    - Big Data analytics
    - Data Warehousing
    - ETL Pipelines
        - Parquet can be used as an intermediate storage format in ETL pipelines
            can transform and process more efficiently before loading into data warehouse
    - Log Analytics: Parquet is well-suited for analysing log files and event data
            faster log analysis
    - Data Archiving: Good choice for long-term data archiving.



Avro Benefits:
    binary row-oriented format
    efficient serialize / deserialize
    Suitable for data interchange
    Schema-based Serialization
        - schema defines data structure 
        - schema and serialized data are stored in Avro data file
    Schema Evolution
        forward / backwared compatability
    Dynamic Typing 
        data can be searialized and deserialized without generating / sharing specific code
    Interoperability 
        data format supports several programming languages

Avro Use Cases:
    Data Interchange   : between apps, services, languages
    Streaming Analytics: used in streaming data pipelines
        efficient Serialization and compatability with streaming platform   
    Messaging Systems  :  message queues, etc.
    Data Replication   :  replicate data from one system to another
    Big Data Processing:  used with big data frameworks, Kafka.  
        data ingested, processed and analyzed efficiently across different stages of data pipeline


Clod Storage Options: Block, Object and File Storage

File Storage: organizes files into a directory tree
    Each directory contains metadata about its files / folders:
        - Name
        - Owner
        - Last Modified
        - Permissions
        - pointer to the actual entity

    Cloud FIle Storage Service 
        - Amazon Elastic File System (EFS)
        - provides access to shared files over a network 
        - Networking, scaling and configuration handled by clour vendor

    File storage sits on top of Block Storage


File Storage: divides files into small, fixed-size blocks of data and stores on disk
    - each block has a unique identifier
    - efficiently retrieve and modify data in individual blocks
    - can distribute blocks across multiple storage disks
        - higher scalability
        - stronger data durability

    i.e. think direct db writes or VM storage


Block Storage Lookup Table 
    File Piece     Block Identifier
  1st Piece             1232  
  2nd Piece             1234
  3rd Piece             1236
  4th Piece             1238


when ask for a file, all blocks are gathered and merged together
    can update a specific block by identifer and don't have to write entire file


Block Storage Use Cases:
    - Ideal for freuent access and modification
    - Enables OLTP systems to perform small and frequent read and 
      write operations with low latency
    - Provides persistent storage for virtual machines

Default storage for EC2:
    Amazon Elastic Block Storage (EBS)
    1. SSD for latency-sensitive workloads
    2. Magnetic disks to store infrequently-accessed data

NOTE: Block storage caps out at around several TB because it is attached to compute


Object Storage decouples data storage layer from the compute layer
    - can scale to Petabytes



Object Storage
    - Stores immutable files as data objects in a flat structure
    - organized into top-level logical containers (S3 Bucker)
    - each object assigned unique identifier (key)

s3://oreilly-data-engineering-book/data-example.json
|                                | |               |
+--------------------------------+ +---------------+
                |                         |
           
(must be unique across AWS)

    - once data is writtent object becomes immutable
    - have to re-write the entire object if file is updated
    - can scale horizontally and support performant parallel operations
        - each node holds shards of objects


Object Storage Use Cases:

        Ideal for                                       Not Ideal for
- Storage layer of cloud                            - Not good for supporting transactional workloads 
  data warehouses and data lakes

- Storing data needed in OLAP systems 
        (read heavy analytics)

- Machine learning pipelines
    - raw text
    - images
    - Videos
    - Audio


Cloud Storage Options 

    File Storage                    Block Storage               Object Storage
-------------------------       ------------------------    -------------------------   
-supports data sharing          - supports transaction      - Supports analytical queries on 
                                   workloads                    massive datasets

-Easy to manage with low        - Allows frequent read      - Offers high scalability and
 with low performance and         and write operations        parallel data processing
 scalabilitya requirements        with low latency


Storage Tiers: Hot, Warm, Cold Data

                         Hot Storage             Warm Storage               Cold Storage
                        -------------           --------------             -------------- 
Access frequency        Very frequent           Less Frequent               Infrequently

Example                 Product                 Reqular reports             Archive
                        Recommendation           & Analsysis

Storage Medium          SSD & Memory            Magnetic Disk or            Low Cost magnetic disks 
                                                hybrid storage systems

Storage Cost            High                        Medium                      Low

Retrieval  Cost         Low                         Medium                      How



AWS Storage Tiers
                                Amazon S3
                              Access Frequency

        Hot Storage              Warm Storage          Cold Storage
        -------------           --------------         -------------- 
    S3 Express One zone         S3 Standard IA         S3 Glacier Flexible Retrieval
    S3 Standard                 S3 One Zone-IA         S3 Glacier Deep Archive
                                                       S3 Glacier Instant Retrieval 
** IA: Infrequently Accessed



# Distributed Storage Systems

How Distributed Storage System Works

    - distribute data across multiple servers called nodes
        connected by network

    - groups of nodes make up a cluster

    - each node contains storage mediums
        - HDD / SSD

    - each node has processing abilities 
        - data management
        - replication / access control
Total Capacity is capacity of all nodes


 [------] [------] [------]
 [......] [......] [......] 
 [------] [------] [------]
    |        |        |
    +--------+--------+
             |
        Nodes / Groups of Nodes = cluster

Can scale horizontally by adding additional nodes to cluster

Single Machine Storage Architecture 
    - can only scale vertical
    - can only upgrade storage capacity of a single server

Multiple Nodes / Clusters: 
    -  Higher fault tolerance and data durability
    -  High Availability
    -  Processing many reads and write 
        operations in parallel
    -  Fast data access 
        - serve request from nearest replica


Distributed Storage Architecture utilized by 
    - Object Storage
    - Cloud Data Warehouse
    - Hadoop HDFS
    - Spark


Methods for Distributing Data

Replication:   Same copy across many nodes 
                High availability and performance                


Partitioning:   splits big dataset into smaller subsets 
(sharding)          i.e. partitions / shards

Takes time for updates to complete:
    - wait for update to complete
    - access "sort of" recent data


Distributed Storage Considerations: CAP Therom

CAP Therom: Any distributed  system can only guarantee 2 of 3 properties
    - Consistency
    - Availability
    - Partition Tolerance

Strong Consistency : every read reflects the latest write operation

Availability       :  Every request will recieve a response

Partition Tolerance:  System continues to function when network experiences 
                       distruptions or failures isolate some nodes from otheres
                        s(split brain)


No Distributed System is immune from network failures or disruptions  
    Network Paritioning has to be tolerated


Hence, have to choose between Availability or Consistence if Partition Tolerance is mandatory
    i.e. only 2 of 3 guaranteed 


    ACID (RDBMS) vs. BASE

BASE:  Baseically Available:  consistent data is available most of the time
       Soft-state          :  uncertain if transaction is committed or uncommitted   
       Eventual Consistency:  at some point, reading data will return consistent values   


# Database Partitioning / Sharding Methods

Common Sharding Methods:

Range-based sharding:
    - method splits rows based on a range of values
i.e. split by the first letter of  customer name

Name            Shard Key
A to I              A
J to S              B
T to Z              C
 

NOTE: can lead to unbalanced shards


Hashed sharding:
    - method uses a mathematical fomula called a hash function to determine how to partition data.
        - get a hash of some data attributes and use that as shard key

Geo sharding:
    - method partitions data by geographic location
        store cutomer informatin in a node that is physically in that location 


Partitioning / sharding resources: 
https://aws.amazon.com/what-is/database-sharding/
https://www.amazon.com/Designing-Data-Intensive-Applications-Reliable-Maintainable/dp/1449373321/ref=sr_1_1?adgrpid=1344703291324157&dib=eyJ2IjoiMSJ9.i1bUGZK7N-KyWM2sQR7-B8KYS_yn_vgEDIPgCZKZEqrD3_kYv1WLMRNg2a_cyMTZkenScKZLD1xQT6PoxGtZjpfYLwagMBcOcvqwyg12Ux6vvPPHgXX1vMZoOg1vTM_pc7M5GoJOYAWtL-UQU8rix049vlX-qOUnpYLTJ2MrssfiHjzXSj62mtpldPZ9F8sSVwb2QyjkabDuQFUBKt8wljiPffjJIMY5B8rR7JfDvO8.HDmJ7Lu7-7fnydSo4DSG8hxechXwbUNz0baNI01HWH0&dib_tag=se&hvadid=84044027549737&hvbmt=be&hvdev=c&hvlocphy=44152&hvnetw=o&hvqmt=e&hvtargid=kwd-84044312865379%3Aloc-190&hydadcr=16438_10463512&keywords=designing+data+intensive+applications&qid=1713574421&sr=8-1





# Comparing Cloud Storage Options


MAP LAST HERE

https://www.coursera.org/learn/data-storage-and-queries/lecture/46Pvt/lab-walkthrough-comparing-cloud-storage-options





.