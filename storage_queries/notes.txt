## Storage Queries
## Course 3
## Week 1






## Data Storage Deep Dive

# Overview



Storage Solution Considerations
    - Data type         - Data size
    - Data format       - Access and update pattern



Storge Hierarchy 
---------------------

[Storage Abstraction  ]
[Cloud Datawarehouse  ]
[Data Lake/Lakehouse  ]
        ^
        |

[Storage Systems     ]
[RDBMS, Graph, Vector]
[Object Storage      ]
        ^
        |
[Raw Ingrediants      ]
[Magnetic, SSD, RAM   ]
[Network, compression ]
[CPU, Caching         ]

Management System:
Organizes data in the raw components and allows 
you to interact with stored data


OLTP: Online Transactional Processing Systems
  Focus on performing read / write queries with low latency


OLAP: Online Analytical Processing Systems
  Focus on applying analytical activities on data
  (aggregation, summarization)


Trade-off between storage costs and performance


# Storage Raw Ingredients - Physical Components of Data Storage

Raw Storage Ingredients

Magnetic Disks / HDDs / Magnetic platters with disk heads
    track / sector give address
    write: encode binary data by change magnetic field
    read:  converts magnetic field into binary data


Solid State Drives / SSD / Flash memory
    electrical charges in flash memory cells
      - charged   cell = 1 bit
      - uncharged cell = 1 bit
    read / writes faster than HDD


Performance Comparison
                       |   Magnetic Disk   |       SSD            |      RAM         |   CPU Cache      |
-----------------------|-------------------|----------------------|------------------|------------------|
Latency                | 4ms               |    0.1ms             | 0.1 microseconds |    1 nanosecond  |
IOPS                   | hundreds          |    10s of thousounds |  Millions        |                  |
Data Transfer speed ** | Up to 300 MB/s    |    4 GB/s            |  100 GB/s        |    1 TB/s        |
Cost                   |  $0.03 = 0.06/GB  |   $0.08 - 0.10/GB    |  > $3/GB         |                  |

** # MBs read/ write form disk to memory 

Improving Performance
Distributed Storage 
    - distribute daa across many HDD
    - read simulatneously 
    - transfer speed limiited by network


Partitioning 
    - slice SSDs into partitions
    - multiple storage controllers running in parallel




Latency (data access time)= Seek time + Rotational latency
Commercial magnetic disk drive: 7200 RPM (rev / min)


# Storage Raw Ingredients - Processes Required for Data Storage


Networking / CPU  
Enhance:
  - read and write performance
  - data durability
  - data availability

Serialization

                Serilize
[ In-memory ] -----------------> [Disk ]
[  Format   ]      Transform     [Format]
              <----------------
                 DeSerialization
Data structures
optimized for 
CPU Usage



Row-based Serialization
[first record][ second record][ third record]
[object 1 ] [ object  2] ....

Column-based Serialization
[ bytes represent 1st col ] [ bytes represent 2nd col ] [ bytes represent 3rd col ] 
[ bytes represent 1st key ] [ bytes represent 2nd key ] [ bytes represent 3rd key ] 





Serialization Formats

Human-readable Textual Formats
    - csv:  row-based format
            prone to error, no defined schema
            adding new rows or columns requires manual handling

    - xml   viewed as legacy format
            slow to serialize / deserialize

    - json  used for plain-text object serialiazation
            viewed as new standard for data echanage of APIs

Binaryi Formats

    - parquet  Column-based format
               for efficient storage and big data proces

    - avro     row-based format
               Uses a schema to define its data structre 
               Supports schema evolution



Compression: a way to reduce the number of bits needed to reqpresent the data
    - remove redundancy and repetition of data to achieve 
       more efficeint encoding 

    - encode characters based on their frequency
    - requces disk space
    - improves query performance
    - Reduces the I/O time needed to load data


Compression Algorithms: 

lossless compression: decompressing file returns exact copy
lossy compression   : decompressions recovers something that sounds/looks like original. 
                      Not an exact copy.  (i.e. audio / video files) 


New compression algos prioritize speed and CPU efficiency over compression ratio:
    - Snappy
    - Zstandard
    - LZFSE
    - LZ4

General purpose compression to compress row and column stores:
    -LZO, LZ4, Snappy, Brotli, Oracle OZIP, Zstd
    

Some algos are specific to column stores: consecutive values from same column are stored together on disk. 
Compression improves database performance:
    - database processes queries faster 
        - less data to read from disk to memory and memory to CPU

Column-based compresion algos:
Run-lengh Encodding: compresses a run of the same values in a column 
      - each run is replaced with a tuple that has 3 elements 
        (value, start position, runLength)
      - each element is represented with a fixed number of bits

eg.     
        34 34 34 63 32 32 32 67 67 67

With RLE, you'll get this result: 

        (34, 1, 3), (63, 4, 1,), (32, 5, 3), (67, 8, 3)

Bit-Vector Encoding (or bitmap encoding): each distinct value is associated with a sequence of bits 
          where the length of the sequence is the same as the number of records / rows in the column  
           a '1' in the i-th position means that the ditinct value appears in the i-th row

34 34 34 63 32 32 32 67 67 67

With this algorithm, this data would be represented by four sequences of ten bits (i.e. the number of rows):

    bit-string for value 34: 1110000000

    bit-string for value 63: 0001000000

    bit-string for value 32: 0000111000

    bit-string for value 67: 0000000111

Bit-vecotr encoding is the most useful when columns have a limited number of unique values 
( i.e. US Staes, store id, product id)


Compression Resources: 
https://stratos.seas.harvard.edu/files/stratos/files/columnstoresfntdbs.pdf
https://docs.aws.amazon.com/redshift/latest/dg/c_Compression_encodings.html
https://martin.kleppmann.com/2012/12/05/schema-evolution-in-avro-protocol-buffers-thrift.html
https://airbyte.com/data-engineering-resources/parquet-vs-avro



Parquet is column based
Avro    is row based


Parquet Benefits:
    columnar storage
    various compression schemes: dictionary, run-length encoding 
    predicate pushdown:  query engines can skip reading irrelevant data blocks
    schema evolution  :  file format enables flexibl changes to data schemas
                            add or modify columns without breaking compatibility
    compatability     :  accomodates big data file formats 

Parquet Use Cases:
    - Big Data analytics
    - Data Warehousing
    - ETL Pipelines
        - Parquet can be used as an intermediate storage format in ETL pipelines
            can transform and process more efficiently before loading into data warehouse
    - Log Analytics: Parquet is well-suited for analysing log files and event data
            faster log analysis
    - Data Archiving: Good choice for long-term data archiving.



Avro Benefits:
    binary row-oriented format
    efficient serialize / deserialize
    Suitable for data interchange
    Schema-based Serialization
        - schema defines data structure 
        - schema and serialized data are stored in Avro data file
    Schema Evolution
        forward / backwared compatability
    Dynamic Typing 
        data can be searialized and deserialized without generating / sharing specific code
    Interoperability 
        data format supports several programming languages

Avro Use Cases:
    Data Interchange   : between apps, services, languages
    Streaming Analytics: used in streaming data pipelines
        efficient Serialization and compatability with streaming platform   
    Messaging Systems  :  message queues, etc.
    Data Replication   :  replicate data from one system to another
    Big Data Processing:  used with big data frameworks, Kafka.  
        data ingested, processed and analyzed efficiently across different stages of data pipeline


Clod Storage Options: Block, Object and File Storage

File Storage: organizes files into a directory tree
    Each directory contains metadata about its files / folders:
        - Name
        - Owner
        - Last Modified
        - Permissions
        - pointer to the actual entity

    Cloud FIle Storage Service 
        - Amazon Elastic File System (EFS)
        - provides access to shared files over a network 
        - Networking, scaling and configuration handled by clour vendor

    File storage sits on top of Block Storage


File Storage: divides files into small, fixed-size blocks of data and stores on disk
    - each block has a unique identifier
    - efficiently retrieve and modify data in individual blocks
    - can distribute blocks across multiple storage disks
        - higher scalability
        - stronger data durability

    i.e. think direct db writes or VM storage


Block Storage Lookup Table 
    File Piece     Block Identifier
  1st Piece             1232  
  2nd Piece             1234
  3rd Piece             1236
  4th Piece             1238


when ask for a file, all blocks are gathered and merged together
    can update a specific block by identifer and don't have to write entire file


Block Storage Use Cases:
    - Ideal for freuent access and modification
    - Enables OLTP systems to perform small and frequent read and 
      write operations with low latency
    - Provides persistent storage for virtual machines

Default storage for EC2:
    Amazon Elastic Block Storage (EBS)
    1. SSD for latency-sensitive workloads
    2. Magnetic disks to store infrequently-accessed data

NOTE: Block storage caps out at around several TB because it is attached to compute


Object Storage decouples data storage layer from the compute layer
    - can scale to Petabytes



Object Storage
    - Stores immutable files as data objects in a flat structure
    - organized into top-level logical containers (S3 Bucker)
    - each object assigned unique identifier (key)

s3://oreilly-data-engineering-book/data-example.json
|                                | |               |
+--------------------------------+ +---------------+
                |                         |
           Bucket Name                Object Key
(must be unique across AWS)

    - once data is writtent object becomes immutable
    - have to re-write the entire object if file is updated
    - can scale horizontally and support performant parallel operations
        - each node holds shards of objects


Object Storage Use Cases:

        Ideal for                                       Not Ideal for
- Storage layer of cloud                            - Not good for supporting transactional workloads 
  data warehouses and data lakes

- Storing data needed in OLAP systems 
        (read heavy analytics)

- Machine learning pipelines
    - raw text
    - images
    - Videos
    - Audio






.